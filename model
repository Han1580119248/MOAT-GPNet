{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29c0f8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from utils.ipynb\n",
      "importing Jupyter notebook from network.ipynb\n"
     ]
    }
   ],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "from sklearn import datasets\n",
    "import import_ipynb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat, reduce\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf6abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1f5d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # ä½¿ç”¨ GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f966d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        nn.init.xavier_normal_(self.weight.data)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.fill_(0.0)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        support = torch.mm(x, self.weight)\n",
    "        output = torch.sparse.mm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0355efd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN_E(nn.Module):\n",
    "    def __init__(self, in_dim, hgcn_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.gc1 = GraphConvolution(in_dim, hgcn_dim[0])\n",
    "        self.gc2 = GraphConvolution(hgcn_dim[0], hgcn_dim[1])\n",
    "        self.gc3 = GraphConvolution(hgcn_dim[1], hgcn_dim[2])\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.gc1(x, adj)\n",
    "        x = F.leaky_relu(x, 0.25)\n",
    "        \n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        \n",
    "        x = self.gc2(x, adj)\n",
    "        x = F.leaky_relu(x, 0.25)\n",
    "        \n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "\n",
    "        x = self.gc3(x, adj)\n",
    "        x = F.leaky_relu(x, 0.25)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65f2340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "           m.bias.data.fill_(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79b763ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_1(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.clf = nn.Sequential(nn.Linear(in_dim, out_dim))\n",
    "        self.clf.apply(xavier_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.clf(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7d29dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph(nn.Module):\n",
    "    def __init__(self, in_dim, hgcn_dim, dropout, classifier_out_dim):\n",
    "        super().__init__()\n",
    "        self.gcn = GCN_E(in_dim, hgcn_dim, dropout)\n",
    "#         self.classifier = Classifier_1(hgcn_dim[-1], classifier_out_dim)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.gcn(x, adj)\n",
    "#         x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2b1e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VCDN(nn.Module):\n",
    "    def __init__(self, num_view, num_cls, hvcdn_dim):\n",
    "        super().__init__()\n",
    "        self.num_cls = num_cls\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(pow(num_cls, num_view), hvcdn_dim),\n",
    "            nn.LeakyReLU(0.25),\n",
    "            nn.Linear(hvcdn_dim, num_cls)\n",
    "        )\n",
    "        self.model.apply(xavier_init)\n",
    "        \n",
    "    def forward(self, in_list):\n",
    "        num_view = len(in_list)\n",
    "        for i in range(num_view):\n",
    "            in_list[i] = torch.sigmoid(in_list[i])\n",
    "        x = torch.reshape(torch.matmul(in_list[0].unsqueeze(-1), in_list[1].unsqueeze(1)),(-1,pow(self.num_cls,2),1))\n",
    "        for i in range(2,num_view):\n",
    "            x = torch.reshape(torch.matmul(x, in_list[i].unsqueeze(1)),(-1,pow(self.num_cls,i+1),1))\n",
    "        vcdn_feat = torch.reshape(x, (-1,pow(self.num_cls,num_view)))\n",
    "        output = self.model(vcdn_feat)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad7d5770",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubNet(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size):\n",
    "        super(SubNet, self).__init__()\n",
    "        encoder1 = nn.Sequential(nn.Linear(in_size, hidden_size),nn.Tanh())\n",
    "        encoder2 = nn.Sequential(nn.Linear(hidden_size, hidden_size), nn.Tanh())\n",
    "        self.encoder = nn.Sequential(encoder1, encoder2)\n",
    "    def forward(self, x):\n",
    "        y = self.encoder(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2aac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CMFM(nn.Module):\n",
    "    def __init__(self, in_size, output_dim, dropout, dim_hvcdn=64):\n",
    "        super(CMFM, self).__init__()\n",
    "        \n",
    "        self.lmf = LMF(in_size, 16)\n",
    "    \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_size, 2), \n",
    "            nn.LeakyReLU(negative_slope=0.01),  # ç”¨ LeakyReLU æ›¿ä»£ ReLU\n",
    "            nn.Dropout(p=dropout)\n",
    "        )\n",
    "\n",
    "        ### Path\n",
    "        self.linear_h1 = nn.Sequential(\n",
    "            nn.Linear(in_size, in_size), \n",
    "            nn.LeakyReLU(negative_slope=0.01)  # ç”¨ LeakyReLU æ›¿ä»£ ReLU\n",
    "        )\n",
    "        self.linear_z1 = nn.Bilinear(in_size, in_size * 2, in_size)\n",
    "        self.linear_o1 = nn.Sequential(\n",
    "            nn.Linear(in_size, in_size), \n",
    "            nn.LeakyReLU(negative_slope=0.01),  # ç”¨ LeakyReLU æ›¿ä»£ ReLU\n",
    "            nn.Dropout(p=dropout)\n",
    "        )\n",
    "        \n",
    "        # å®šä¹‰åˆå§‹åŒ–å‡½æ•°\n",
    "        self.apply(self.init_weights)\n",
    "    \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "        elif isinstance(m, nn.Bilinear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "    \n",
    "    def forward(self, dec_logits1, dec_logits2, dec_logits3):\n",
    "        h1 = self.linear_h1(dec_logits1)\n",
    "        vec31 = torch.cat((dec_logits2, dec_logits3), dim=1)\n",
    "        z1 = self.linear_z1(dec_logits1, vec31)\n",
    "        o1 = self.linear_o1(torch.sigmoid(z1) * h1)\n",
    "        \n",
    "        h2 = self.linear_h1(dec_logits2)\n",
    "        vec32 = torch.cat((dec_logits1, dec_logits3), dim=1)\n",
    "        z2 = self.linear_z1(dec_logits2, vec32)\n",
    "        o2 = self.linear_o1(torch.sigmoid(z2) * h2)\n",
    "        \n",
    "        h3 = self.linear_h1(dec_logits3)\n",
    "        vec33 = torch.cat((dec_logits1, dec_logits2), dim=1)\n",
    "        z3 = self.linear_z1(dec_logits2, vec33)\n",
    "        o3 = self.linear_o1(torch.sigmoid(z3) * h3)\n",
    "        \n",
    "        lmf = self.lmf(o1, o2, o3)\n",
    "        \n",
    "        return lmf, dec_logits1, dec_logits2, dec_logits3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cbdbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, num_class, dropout):\n",
    "        super().__init__()\n",
    "        self.views = len(in_dim)+1\n",
    "        self.classes = num_class\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.TCPConfidenceLayer = nn.ModuleList([Classifier_1(hidden_dim[0], 1) for _ in range(self.views)])\n",
    "        self.TCPClassifierLayer = nn.ModuleList([Classifier_1(hidden_dim[0], num_class) for _ in range(self.views)])\n",
    "#         self.classifier = Classifier_1(hidden_dim, num_class)\n",
    "\n",
    "        self.MMClasifier = []\n",
    "        for layer in range(1, len(hidden_dim) - 1):\n",
    "            self.MMClasifier.append(Classifier_1(self.views * hidden_dim[0], hidden_dim[layer]))\n",
    "            self.MMClasifier.append(nn.ReLU())\n",
    "            self.MMClasifier.append(nn.Dropout(p=dropout))\n",
    "        if len(self.MMClasifier):\n",
    "            self.MMClasifier.append(Classifier_1(hidden_dim[-1], num_class))\n",
    "        else:\n",
    "            self.MMClasifier.append(Classifier_1(self.views * hidden_dim[-1], num_class))\n",
    "        \n",
    "        self.MMClasifier = nn.Sequential(*self.MMClasifier)\n",
    "\n",
    "    def forward(self, feature, label=None, infer=False):\n",
    "        criterion = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        TCPLogit, TCPConfidence =  dict(), dict()\n",
    "        all_cord = []\n",
    "        for view in range(self.views):\n",
    "            feature[view] = F.relu(feature[view])\n",
    "            feature[view] = F.dropout(feature[view], self.dropout, training=self.training)\n",
    "            TCPLogit[view] = self.TCPClassifierLayer[view](feature[view])\n",
    "            TCPConfidence[view] = self.TCPConfidenceLayer[view](feature[view])\n",
    "            feature[view] = feature[view] * TCPConfidence[view]\n",
    "            all_cord.append(feature[view])\n",
    "        \n",
    "        # ä½¿ç”¨ torch.cat() æ²¿ç€ç¬¬ 1 ç»´ï¼ˆåˆ—ç»´åº¦ï¼‰æ‹¼æ¥\n",
    "        MMfeature = torch.cat(all_cord, dim=1)\n",
    "        MMlogit = self.MMClasifier(MMfeature)\n",
    "        if infer:\n",
    "            return MMlogit\n",
    "        MMLoss = torch.mean(criterion(MMlogit, label))\n",
    "        for view in range(self.views):\n",
    "            pred = F.softmax(TCPLogit[view], dim=1)\n",
    "            p_target = torch.gather(input=pred, dim=1, index=label.unsqueeze(dim=1)).view(-1)\n",
    "            confidence_loss = torch.mean(\n",
    "                F.mse_loss(TCPConfidence[view].view(-1), p_target) + criterion(TCPLogit[view], label))\n",
    "            MMLoss = MMLoss + confidence_loss\n",
    "        return MMLoss, MMlogit, MMfeature\n",
    "    # def get_embedding(self, feature):  # â† ğŸ‘ˆ ä½ æ·»åŠ åœ¨è¿™é‡Œ\n",
    "    #     all_cord = []\n",
    "    #     for view in range(self.views):\n",
    "    #         f = F.relu(feature[view])\n",
    "    #         f = F.dropout(f, self.dropout, training=False)\n",
    "    #         confidence = self.TCPConfidenceLayer[view](f)\n",
    "    #         f = f * confidence\n",
    "    #         all_cord.append(f)\n",
    "    #     MMfeature = torch.cat(all_cord, dim=1)\n",
    "    #     return MMfeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "149e09bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ead5ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default(val, default_val):\n",
    "    return val if val is not None else default_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20f2c7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_zero_(layer):\n",
    "    nn.init.constant_(layer.weight, 0.)\n",
    "    if exists(layer.bias):\n",
    "        nn.init.constant_(layer.bias, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea31456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GEGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gates = x.chunk(2, dim=-1)\n",
    "        return x * F.gelu(gates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26c6bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding_layer(nn.Module):\n",
    "    def __init__(self, feature_len, embeding_dim):\n",
    "        super(Embedding_layer, self).__init__()\n",
    "        self.M = torch.nn.Parameter(torch.tensor(scale(np.random.rand(feature_len, embeding_dim))).float(),\n",
    "                                    requires_grad=True)\n",
    "\n",
    "    def forward(self, enc_inputs):  # X: [batch_size, feature_len,embeding_len]\n",
    "        if enc_inputs.shape[2]>2:\n",
    "            X=enc_inputs[:,:,0:1]* self.M\n",
    "            for i in range(1,enc_inputs.shape[2]):\n",
    "                X = torch.cat((X,enc_inputs[:,:,i:(i+1)] * self.M),2)\n",
    "        else:\n",
    "            X = enc_inputs * self.M\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a08b5147",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            mult=4,\n",
    "            dropout=0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult * 2),\n",
    "            GEGLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "        init_zero_(self.net[-1])\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10c065e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            heads,\n",
    "            dim_head,\n",
    "            dropout,\n",
    "            gating_module=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.gating_module = gating_module\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "        if self.gating_module:\n",
    "            self.gating = nn.Linear(dim, inner_dim)\n",
    "            nn.init.constant_(self.gating.weight, 0.)\n",
    "            nn.init.constant_(self.gating.bias, 1.)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        init_zero_(self.to_out)\n",
    "\n",
    "    def forward(self, x, attn_bias=None, context=None, context_mask=None, tie_dim=None,output_attentions=False):\n",
    "        device, orig_shape, h, has_context = x.device, x.shape, self.heads, exists(context)\n",
    "        context = default(context, x)\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim=-1))\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), (q, k, v))\n",
    "        q = q * self.scale\n",
    "\n",
    "        if exists(tie_dim):\n",
    "            q, k = map(lambda t: rearrange(t, '(b r) ... -> b r ...', r=tie_dim), (q, k))\n",
    "            q = q.mean(dim=1)\n",
    "            dots = einsum('b h i d, b r h j d -> b r h i j', q, k)\n",
    "            dots = rearrange(dots, 'b r ... -> (b r) ...')\n",
    "        else:\n",
    "            dots = torch.einsum('b h i d, b h j d -> b h i j', q, k)\n",
    "\n",
    "        if output_attentions:\n",
    "            dots_out = dots\n",
    "        if exists(attn_bias):\n",
    "            dots = dots + attn_bias\n",
    "        # attention\n",
    "        dots = dots - dots.max(dim=-1, keepdims=True).values\n",
    "        attn = dots.softmax(dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        # aggregate\n",
    "        out = torch.einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        # merge heads\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        # gating\n",
    "        if self.gating_module:\n",
    "            gates = self.gating(x)\n",
    "            out = out * gates.sigmoid()\n",
    "        # combine to out\n",
    "        out = self.to_out(out)\n",
    "        if output_attentions:\n",
    "            return out, attn, dots_out\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc067993",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AxialAttention(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            heads,\n",
    "            dropout=0.,\n",
    "            row_attn=True,\n",
    "            col_attn=True,\n",
    "            accept_edges=False,\n",
    "            global_query_attn=False,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert not (not row_attn and not col_attn), 'row or column attention must be turned on'\n",
    "        self.heads = heads\n",
    "        self.row_attn = row_attn\n",
    "        self.col_attn = col_attn\n",
    "        self.global_query_attn = global_query_attn\n",
    "        self.accept_edges = accept_edges\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim=dim, heads=heads,dropout=dropout, **kwargs)\n",
    "\n",
    "    def forward(self, x, edges=None, output_attentions=False):\n",
    "        assert self.row_attn ^ self.col_attn, 'has to be either row or column attention, but not both'\n",
    "\n",
    "        b, h, w = x.shape\n",
    "\n",
    "        # axial attention\n",
    "        if self.row_attn:\n",
    "            axial_dim = b\n",
    "            input_fold_eq = 'b h w -> b w h'\n",
    "            output_fold_eq = 'b h w -> b w h'\n",
    "        elif self.col_attn:\n",
    "            axial_dim = b\n",
    "            input_fold_eq = 'b w h -> b w h'\n",
    "            output_fold_eq = 'b w h -> b w h'\n",
    "\n",
    "        x = rearrange(x, input_fold_eq)\n",
    "\n",
    "        attn_bias = None\n",
    "        if self.accept_edges and exists(edges):\n",
    "            attn_bias = repeat(edges, 'b i j-> b x i j', x=self.heads)\n",
    "\n",
    "        tie_dim = axial_dim if self.global_query_attn else None\n",
    "        if output_attentions:\n",
    "            out, attn_out_1, attn_out_2 = self.attn(x, attn_bias=attn_bias, tie_dim=tie_dim, output_attentions=output_attentions)\n",
    "            out = rearrange(out, output_fold_eq)\n",
    "            return out, attn_out_1, attn_out_2\n",
    "        else:\n",
    "            out = self.attn(x, attn_bias=attn_bias, tie_dim=tie_dim, output_attentions=output_attentions)\n",
    "            out = rearrange(out, output_fold_eq)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4ad9e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvoformerBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            *,\n",
    "            row_dim,\n",
    "            col_dim,\n",
    "            heads,\n",
    "            dim_head,\n",
    "            beta,\n",
    "            attn_dropout=0.,\n",
    "            ff_dropout=0.,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.beta=beta\n",
    "        self.layer = nn.ModuleList([\n",
    "            AxialAttention(dim=row_dim, heads=heads, dim_head=dim_head, dropout=attn_dropout, row_attn=True,col_attn=False),\n",
    "            FeedForward(dim=row_dim, dropout=ff_dropout)\n",
    "        ])\n",
    "\n",
    "    def forward(self, m, output_attentions=False):\n",
    "        msa_attn_row, msa_ff_row = self.layer\n",
    "        # msa attention and transition\n",
    "        if output_attentions:\n",
    "            m_, attn_out_row_1, attn_out_row_2 = msa_attn_row(m, output_attentions=output_attentions)\n",
    "            m = m_+m\n",
    "            m = msa_ff_row(m.permute(0,2,1)) + m.permute(0,2,1)\n",
    "            m = m.permute(0,2,1)\n",
    "            m_, attn_out_col_1, attn_out_col_2 = msa_attn_col(m, output_attentions=output_attentions)\n",
    "            m = self.beta*m_+m\n",
    "            m = msa_ff_col(m) + m\n",
    "        else:\n",
    "            m = msa_attn_row(m) + m\n",
    "            m = msa_ff_row(m.permute(0,2,1)) + m.permute(0,2,1)\n",
    "            m = m.permute(0,2,1)\n",
    "\n",
    "        # æ›´æ–°x\n",
    "        m=m.unsqueeze(3)\n",
    "        if output_attentions:\n",
    "            return m[:,:,:,0], attn_out_row_1, attn_out_row_2, attn_out_col_1, attn_out_col_2\n",
    "        else:\n",
    "            return  m[:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4d15e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evoformer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            depth,\n",
    "            dim_network,\n",
    "            row_dim,\n",
    "            col_dim,\n",
    "            heads,\n",
    "            dim_head,\n",
    "            embeding,\n",
    "            beta,\n",
    "            attn_dropout,\n",
    "            ff_dropout,\n",
    "            embeding_num,\n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.dim_network = dim_network\n",
    "        self.row_dim = row_dim\n",
    "        self.col_dim = col_dim\n",
    "        self.heads = heads\n",
    "        self.dim_head = dim_head\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.ff_dropout = ff_dropout\n",
    "        self.beta=beta\n",
    "        self.embeding = embeding\n",
    "        self.embeding_num=embeding_num\n",
    "        if self.embeding:\n",
    "            self.embedding_layer = Embedding_layer(feature_len=self.col_dim, embeding_dim=int(self.embeding_num/self.row_dim))\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EvoformerBlock(row_dim=self.row_dim,col_dim=self.col_dim, heads=self.heads, dim_head=self.dim_head,beta=self.beta,\n",
    "                              attn_dropout=self.attn_dropout, ff_dropout=self.ff_dropout) for _ in range(self.depth)])\n",
    "\n",
    "    def forward(self, m, output_attentions=False):\n",
    "        if self.embeding:\n",
    "            m=self.embedding_layer(m.permute(0,2,1)).permute(0,2,1)\n",
    "        attn_out_row_1_list = []\n",
    "        attn_out_row_2_list = []\n",
    "        attn_out_col_1_list = []\n",
    "        attn_out_col_2_list = []\n",
    "        for layer in self.layers:\n",
    "            if output_attentions:\n",
    "                m, attn_out_row_1, attn_out_row_2, attn_out_col_1, attn_out_col_2 = layer(m,output_attentions=output_attentions)\n",
    "                attn_out_row_1_list.append(attn_out_row_1)\n",
    "                attn_out_row_2_list.append(attn_out_row_2)\n",
    "                attn_out_col_1_list.append(attn_out_col_1)\n",
    "                attn_out_col_2_list.append(attn_out_col_2)\n",
    "            else:\n",
    "                m = layer(m)\n",
    "\n",
    "        if output_attentions:\n",
    "            return m,attn_out_row_1_list, attn_out_row_2_list, attn_out_col_1_list, attn_out_col_2_list\n",
    "        else:\n",
    "            return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2126d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pathformer_model(nn.Module):\n",
    "    def __init__(self,\n",
    "                 depth,\n",
    "                 row_dim,\n",
    "                 col_dim,\n",
    "                 heads,\n",
    "                 dim_head,\n",
    "                 classifier_input,  # ç°åœ¨æ˜¯ä¸€ä¸ªæ•°ç»„ï¼ŒåŒ…å«ä¸åŒè¾“å…¥çš„ç‰¹å¾æ•°ç›®\n",
    "                 classifier_dim,\n",
    "                 label_dim,\n",
    "                 beta,\n",
    "                 attn_dropout,\n",
    "                 ff_dropout,\n",
    "                 classifier_dropout,\n",
    "                 num_view,\n",
    "                 dim_hvcdn,\n",
    "                 embeding=False,\n",
    "                 embeding_num=32\n",
    "                ):\n",
    "        super(pathformer_model, self).__init__()\n",
    "        self.Evoformer_model = Evoformer(depth=depth, dim_network=1, row_dim=row_dim,\n",
    "                                         col_dim=col_dim, heads=heads, dim_head=dim_head,\n",
    "                                         embeding=embeding, embeding_num=embeding_num,\n",
    "                                         beta=beta, attn_dropout=attn_dropout, ff_dropout=ff_dropout)\n",
    "        \n",
    "        # åˆå§‹åŒ– Graph_model æ—¶ä¸å†ä½¿ç”¨å•ä¸€çš„ classifier_inputï¼Œè€Œæ˜¯ä½¿ç”¨ä¸åŒè¾“å…¥çš„ç‰¹å¾æ•°ç›®\n",
    "        self.Graph_model_1 = Graph(classifier_input[0], classifier_dim[0], classifier_dropout, label_dim)  # å¯¹åº” Methylation\n",
    "        self.Graph_model_2 = Graph(classifier_input[1], classifier_dim[1], classifier_dropout, label_dim)  # å¯¹åº” miRNASeq\n",
    "        self.Graph_model_3 = Graph(classifier_input[2], classifier_dim[2], classifier_dropout, label_dim)  # å¯¹åº” RNAseq\n",
    "        \n",
    "#         self.VCDN = VCDN(num_view+1, label_dim, dim_hvcdn)\n",
    "#         self.HFBSurv = HFBSurv(classifier_dim[0][-1], dim_hvcdn, label_dim, attn_dropout, 20, classifier_dropout)\n",
    "        self.CMFM = CMFM(classifier_dim[0][-1], label_dim, classifier_dropout, dim_hvcdn)\n",
    "        self.TCP = TCP(classifier_input, [classifier_dim[0][-1]], label_dim, classifier_dropout)\n",
    "\n",
    "    def forward(self, Methylation, miRNASeq, RNAseq, graph, label, output_attentions):\n",
    "        # è·å– Evoformer æ¨¡å‹çš„è¾“å‡º\n",
    "        if output_attentions:\n",
    "            m, attn_out_row_1_list, attn_out_row_2_list, attn_out_col_1_list, attn_out_col_2_list = self.Evoformer_model(graph, output_attentions=output_attentions)\n",
    "        else:\n",
    "            m = self.Evoformer_model(graph, output_attentions=output_attentions)\n",
    "        \n",
    "        # æ‹†åˆ† m\n",
    "        m_splits = torch.unbind(m, dim=1)\n",
    "#         # æ‹†åˆ† m\n",
    "#         m_splits = torch.unbind(graph, dim=1)\n",
    "\n",
    "        # å°†æ¯ä¸ªåˆ‡åˆ†åçš„éƒ¨åˆ†åˆ†åˆ«ä¼ å…¥å¯¹åº”çš„ Graph_model\n",
    "        dec_logits1 = self.Graph_model_1(Methylation, m_splits[0])  # Methylation å¯¹åº”çš„ Graph_model\n",
    "        dec_logits2 = self.Graph_model_2(miRNASeq, m_splits[1])         # miRNASeq å¯¹åº”çš„ Graph_model\n",
    "        dec_logits3 = self.Graph_model_3(RNAseq, m_splits[2])       # RNAseq å¯¹åº”çš„ Graph_model\n",
    "        # å°†å››ä¸ªç»“æœåˆæˆåˆ—è¡¨\n",
    "#         result_list = [dec_logits1, dec_logits2, dec_logits3]\n",
    "        \n",
    "        # å°†ç»“æœä¼ å…¥ VCDN æ¨¡å—ç”Ÿæˆæœ€ç»ˆè¾“å‡º\n",
    "#         output = self.VCDN(result_list)\n",
    "#         output = self.VCDN(dec_logits1, dec_logits2, dec_logits3)\n",
    "#         output = self.HFBSurv(dec_logits1, dec_logits2, dec_logits3)\n",
    "        output, _, _, _ = self.CMFM(dec_logits1, dec_logits2, dec_logits3)\n",
    "#       å°†å››ä¸ªç»“æœåˆæˆåˆ—è¡¨\n",
    "        result_list = [output, dec_logits1, dec_logits2, dec_logits3]\n",
    "#         output = self.VCDN(result_list)\n",
    "        label = label.squeeze()\n",
    "        loss, output, feature = self.TCP(result_list, label)\n",
    "    \n",
    "        if output_attentions:\n",
    "            return output, attn_out_row_1_list, attn_out_row_2_list, attn_out_col_1_list, attn_out_col_2_list, m\n",
    "        else:\n",
    "            return loss, output, feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417acd80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimoding",
   "language": "python",
   "name": "multimoding"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
