{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b87dbca5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:38.533999Z",
     "iopub.status.busy": "2025-05-09T13:26:38.533670Z",
     "iopub.status.idle": "2025-05-09T13:26:41.672228Z",
     "shell.execute_reply": "2025-05-09T13:26:41.671593Z"
    },
    "papermill": {
     "duration": 3.145536,
     "end_time": "2025-05-09T13:26:41.673258",
     "exception": false,
     "start_time": "2025-05-09T13:26:38.527722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "import torch.nn.functional as F\n",
    "import nbimporter\n",
    "from network import *\n",
    "import os\n",
    "from utils import *\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66ceb4f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:41.693702Z",
     "iopub.status.busy": "2025-05-09T13:26:41.693454Z",
     "iopub.status.idle": "2025-05-09T13:26:41.699963Z",
     "shell.execute_reply": "2025-05-09T13:26:41.699528Z"
    },
    "papermill": {
     "duration": 0.023244,
     "end_time": "2025-05-09T13:26:41.700641",
     "exception": false,
     "start_time": "2025-05-09T13:26:41.677397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed=42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "446fb3db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:41.708991Z",
     "iopub.status.busy": "2025-05-09T13:26:41.708771Z",
     "iopub.status.idle": "2025-05-09T13:26:41.711249Z",
     "shell.execute_reply": "2025-05-09T13:26:41.710843Z"
    },
    "papermill": {
     "duration": 0.007345,
     "end_time": "2025-05-09T13:26:41.711900",
     "exception": false,
     "start_time": "2025-05-09T13:26:41.704555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_class=4\n",
    "num_epoch=2500\n",
    "test_inverval = 50\n",
    "num_view = 3\n",
    "dim_hvcdn= 64\n",
    "batch_size = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61bf9054",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:41.722741Z",
     "iopub.status.busy": "2025-05-09T13:26:41.722454Z",
     "iopub.status.idle": "2025-05-09T13:26:41.813143Z",
     "shell.execute_reply": "2025-05-09T13:26:41.812665Z"
    },
    "papermill": {
     "duration": 0.095822,
     "end_time": "2025-05-09T13:26:41.813886",
     "exception": false,
     "start_time": "2025-05-09T13:26:41.718064",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0752463054187192</th>\n",
       "      <th>0.9911882142365416</th>\n",
       "      <th>0.6800423784929148</th>\n",
       "      <th>0.969684935133704</th>\n",
       "      <th>0.9089919103920348</th>\n",
       "      <th>0.046335020911292</th>\n",
       "      <th>0.032830429239269</th>\n",
       "      <th>0.983728813559322</th>\n",
       "      <th>0.9732649498035792</th>\n",
       "      <th>0.108658355255633</th>\n",
       "      <th>...</th>\n",
       "      <th>0.2962497141550423</th>\n",
       "      <th>0.1931479642502482</th>\n",
       "      <th>0.8875406925125778</th>\n",
       "      <th>0.0380033357159876</th>\n",
       "      <th>0.9659176975511234</th>\n",
       "      <th>0.94608195542775</th>\n",
       "      <th>0.9141064474253572</th>\n",
       "      <th>0.6571871040405464</th>\n",
       "      <th>0.527659005879209</th>\n",
       "      <th>0.7971216341689878</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.322167</td>\n",
       "      <td>0.739777</td>\n",
       "      <td>0.968878</td>\n",
       "      <td>0.505030</td>\n",
       "      <td>0.769757</td>\n",
       "      <td>0.420977</td>\n",
       "      <td>0.567255</td>\n",
       "      <td>0.902486</td>\n",
       "      <td>0.713771</td>\n",
       "      <td>0.970948</td>\n",
       "      <td>...</td>\n",
       "      <td>0.968100</td>\n",
       "      <td>0.208540</td>\n",
       "      <td>0.802012</td>\n",
       "      <td>0.930665</td>\n",
       "      <td>0.862282</td>\n",
       "      <td>0.787203</td>\n",
       "      <td>0.855257</td>\n",
       "      <td>0.972265</td>\n",
       "      <td>0.301042</td>\n",
       "      <td>0.684308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.786453</td>\n",
       "      <td>0.432741</td>\n",
       "      <td>0.811681</td>\n",
       "      <td>0.432883</td>\n",
       "      <td>0.484287</td>\n",
       "      <td>0.922518</td>\n",
       "      <td>0.952401</td>\n",
       "      <td>0.764520</td>\n",
       "      <td>0.643278</td>\n",
       "      <td>0.671737</td>\n",
       "      <td>...</td>\n",
       "      <td>0.795907</td>\n",
       "      <td>0.439424</td>\n",
       "      <td>0.161586</td>\n",
       "      <td>0.865857</td>\n",
       "      <td>0.574729</td>\n",
       "      <td>0.613515</td>\n",
       "      <td>0.429035</td>\n",
       "      <td>0.924257</td>\n",
       "      <td>0.094602</td>\n",
       "      <td>0.995125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.508005</td>\n",
       "      <td>0.633485</td>\n",
       "      <td>0.970335</td>\n",
       "      <td>0.701747</td>\n",
       "      <td>0.586185</td>\n",
       "      <td>0.950693</td>\n",
       "      <td>0.560561</td>\n",
       "      <td>0.578757</td>\n",
       "      <td>0.709515</td>\n",
       "      <td>0.441153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.537389</td>\n",
       "      <td>0.292949</td>\n",
       "      <td>0.598698</td>\n",
       "      <td>0.661306</td>\n",
       "      <td>0.895228</td>\n",
       "      <td>0.692451</td>\n",
       "      <td>0.655344</td>\n",
       "      <td>0.773617</td>\n",
       "      <td>0.182924</td>\n",
       "      <td>0.955896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.300246</td>\n",
       "      <td>0.628115</td>\n",
       "      <td>0.838564</td>\n",
       "      <td>0.356367</td>\n",
       "      <td>0.791693</td>\n",
       "      <td>0.768105</td>\n",
       "      <td>0.379516</td>\n",
       "      <td>0.580791</td>\n",
       "      <td>0.446639</td>\n",
       "      <td>0.541690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.452321</td>\n",
       "      <td>0.284012</td>\n",
       "      <td>0.560817</td>\n",
       "      <td>0.472719</td>\n",
       "      <td>0.833502</td>\n",
       "      <td>0.448886</td>\n",
       "      <td>0.535482</td>\n",
       "      <td>0.900324</td>\n",
       "      <td>0.412079</td>\n",
       "      <td>0.897981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.614039</td>\n",
       "      <td>0.701776</td>\n",
       "      <td>0.911138</td>\n",
       "      <td>0.509664</td>\n",
       "      <td>0.651836</td>\n",
       "      <td>0.685560</td>\n",
       "      <td>0.665427</td>\n",
       "      <td>0.880565</td>\n",
       "      <td>0.538848</td>\n",
       "      <td>0.517671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.515436</td>\n",
       "      <td>0.298908</td>\n",
       "      <td>0.479728</td>\n",
       "      <td>0.753395</td>\n",
       "      <td>0.692881</td>\n",
       "      <td>0.642847</td>\n",
       "      <td>0.421679</td>\n",
       "      <td>0.807968</td>\n",
       "      <td>0.105024</td>\n",
       "      <td>0.806175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.915887</td>\n",
       "      <td>0.607600</td>\n",
       "      <td>0.938021</td>\n",
       "      <td>0.533492</td>\n",
       "      <td>0.298538</td>\n",
       "      <td>0.928021</td>\n",
       "      <td>0.953145</td>\n",
       "      <td>0.865311</td>\n",
       "      <td>0.542012</td>\n",
       "      <td>0.931259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.822319</td>\n",
       "      <td>0.270109</td>\n",
       "      <td>0.491270</td>\n",
       "      <td>0.854658</td>\n",
       "      <td>0.353699</td>\n",
       "      <td>0.697628</td>\n",
       "      <td>0.533535</td>\n",
       "      <td>0.876813</td>\n",
       "      <td>0.190807</td>\n",
       "      <td>0.965297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.222291</td>\n",
       "      <td>0.774611</td>\n",
       "      <td>0.425904</td>\n",
       "      <td>0.544612</td>\n",
       "      <td>0.949751</td>\n",
       "      <td>0.052939</td>\n",
       "      <td>0.046111</td>\n",
       "      <td>0.963955</td>\n",
       "      <td>0.915976</td>\n",
       "      <td>0.119181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158701</td>\n",
       "      <td>0.249255</td>\n",
       "      <td>0.839006</td>\n",
       "      <td>0.037646</td>\n",
       "      <td>0.972734</td>\n",
       "      <td>0.951689</td>\n",
       "      <td>0.885980</td>\n",
       "      <td>0.364775</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.648909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.354557</td>\n",
       "      <td>0.673276</td>\n",
       "      <td>0.944378</td>\n",
       "      <td>0.458830</td>\n",
       "      <td>0.753889</td>\n",
       "      <td>0.979419</td>\n",
       "      <td>0.940395</td>\n",
       "      <td>0.871751</td>\n",
       "      <td>0.616652</td>\n",
       "      <td>0.598307</td>\n",
       "      <td>...</td>\n",
       "      <td>0.401898</td>\n",
       "      <td>0.452830</td>\n",
       "      <td>0.676532</td>\n",
       "      <td>0.519538</td>\n",
       "      <td>0.902045</td>\n",
       "      <td>0.621136</td>\n",
       "      <td>0.601904</td>\n",
       "      <td>0.741236</td>\n",
       "      <td>0.057723</td>\n",
       "      <td>0.977368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.316010</td>\n",
       "      <td>0.719813</td>\n",
       "      <td>0.820554</td>\n",
       "      <td>0.513768</td>\n",
       "      <td>0.901369</td>\n",
       "      <td>0.463790</td>\n",
       "      <td>0.124947</td>\n",
       "      <td>0.791638</td>\n",
       "      <td>0.553252</td>\n",
       "      <td>0.389683</td>\n",
       "      <td>...</td>\n",
       "      <td>0.382689</td>\n",
       "      <td>0.253227</td>\n",
       "      <td>0.566440</td>\n",
       "      <td>0.587801</td>\n",
       "      <td>0.917319</td>\n",
       "      <td>0.472178</td>\n",
       "      <td>0.315664</td>\n",
       "      <td>0.797691</td>\n",
       "      <td>0.127873</td>\n",
       "      <td>0.865367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.630911</td>\n",
       "      <td>0.799394</td>\n",
       "      <td>0.816845</td>\n",
       "      <td>0.777734</td>\n",
       "      <td>0.686994</td>\n",
       "      <td>0.750605</td>\n",
       "      <td>0.203676</td>\n",
       "      <td>0.865085</td>\n",
       "      <td>0.594173</td>\n",
       "      <td>0.439666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.466270</td>\n",
       "      <td>0.576465</td>\n",
       "      <td>0.480320</td>\n",
       "      <td>0.737312</td>\n",
       "      <td>0.795759</td>\n",
       "      <td>0.775413</td>\n",
       "      <td>0.537213</td>\n",
       "      <td>0.772350</td>\n",
       "      <td>0.124666</td>\n",
       "      <td>0.793524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>301 rows × 1452 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0.0752463054187192  0.9911882142365416  0.6800423784929148  \\\n",
       "0              0.322167            0.739777            0.968878   \n",
       "1              0.786453            0.432741            0.811681   \n",
       "2              0.508005            0.633485            0.970335   \n",
       "3              0.300246            0.628115            0.838564   \n",
       "4              0.614039            0.701776            0.911138   \n",
       "..                  ...                 ...                 ...   \n",
       "296            0.915887            0.607600            0.938021   \n",
       "297            0.222291            0.774611            0.425904   \n",
       "298            0.354557            0.673276            0.944378   \n",
       "299            0.316010            0.719813            0.820554   \n",
       "300            0.630911            0.799394            0.816845   \n",
       "\n",
       "     0.969684935133704  0.9089919103920348  0.046335020911292  \\\n",
       "0             0.505030            0.769757           0.420977   \n",
       "1             0.432883            0.484287           0.922518   \n",
       "2             0.701747            0.586185           0.950693   \n",
       "3             0.356367            0.791693           0.768105   \n",
       "4             0.509664            0.651836           0.685560   \n",
       "..                 ...                 ...                ...   \n",
       "296           0.533492            0.298538           0.928021   \n",
       "297           0.544612            0.949751           0.052939   \n",
       "298           0.458830            0.753889           0.979419   \n",
       "299           0.513768            0.901369           0.463790   \n",
       "300           0.777734            0.686994           0.750605   \n",
       "\n",
       "     0.032830429239269  0.983728813559322  0.9732649498035792  \\\n",
       "0             0.567255           0.902486            0.713771   \n",
       "1             0.952401           0.764520            0.643278   \n",
       "2             0.560561           0.578757            0.709515   \n",
       "3             0.379516           0.580791            0.446639   \n",
       "4             0.665427           0.880565            0.538848   \n",
       "..                 ...                ...                 ...   \n",
       "296           0.953145           0.865311            0.542012   \n",
       "297           0.046111           0.963955            0.915976   \n",
       "298           0.940395           0.871751            0.616652   \n",
       "299           0.124947           0.791638            0.553252   \n",
       "300           0.203676           0.865085            0.594173   \n",
       "\n",
       "     0.108658355255633  ...  0.2962497141550423  0.1931479642502482  \\\n",
       "0             0.970948  ...            0.968100            0.208540   \n",
       "1             0.671737  ...            0.795907            0.439424   \n",
       "2             0.441153  ...            0.537389            0.292949   \n",
       "3             0.541690  ...            0.452321            0.284012   \n",
       "4             0.517671  ...            0.515436            0.298908   \n",
       "..                 ...  ...                 ...                 ...   \n",
       "296           0.931259  ...            0.822319            0.270109   \n",
       "297           0.119181  ...            0.158701            0.249255   \n",
       "298           0.598307  ...            0.401898            0.452830   \n",
       "299           0.389683  ...            0.382689            0.253227   \n",
       "300           0.439666  ...            0.466270            0.576465   \n",
       "\n",
       "     0.8875406925125778  0.0380033357159876  0.9659176975511234  \\\n",
       "0              0.802012            0.930665            0.862282   \n",
       "1              0.161586            0.865857            0.574729   \n",
       "2              0.598698            0.661306            0.895228   \n",
       "3              0.560817            0.472719            0.833502   \n",
       "4              0.479728            0.753395            0.692881   \n",
       "..                  ...                 ...                 ...   \n",
       "296            0.491270            0.854658            0.353699   \n",
       "297            0.839006            0.037646            0.972734   \n",
       "298            0.676532            0.519538            0.902045   \n",
       "299            0.566440            0.587801            0.917319   \n",
       "300            0.480320            0.737312            0.795759   \n",
       "\n",
       "     0.94608195542775  0.9141064474253572  0.6571871040405464  \\\n",
       "0            0.787203            0.855257            0.972265   \n",
       "1            0.613515            0.429035            0.924257   \n",
       "2            0.692451            0.655344            0.773617   \n",
       "3            0.448886            0.535482            0.900324   \n",
       "4            0.642847            0.421679            0.807968   \n",
       "..                ...                 ...                 ...   \n",
       "296          0.697628            0.533535            0.876813   \n",
       "297          0.951689            0.885980            0.364775   \n",
       "298          0.621136            0.601904            0.741236   \n",
       "299          0.472178            0.315664            0.797691   \n",
       "300          0.775413            0.537213            0.772350   \n",
       "\n",
       "     0.527659005879209  0.7971216341689878  \n",
       "0             0.301042            0.684308  \n",
       "1             0.094602            0.995125  \n",
       "2             0.182924            0.955896  \n",
       "3             0.412079            0.897981  \n",
       "4             0.105024            0.806175  \n",
       "..                 ...                 ...  \n",
       "296           0.190807            0.965297  \n",
       "297           1.000000            0.648909  \n",
       "298           0.057723            0.977368  \n",
       "299           0.127873            0.865367  \n",
       "300           0.124666            0.793524  \n",
       "\n",
       "[301 rows x 1452 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取制表符分隔的CSV文件\n",
    "Methylation_train = pd.read_csv('../../DATA/BLCA_parting/1_tr.csv')\n",
    "# 显示 DataFrame\n",
    "Methylation_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96898d81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:41.834451Z",
     "iopub.status.busy": "2025-05-09T13:26:41.834208Z",
     "iopub.status.idle": "2025-05-09T13:26:41.883485Z",
     "shell.execute_reply": "2025-05-09T13:26:41.883026Z"
    },
    "papermill": {
     "duration": 0.065661,
     "end_time": "2025-05-09T13:26:41.884141",
     "exception": false,
     "start_time": "2025-05-09T13:26:41.818480",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.8200738916256156</th>\n",
       "      <th>0.499793473771169</th>\n",
       "      <th>0.7425506555423123</th>\n",
       "      <th>0.3591474715382579</th>\n",
       "      <th>0.6029869321717485</th>\n",
       "      <th>0.8518600044023772</th>\n",
       "      <th>0.7771993200169997</th>\n",
       "      <th>0.6792090395480226</th>\n",
       "      <th>0.5947184635530336</th>\n",
       "      <th>0.6680773189980557</th>\n",
       "      <th>...</th>\n",
       "      <th>0.7537159844500342</th>\n",
       "      <th>0.2452830188679242</th>\n",
       "      <th>0.1787511097957975</th>\n",
       "      <th>0.7158684774839171</th>\n",
       "      <th>0.3797020954304468</th>\n",
       "      <th>0.4990654205607476</th>\n",
       "      <th>0.1867157074859368</th>\n",
       "      <th>0.8751231873856118</th>\n",
       "      <th>0.14470871191876</th>\n",
       "      <th>0.8487697307335189</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.109606</td>\n",
       "      <td>0.888751</td>\n",
       "      <td>0.674877</td>\n",
       "      <td>0.731136</td>\n",
       "      <td>0.734599</td>\n",
       "      <td>0.800242</td>\n",
       "      <td>0.084254</td>\n",
       "      <td>0.517175</td>\n",
       "      <td>0.596683</td>\n",
       "      <td>0.259522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174365</td>\n",
       "      <td>0.111718</td>\n",
       "      <td>0.730098</td>\n",
       "      <td>0.085895</td>\n",
       "      <td>0.723933</td>\n",
       "      <td>0.737599</td>\n",
       "      <td>0.894202</td>\n",
       "      <td>0.498663</td>\n",
       "      <td>0.159808</td>\n",
       "      <td>0.638812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.233744</td>\n",
       "      <td>0.771720</td>\n",
       "      <td>0.436101</td>\n",
       "      <td>0.864046</td>\n",
       "      <td>0.923460</td>\n",
       "      <td>0.484922</td>\n",
       "      <td>0.270506</td>\n",
       "      <td>0.925424</td>\n",
       "      <td>0.880838</td>\n",
       "      <td>0.288116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236108</td>\n",
       "      <td>0.406157</td>\n",
       "      <td>0.678899</td>\n",
       "      <td>0.559209</td>\n",
       "      <td>0.944585</td>\n",
       "      <td>0.817541</td>\n",
       "      <td>0.724794</td>\n",
       "      <td>0.861608</td>\n",
       "      <td>0.151390</td>\n",
       "      <td>0.685469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.246429</td>\n",
       "      <td>0.942723</td>\n",
       "      <td>0.978149</td>\n",
       "      <td>0.771909</td>\n",
       "      <td>0.981487</td>\n",
       "      <td>0.203170</td>\n",
       "      <td>0.048874</td>\n",
       "      <td>0.898870</td>\n",
       "      <td>0.857704</td>\n",
       "      <td>0.377788</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255888</td>\n",
       "      <td>0.395233</td>\n",
       "      <td>0.851435</td>\n",
       "      <td>0.918037</td>\n",
       "      <td>0.943449</td>\n",
       "      <td>0.907692</td>\n",
       "      <td>0.857205</td>\n",
       "      <td>0.712657</td>\n",
       "      <td>0.118520</td>\n",
       "      <td>0.749768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.109236</td>\n",
       "      <td>0.934325</td>\n",
       "      <td>0.922262</td>\n",
       "      <td>0.926794</td>\n",
       "      <td>0.923927</td>\n",
       "      <td>0.250275</td>\n",
       "      <td>0.064386</td>\n",
       "      <td>0.507910</td>\n",
       "      <td>0.891205</td>\n",
       "      <td>0.118838</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098902</td>\n",
       "      <td>0.110228</td>\n",
       "      <td>0.693104</td>\n",
       "      <td>0.038122</td>\n",
       "      <td>0.872507</td>\n",
       "      <td>0.839396</td>\n",
       "      <td>0.815015</td>\n",
       "      <td>0.125862</td>\n",
       "      <td>0.107296</td>\n",
       "      <td>0.293756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.820936</td>\n",
       "      <td>0.509018</td>\n",
       "      <td>0.916038</td>\n",
       "      <td>0.506222</td>\n",
       "      <td>0.459241</td>\n",
       "      <td>0.943980</td>\n",
       "      <td>0.778687</td>\n",
       "      <td>0.824181</td>\n",
       "      <td>0.503165</td>\n",
       "      <td>0.613519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.827350</td>\n",
       "      <td>0.254220</td>\n",
       "      <td>0.278485</td>\n",
       "      <td>0.713128</td>\n",
       "      <td>0.674451</td>\n",
       "      <td>0.536161</td>\n",
       "      <td>0.268066</td>\n",
       "      <td>0.915388</td>\n",
       "      <td>0.024586</td>\n",
       "      <td>0.948932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.513177</td>\n",
       "      <td>0.917114</td>\n",
       "      <td>0.685207</td>\n",
       "      <td>0.369473</td>\n",
       "      <td>0.885190</td>\n",
       "      <td>0.115672</td>\n",
       "      <td>0.091585</td>\n",
       "      <td>0.978305</td>\n",
       "      <td>0.849520</td>\n",
       "      <td>0.116322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.378459</td>\n",
       "      <td>0.121648</td>\n",
       "      <td>0.774489</td>\n",
       "      <td>0.097689</td>\n",
       "      <td>0.960237</td>\n",
       "      <td>0.870165</td>\n",
       "      <td>0.839031</td>\n",
       "      <td>0.494580</td>\n",
       "      <td>0.077098</td>\n",
       "      <td>0.339949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.290394</td>\n",
       "      <td>0.330029</td>\n",
       "      <td>0.873659</td>\n",
       "      <td>0.685597</td>\n",
       "      <td>0.906970</td>\n",
       "      <td>0.954876</td>\n",
       "      <td>0.102635</td>\n",
       "      <td>0.963277</td>\n",
       "      <td>0.780554</td>\n",
       "      <td>0.721606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.628630</td>\n",
       "      <td>0.251738</td>\n",
       "      <td>0.708790</td>\n",
       "      <td>0.175959</td>\n",
       "      <td>0.908988</td>\n",
       "      <td>0.838821</td>\n",
       "      <td>0.737343</td>\n",
       "      <td>0.872167</td>\n",
       "      <td>0.079236</td>\n",
       "      <td>0.976091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.401970</td>\n",
       "      <td>0.672587</td>\n",
       "      <td>0.384585</td>\n",
       "      <td>0.302224</td>\n",
       "      <td>0.803049</td>\n",
       "      <td>0.231015</td>\n",
       "      <td>0.147471</td>\n",
       "      <td>0.898305</td>\n",
       "      <td>0.706787</td>\n",
       "      <td>0.909184</td>\n",
       "      <td>...</td>\n",
       "      <td>0.954493</td>\n",
       "      <td>0.361470</td>\n",
       "      <td>0.671796</td>\n",
       "      <td>0.613057</td>\n",
       "      <td>0.895102</td>\n",
       "      <td>0.777426</td>\n",
       "      <td>0.109260</td>\n",
       "      <td>0.725609</td>\n",
       "      <td>0.316542</td>\n",
       "      <td>0.935817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.337562</td>\n",
       "      <td>0.782459</td>\n",
       "      <td>0.877765</td>\n",
       "      <td>0.784088</td>\n",
       "      <td>0.670504</td>\n",
       "      <td>0.945741</td>\n",
       "      <td>0.328836</td>\n",
       "      <td>0.632542</td>\n",
       "      <td>0.650153</td>\n",
       "      <td>0.510122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.476904</td>\n",
       "      <td>0.603774</td>\n",
       "      <td>0.767091</td>\n",
       "      <td>0.552657</td>\n",
       "      <td>0.843221</td>\n",
       "      <td>0.724515</td>\n",
       "      <td>0.803332</td>\n",
       "      <td>0.851471</td>\n",
       "      <td>0.121459</td>\n",
       "      <td>0.957869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.366995</td>\n",
       "      <td>0.824315</td>\n",
       "      <td>0.466826</td>\n",
       "      <td>0.783956</td>\n",
       "      <td>0.889857</td>\n",
       "      <td>0.373542</td>\n",
       "      <td>0.651827</td>\n",
       "      <td>0.971186</td>\n",
       "      <td>0.849411</td>\n",
       "      <td>0.533913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280357</td>\n",
       "      <td>0.177756</td>\n",
       "      <td>0.662030</td>\n",
       "      <td>0.520133</td>\n",
       "      <td>0.920980</td>\n",
       "      <td>0.769806</td>\n",
       "      <td>0.821939</td>\n",
       "      <td>0.780374</td>\n",
       "      <td>0.663415</td>\n",
       "      <td>0.636142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1452 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0.8200738916256156  0.499793473771169  0.7425506555423123  \\\n",
       "0             0.109606           0.888751            0.674877   \n",
       "1             0.233744           0.771720            0.436101   \n",
       "2             0.246429           0.942723            0.978149   \n",
       "3             0.109236           0.934325            0.922262   \n",
       "4             0.820936           0.509018            0.916038   \n",
       "..                 ...                ...                 ...   \n",
       "95            0.513177           0.917114            0.685207   \n",
       "96            0.290394           0.330029            0.873659   \n",
       "97            0.401970           0.672587            0.384585   \n",
       "98            0.337562           0.782459            0.877765   \n",
       "99            0.366995           0.824315            0.466826   \n",
       "\n",
       "    0.3591474715382579  0.6029869321717485  0.8518600044023772  \\\n",
       "0             0.731136            0.734599            0.800242   \n",
       "1             0.864046            0.923460            0.484922   \n",
       "2             0.771909            0.981487            0.203170   \n",
       "3             0.926794            0.923927            0.250275   \n",
       "4             0.506222            0.459241            0.943980   \n",
       "..                 ...                 ...                 ...   \n",
       "95            0.369473            0.885190            0.115672   \n",
       "96            0.685597            0.906970            0.954876   \n",
       "97            0.302224            0.803049            0.231015   \n",
       "98            0.784088            0.670504            0.945741   \n",
       "99            0.783956            0.889857            0.373542   \n",
       "\n",
       "    0.7771993200169997  0.6792090395480226  0.5947184635530336  \\\n",
       "0             0.084254            0.517175            0.596683   \n",
       "1             0.270506            0.925424            0.880838   \n",
       "2             0.048874            0.898870            0.857704   \n",
       "3             0.064386            0.507910            0.891205   \n",
       "4             0.778687            0.824181            0.503165   \n",
       "..                 ...                 ...                 ...   \n",
       "95            0.091585            0.978305            0.849520   \n",
       "96            0.102635            0.963277            0.780554   \n",
       "97            0.147471            0.898305            0.706787   \n",
       "98            0.328836            0.632542            0.650153   \n",
       "99            0.651827            0.971186            0.849411   \n",
       "\n",
       "    0.6680773189980557  ...  0.7537159844500342  0.2452830188679242  \\\n",
       "0             0.259522  ...            0.174365            0.111718   \n",
       "1             0.288116  ...            0.236108            0.406157   \n",
       "2             0.377788  ...            0.255888            0.395233   \n",
       "3             0.118838  ...            0.098902            0.110228   \n",
       "4             0.613519  ...            0.827350            0.254220   \n",
       "..                 ...  ...                 ...                 ...   \n",
       "95            0.116322  ...            0.378459            0.121648   \n",
       "96            0.721606  ...            0.628630            0.251738   \n",
       "97            0.909184  ...            0.954493            0.361470   \n",
       "98            0.510122  ...            0.476904            0.603774   \n",
       "99            0.533913  ...            0.280357            0.177756   \n",
       "\n",
       "    0.1787511097957975  0.7158684774839171  0.3797020954304468  \\\n",
       "0             0.730098            0.085895            0.723933   \n",
       "1             0.678899            0.559209            0.944585   \n",
       "2             0.851435            0.918037            0.943449   \n",
       "3             0.693104            0.038122            0.872507   \n",
       "4             0.278485            0.713128            0.674451   \n",
       "..                 ...                 ...                 ...   \n",
       "95            0.774489            0.097689            0.960237   \n",
       "96            0.708790            0.175959            0.908988   \n",
       "97            0.671796            0.613057            0.895102   \n",
       "98            0.767091            0.552657            0.843221   \n",
       "99            0.662030            0.520133            0.920980   \n",
       "\n",
       "    0.4990654205607476  0.1867157074859368  0.8751231873856118  \\\n",
       "0             0.737599            0.894202            0.498663   \n",
       "1             0.817541            0.724794            0.861608   \n",
       "2             0.907692            0.857205            0.712657   \n",
       "3             0.839396            0.815015            0.125862   \n",
       "4             0.536161            0.268066            0.915388   \n",
       "..                 ...                 ...                 ...   \n",
       "95            0.870165            0.839031            0.494580   \n",
       "96            0.838821            0.737343            0.872167   \n",
       "97            0.777426            0.109260            0.725609   \n",
       "98            0.724515            0.803332            0.851471   \n",
       "99            0.769806            0.821939            0.780374   \n",
       "\n",
       "    0.14470871191876  0.8487697307335189  \n",
       "0           0.159808            0.638812  \n",
       "1           0.151390            0.685469  \n",
       "2           0.118520            0.749768  \n",
       "3           0.107296            0.293756  \n",
       "4           0.024586            0.948932  \n",
       "..               ...                 ...  \n",
       "95          0.077098            0.339949  \n",
       "96          0.079236            0.976091  \n",
       "97          0.316542            0.935817  \n",
       "98          0.121459            0.957869  \n",
       "99          0.663415            0.636142  \n",
       "\n",
       "[100 rows x 1452 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取制表符分隔的CSV文件\n",
    "Methylation_test = pd.read_csv('../../DATA/BLCA_parting/1_te.csv')\n",
    "# 显示 DataFrame\n",
    "Methylation_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "658d5fc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:41.903065Z",
     "iopub.status.busy": "2025-05-09T13:26:41.902782Z",
     "iopub.status.idle": "2025-05-09T13:26:41.904964Z",
     "shell.execute_reply": "2025-05-09T13:26:41.904539Z"
    },
    "papermill": {
     "duration": 0.016879,
     "end_time": "2025-05-09T13:26:41.905595",
     "exception": false,
     "start_time": "2025-05-09T13:26:41.888716",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 读取制表符分隔的CSV文件\n",
    "# Methylation_val = pd.read_csv('../../DATA/BLCA_parting/methylation_val.csv')\n",
    "# # 显示 DataFrame\n",
    "# Methylation_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04e4836d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:41.927884Z",
     "iopub.status.busy": "2025-05-09T13:26:41.927442Z",
     "iopub.status.idle": "2025-05-09T13:26:41.945654Z",
     "shell.execute_reply": "2025-05-09T13:26:41.945214Z"
    },
    "papermill": {
     "duration": 0.032253,
     "end_time": "2025-05-09T13:26:41.946400",
     "exception": false,
     "start_time": "2025-05-09T13:26:41.914147",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0532462506047412</th>\n",
       "      <th>0.6213407206570001</th>\n",
       "      <th>0.5117845583556946</th>\n",
       "      <th>0.7412033357304164</th>\n",
       "      <th>0.5261464038699875</th>\n",
       "      <th>0.7201409811835597</th>\n",
       "      <th>0.2506968964876417</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.1802774662617658</th>\n",
       "      <th>0.4441372631710479</th>\n",
       "      <th>...</th>\n",
       "      <th>0.5514555973918632</th>\n",
       "      <th>0.0.2</th>\n",
       "      <th>0.1591204831336959</th>\n",
       "      <th>0.0694270464571646</th>\n",
       "      <th>0.5643671444733146</th>\n",
       "      <th>0.2063787901732152</th>\n",
       "      <th>0.0.3</th>\n",
       "      <th>0.4564890760856177</th>\n",
       "      <th>0.1791856570896179</th>\n",
       "      <th>0.1370675521861261</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.278616</td>\n",
       "      <td>0.439644</td>\n",
       "      <td>0.585549</td>\n",
       "      <td>0.828293</td>\n",
       "      <td>0.591552</td>\n",
       "      <td>0.691661</td>\n",
       "      <td>0.553106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.304608</td>\n",
       "      <td>0.458382</td>\n",
       "      <td>...</td>\n",
       "      <td>0.456387</td>\n",
       "      <td>0.174968</td>\n",
       "      <td>0.450202</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.136678</td>\n",
       "      <td>0.498864</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.521675</td>\n",
       "      <td>0.425050</td>\n",
       "      <td>0.407370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.635172</td>\n",
       "      <td>0.301839</td>\n",
       "      <td>0.422875</td>\n",
       "      <td>0.570733</td>\n",
       "      <td>0.415744</td>\n",
       "      <td>0.335361</td>\n",
       "      <td>0.598196</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.482970</td>\n",
       "      <td>0.504069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.473781</td>\n",
       "      <td>0.049656</td>\n",
       "      <td>0.539766</td>\n",
       "      <td>0.831314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311380</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.325427</td>\n",
       "      <td>0.502925</td>\n",
       "      <td>0.489051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.527054</td>\n",
       "      <td>0.343373</td>\n",
       "      <td>0.434881</td>\n",
       "      <td>0.533603</td>\n",
       "      <td>0.381781</td>\n",
       "      <td>0.281185</td>\n",
       "      <td>0.352899</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.354355</td>\n",
       "      <td>0.495666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248673</td>\n",
       "      <td>0.046016</td>\n",
       "      <td>0.425144</td>\n",
       "      <td>0.199635</td>\n",
       "      <td>0.056106</td>\n",
       "      <td>0.113924</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.230732</td>\n",
       "      <td>0.331092</td>\n",
       "      <td>0.353895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.259816</td>\n",
       "      <td>0.214468</td>\n",
       "      <td>0.626241</td>\n",
       "      <td>0.714003</td>\n",
       "      <td>0.621317</td>\n",
       "      <td>0.598924</td>\n",
       "      <td>0.443842</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.386852</td>\n",
       "      <td>0.645456</td>\n",
       "      <td>...</td>\n",
       "      <td>0.577519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349115</td>\n",
       "      <td>0.190989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.366302</td>\n",
       "      <td>0.364675</td>\n",
       "      <td>0.233117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.471224</td>\n",
       "      <td>0.238654</td>\n",
       "      <td>0.324850</td>\n",
       "      <td>0.408501</td>\n",
       "      <td>0.164794</td>\n",
       "      <td>0.310860</td>\n",
       "      <td>0.309226</td>\n",
       "      <td>0.029163</td>\n",
       "      <td>0.198233</td>\n",
       "      <td>0.449948</td>\n",
       "      <td>...</td>\n",
       "      <td>0.211002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.306696</td>\n",
       "      <td>0.702957</td>\n",
       "      <td>0.090742</td>\n",
       "      <td>0.091535</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.143045</td>\n",
       "      <td>0.335608</td>\n",
       "      <td>0.262001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.388592</td>\n",
       "      <td>0.022909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.271804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142753</td>\n",
       "      <td>0.223482</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007443</td>\n",
       "      <td>0.329485</td>\n",
       "      <td>0.596781</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.298215</td>\n",
       "      <td>0.164040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.234649</td>\n",
       "      <td>0.538371</td>\n",
       "      <td>0.317326</td>\n",
       "      <td>0.545564</td>\n",
       "      <td>0.605125</td>\n",
       "      <td>0.600970</td>\n",
       "      <td>0.215427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.109260</td>\n",
       "      <td>0.292489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419781</td>\n",
       "      <td>0.044486</td>\n",
       "      <td>0.141314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.174252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.398926</td>\n",
       "      <td>0.056052</td>\n",
       "      <td>0.132682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.426144</td>\n",
       "      <td>0.394907</td>\n",
       "      <td>0.501950</td>\n",
       "      <td>0.648953</td>\n",
       "      <td>0.349058</td>\n",
       "      <td>0.333731</td>\n",
       "      <td>0.516232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.387546</td>\n",
       "      <td>0.509551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.466930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.483579</td>\n",
       "      <td>0.057159</td>\n",
       "      <td>0.036426</td>\n",
       "      <td>0.315270</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458127</td>\n",
       "      <td>0.432461</td>\n",
       "      <td>0.384334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.313875</td>\n",
       "      <td>0.756533</td>\n",
       "      <td>0.866373</td>\n",
       "      <td>0.842701</td>\n",
       "      <td>0.791588</td>\n",
       "      <td>0.393572</td>\n",
       "      <td>0.465566</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.376583</td>\n",
       "      <td>0.511707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.911415</td>\n",
       "      <td>0.067112</td>\n",
       "      <td>0.343666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.789637</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.761201</td>\n",
       "      <td>0.348408</td>\n",
       "      <td>0.274939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.414969</td>\n",
       "      <td>0.524107</td>\n",
       "      <td>0.144527</td>\n",
       "      <td>0.289568</td>\n",
       "      <td>0.490265</td>\n",
       "      <td>0.645728</td>\n",
       "      <td>0.553478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.330628</td>\n",
       "      <td>0.398904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.621985</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.472323</td>\n",
       "      <td>0.245697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.268315</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.255684</td>\n",
       "      <td>0.395034</td>\n",
       "      <td>0.385619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>301 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0.0532462506047412  0.6213407206570001  0.5117845583556946  \\\n",
       "0              0.278616            0.439644            0.585549   \n",
       "1              0.635172            0.301839            0.422875   \n",
       "2              0.527054            0.343373            0.434881   \n",
       "3              0.259816            0.214468            0.626241   \n",
       "4              0.471224            0.238654            0.324850   \n",
       "..                  ...                 ...                 ...   \n",
       "296            0.388592            0.022909            0.000000   \n",
       "297            0.234649            0.538371            0.317326   \n",
       "298            0.426144            0.394907            0.501950   \n",
       "299            0.313875            0.756533            0.866373   \n",
       "300            0.414969            0.524107            0.144527   \n",
       "\n",
       "     0.7412033357304164  0.5261464038699875  0.7201409811835597  \\\n",
       "0              0.828293            0.591552            0.691661   \n",
       "1              0.570733            0.415744            0.335361   \n",
       "2              0.533603            0.381781            0.281185   \n",
       "3              0.714003            0.621317            0.598924   \n",
       "4              0.408501            0.164794            0.310860   \n",
       "..                  ...                 ...                 ...   \n",
       "296            0.000000            0.000000            0.000000   \n",
       "297            0.545564            0.605125            0.600970   \n",
       "298            0.648953            0.349058            0.333731   \n",
       "299            0.842701            0.791588            0.393572   \n",
       "300            0.289568            0.490265            0.645728   \n",
       "\n",
       "     0.2506968964876417       0.0  0.1802774662617658  0.4441372631710479  \\\n",
       "0              0.553106  0.000000            0.304608            0.458382   \n",
       "1              0.598196  0.000000            0.482970            0.504069   \n",
       "2              0.352899  0.000000            0.354355            0.495666   \n",
       "3              0.443842  0.000000            0.386852            0.645456   \n",
       "4              0.309226  0.029163            0.198233            0.449948   \n",
       "..                  ...       ...                 ...                 ...   \n",
       "296            0.271804  0.000000            0.142753            0.223482   \n",
       "297            0.215427  0.000000            0.109260            0.292489   \n",
       "298            0.516232  0.000000            0.387546            0.509551   \n",
       "299            0.465566  0.000000            0.376583            0.511707   \n",
       "300            0.553478  0.000000            0.330628            0.398904   \n",
       "\n",
       "     ...  0.5514555973918632     0.0.2  0.1591204831336959  \\\n",
       "0    ...            0.456387  0.174968            0.450202   \n",
       "1    ...            0.473781  0.049656            0.539766   \n",
       "2    ...            0.248673  0.046016            0.425144   \n",
       "3    ...            0.577519  0.000000            0.349115   \n",
       "4    ...            0.211002  0.000000            0.306696   \n",
       "..   ...                 ...       ...                 ...   \n",
       "296  ...            0.000000  0.007443            0.329485   \n",
       "297  ...            0.419781  0.044486            0.141314   \n",
       "298  ...            0.466930  0.000000            0.483579   \n",
       "299  ...            0.911415  0.067112            0.343666   \n",
       "300  ...            0.621985  0.000000            0.472323   \n",
       "\n",
       "     0.0694270464571646  0.5643671444733146  0.2063787901732152  0.0.3  \\\n",
       "0              0.000000            0.136678            0.498864    0.0   \n",
       "1              0.831314            0.000000            0.311380    0.0   \n",
       "2              0.199635            0.056106            0.113924    0.0   \n",
       "3              0.190989            0.000000            0.317274    0.0   \n",
       "4              0.702957            0.090742            0.091535    0.0   \n",
       "..                  ...                 ...                 ...    ...   \n",
       "296            0.596781            0.000000            0.000000    0.0   \n",
       "297            0.000000            0.000000            0.174252    0.0   \n",
       "298            0.057159            0.036426            0.315270    0.0   \n",
       "299            0.000000            0.000000            0.789637    0.0   \n",
       "300            0.245697            0.000000            0.268315    0.0   \n",
       "\n",
       "     0.4564890760856177  0.1791856570896179  0.1370675521861261  \n",
       "0              0.521675            0.425050            0.407370  \n",
       "1              0.325427            0.502925            0.489051  \n",
       "2              0.230732            0.331092            0.353895  \n",
       "3              0.366302            0.364675            0.233117  \n",
       "4              0.143045            0.335608            0.262001  \n",
       "..                  ...                 ...                 ...  \n",
       "296            0.000000            0.298215            0.164040  \n",
       "297            0.398926            0.056052            0.132682  \n",
       "298            0.458127            0.432461            0.384334  \n",
       "299            0.761201            0.348408            0.274939  \n",
       "300            0.255684            0.395034            0.385619  \n",
       "\n",
       "[301 rows x 39 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取制表符分隔的CSV文件\n",
    "miRNASeq_train = pd.read_csv('../../DATA/BLCA_parting/3_tr.csv')\n",
    "# 显示 DataFrame\n",
    "miRNASeq_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f744b2a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:41.963370Z",
     "iopub.status.busy": "2025-05-09T13:26:41.963112Z",
     "iopub.status.idle": "2025-05-09T13:26:41.978044Z",
     "shell.execute_reply": "2025-05-09T13:26:41.977570Z"
    },
    "papermill": {
     "duration": 0.027295,
     "end_time": "2025-05-09T13:26:41.978760",
     "exception": false,
     "start_time": "2025-05-09T13:26:41.951465",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.4608805031446541</th>\n",
       "      <th>0.1187276513811936</th>\n",
       "      <th>0.2821793133973277</th>\n",
       "      <th>0.3808586604090181</th>\n",
       "      <th>0.3314076680126334</th>\n",
       "      <th>0.3688249673128305</th>\n",
       "      <th>0.3985074188910298</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.2672975390935094</th>\n",
       "      <th>0.3747247083079518</th>\n",
       "      <th>...</th>\n",
       "      <th>0.3353659656534116</th>\n",
       "      <th>0.0151006882487891</th>\n",
       "      <th>0.2896244147119993</th>\n",
       "      <th>0.8896438102499863</th>\n",
       "      <th>0.0.1</th>\n",
       "      <th>0.1569420014677274</th>\n",
       "      <th>0.0.2</th>\n",
       "      <th>0.2102514840267</th>\n",
       "      <th>0.3648511402647327</th>\n",
       "      <th>0.3075509330325909</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.236913</td>\n",
       "      <td>0.683111</td>\n",
       "      <td>0.468740</td>\n",
       "      <td>0.715012</td>\n",
       "      <td>0.594271</td>\n",
       "      <td>0.546833</td>\n",
       "      <td>0.296492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078804</td>\n",
       "      <td>0.432501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.330554</td>\n",
       "      <td>0.035310</td>\n",
       "      <td>0.260463</td>\n",
       "      <td>0.047152</td>\n",
       "      <td>0.055385</td>\n",
       "      <td>0.154007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423508</td>\n",
       "      <td>0.242708</td>\n",
       "      <td>0.197634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.225206</td>\n",
       "      <td>0.817851</td>\n",
       "      <td>0.760215</td>\n",
       "      <td>0.861953</td>\n",
       "      <td>0.622496</td>\n",
       "      <td>0.702196</td>\n",
       "      <td>0.424290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152556</td>\n",
       "      <td>0.611952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.789825</td>\n",
       "      <td>0.041795</td>\n",
       "      <td>0.276456</td>\n",
       "      <td>0.099232</td>\n",
       "      <td>0.123053</td>\n",
       "      <td>0.396286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.524508</td>\n",
       "      <td>0.288806</td>\n",
       "      <td>0.230392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.153033</td>\n",
       "      <td>0.459311</td>\n",
       "      <td>0.739274</td>\n",
       "      <td>0.917805</td>\n",
       "      <td>0.764063</td>\n",
       "      <td>0.864874</td>\n",
       "      <td>0.384843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.248523</td>\n",
       "      <td>0.271293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.653504</td>\n",
       "      <td>0.034678</td>\n",
       "      <td>0.196001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.398919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.547464</td>\n",
       "      <td>0.269265</td>\n",
       "      <td>0.225097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.202313</td>\n",
       "      <td>0.638060</td>\n",
       "      <td>0.683836</td>\n",
       "      <td>0.901783</td>\n",
       "      <td>0.567985</td>\n",
       "      <td>0.632937</td>\n",
       "      <td>0.475210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249153</td>\n",
       "      <td>0.550458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.558711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.219913</td>\n",
       "      <td>0.164084</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520407</td>\n",
       "      <td>0.322833</td>\n",
       "      <td>0.362854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.476613</td>\n",
       "      <td>0.203819</td>\n",
       "      <td>0.512656</td>\n",
       "      <td>0.468832</td>\n",
       "      <td>0.663995</td>\n",
       "      <td>0.356034</td>\n",
       "      <td>0.541017</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.404355</td>\n",
       "      <td>0.576105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.528680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.424255</td>\n",
       "      <td>0.523692</td>\n",
       "      <td>0.086788</td>\n",
       "      <td>0.354596</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.335624</td>\n",
       "      <td>0.463954</td>\n",
       "      <td>0.403256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.200726</td>\n",
       "      <td>0.669398</td>\n",
       "      <td>0.507829</td>\n",
       "      <td>0.759434</td>\n",
       "      <td>0.550054</td>\n",
       "      <td>0.692286</td>\n",
       "      <td>0.380207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.242235</td>\n",
       "      <td>0.246786</td>\n",
       "      <td>...</td>\n",
       "      <td>0.602700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.247028</td>\n",
       "      <td>0.210392</td>\n",
       "      <td>0.028830</td>\n",
       "      <td>0.274780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.537563</td>\n",
       "      <td>0.254065</td>\n",
       "      <td>0.262440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.218423</td>\n",
       "      <td>0.319738</td>\n",
       "      <td>0.549891</td>\n",
       "      <td>0.618561</td>\n",
       "      <td>0.373926</td>\n",
       "      <td>0.435469</td>\n",
       "      <td>0.396757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263782</td>\n",
       "      <td>0.635787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.468803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284842</td>\n",
       "      <td>0.055539</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.417362</td>\n",
       "      <td>0.335977</td>\n",
       "      <td>0.275555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.421993</td>\n",
       "      <td>0.890565</td>\n",
       "      <td>0.463056</td>\n",
       "      <td>0.684569</td>\n",
       "      <td>0.593711</td>\n",
       "      <td>0.349080</td>\n",
       "      <td>0.540508</td>\n",
       "      <td>0.044321</td>\n",
       "      <td>0.429934</td>\n",
       "      <td>0.784765</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938782</td>\n",
       "      <td>0.084986</td>\n",
       "      <td>0.411431</td>\n",
       "      <td>0.140297</td>\n",
       "      <td>0.311909</td>\n",
       "      <td>0.255466</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.585372</td>\n",
       "      <td>0.446990</td>\n",
       "      <td>0.480018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.264586</td>\n",
       "      <td>0.170537</td>\n",
       "      <td>0.517054</td>\n",
       "      <td>0.704894</td>\n",
       "      <td>0.505417</td>\n",
       "      <td>0.619351</td>\n",
       "      <td>0.351275</td>\n",
       "      <td>0.028393</td>\n",
       "      <td>0.246431</td>\n",
       "      <td>0.576387</td>\n",
       "      <td>...</td>\n",
       "      <td>0.404463</td>\n",
       "      <td>0.021830</td>\n",
       "      <td>0.298811</td>\n",
       "      <td>0.029151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385278</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.374701</td>\n",
       "      <td>0.287664</td>\n",
       "      <td>0.210311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.458897</td>\n",
       "      <td>0.529903</td>\n",
       "      <td>0.720034</td>\n",
       "      <td>0.831409</td>\n",
       "      <td>0.691900</td>\n",
       "      <td>0.532867</td>\n",
       "      <td>0.364626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.154774</td>\n",
       "      <td>0.645206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.623565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.259151</td>\n",
       "      <td>0.237487</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.597027</td>\n",
       "      <td>0.406463</td>\n",
       "      <td>0.704463</td>\n",
       "      <td>0.195578</td>\n",
       "      <td>0.182137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0.4608805031446541  0.1187276513811936  0.2821793133973277  \\\n",
       "0             0.236913            0.683111            0.468740   \n",
       "1             0.225206            0.817851            0.760215   \n",
       "2             0.153033            0.459311            0.739274   \n",
       "3             0.202313            0.638060            0.683836   \n",
       "4             0.476613            0.203819            0.512656   \n",
       "..                 ...                 ...                 ...   \n",
       "95            0.200726            0.669398            0.507829   \n",
       "96            0.218423            0.319738            0.549891   \n",
       "97            0.421993            0.890565            0.463056   \n",
       "98            0.264586            0.170537            0.517054   \n",
       "99            0.458897            0.529903            0.720034   \n",
       "\n",
       "    0.3808586604090181  0.3314076680126334  0.3688249673128305  \\\n",
       "0             0.715012            0.594271            0.546833   \n",
       "1             0.861953            0.622496            0.702196   \n",
       "2             0.917805            0.764063            0.864874   \n",
       "3             0.901783            0.567985            0.632937   \n",
       "4             0.468832            0.663995            0.356034   \n",
       "..                 ...                 ...                 ...   \n",
       "95            0.759434            0.550054            0.692286   \n",
       "96            0.618561            0.373926            0.435469   \n",
       "97            0.684569            0.593711            0.349080   \n",
       "98            0.704894            0.505417            0.619351   \n",
       "99            0.831409            0.691900            0.532867   \n",
       "\n",
       "    0.3985074188910298       0.0  0.2672975390935094  0.3747247083079518  ...  \\\n",
       "0             0.296492  0.000000            0.078804            0.432501  ...   \n",
       "1             0.424290  0.000000            0.152556            0.611952  ...   \n",
       "2             0.384843  0.000000            0.248523            0.271293  ...   \n",
       "3             0.475210  0.000000            0.249153            0.550458  ...   \n",
       "4             0.541017  0.000000            0.404355            0.576105  ...   \n",
       "..                 ...       ...                 ...                 ...  ...   \n",
       "95            0.380207  0.000000            0.242235            0.246786  ...   \n",
       "96            0.396757  0.000000            0.263782            0.635787  ...   \n",
       "97            0.540508  0.044321            0.429934            0.784765  ...   \n",
       "98            0.351275  0.028393            0.246431            0.576387  ...   \n",
       "99            0.364626  0.000000            0.154774            0.645206  ...   \n",
       "\n",
       "    0.3353659656534116  0.0151006882487891  0.2896244147119993  \\\n",
       "0             0.330554            0.035310            0.260463   \n",
       "1             0.789825            0.041795            0.276456   \n",
       "2             0.653504            0.034678            0.196001   \n",
       "3             0.558711            0.000000            0.219913   \n",
       "4             0.528680            0.000000            0.424255   \n",
       "..                 ...                 ...                 ...   \n",
       "95            0.602700            0.000000            0.247028   \n",
       "96            0.468803            0.000000            0.284842   \n",
       "97            0.938782            0.084986            0.411431   \n",
       "98            0.404463            0.021830            0.298811   \n",
       "99            0.623565            0.000000            0.259151   \n",
       "\n",
       "    0.8896438102499863     0.0.1  0.1569420014677274     0.0.2  \\\n",
       "0             0.047152  0.055385            0.154007  0.000000   \n",
       "1             0.099232  0.123053            0.396286  0.000000   \n",
       "2             0.000000  0.000000            0.398919  0.000000   \n",
       "3             0.164084  0.000000            0.266824  0.000000   \n",
       "4             0.523692  0.086788            0.354596  0.000000   \n",
       "..                 ...       ...                 ...       ...   \n",
       "95            0.210392  0.028830            0.274780  0.000000   \n",
       "96            0.055539  0.000000            0.204608  0.000000   \n",
       "97            0.140297  0.311909            0.255466  0.000000   \n",
       "98            0.029151  0.000000            0.385278  0.000000   \n",
       "99            0.237487  0.000000            0.597027  0.406463   \n",
       "\n",
       "    0.2102514840267  0.3648511402647327  0.3075509330325909  \n",
       "0          0.423508            0.242708            0.197634  \n",
       "1          0.524508            0.288806            0.230392  \n",
       "2          0.547464            0.269265            0.225097  \n",
       "3          0.520407            0.322833            0.362854  \n",
       "4          0.335624            0.463954            0.403256  \n",
       "..              ...                 ...                 ...  \n",
       "95         0.537563            0.254065            0.262440  \n",
       "96         0.417362            0.335977            0.275555  \n",
       "97         0.585372            0.446990            0.480018  \n",
       "98         0.374701            0.287664            0.210311  \n",
       "99         0.704463            0.195578            0.182137  \n",
       "\n",
       "[100 rows x 39 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取制表符分隔的CSV文件\n",
    "miRNASeq_test = pd.read_csv('../../DATA/BLCA_parting/3_te.csv')\n",
    "# 显示 DataFrame\n",
    "miRNASeq_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1220144",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:41.989957Z",
     "iopub.status.busy": "2025-05-09T13:26:41.989660Z",
     "iopub.status.idle": "2025-05-09T13:26:41.991885Z",
     "shell.execute_reply": "2025-05-09T13:26:41.991456Z"
    },
    "papermill": {
     "duration": 0.00855,
     "end_time": "2025-05-09T13:26:41.992545",
     "exception": false,
     "start_time": "2025-05-09T13:26:41.983995",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 读取制表符分隔的CSV文件\n",
    "# miRNASeq_val = pd.read_csv('../../DATA/BLCA_parting/miRNASeq_val.csv')\n",
    "# # 显示 DataFrame\n",
    "# miRNASeq_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8d88d1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:42.010238Z",
     "iopub.status.busy": "2025-05-09T13:26:42.009965Z",
     "iopub.status.idle": "2025-05-09T13:26:42.151268Z",
     "shell.execute_reply": "2025-05-09T13:26:42.150800Z"
    },
    "papermill": {
     "duration": 0.154297,
     "end_time": "2025-05-09T13:26:42.152054",
     "exception": false,
     "start_time": "2025-05-09T13:26:41.997757",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.377954420035459</th>\n",
       "      <th>0.3005685724575208</th>\n",
       "      <th>0.2730827836798193</th>\n",
       "      <th>0.1161187365375847</th>\n",
       "      <th>0.4779390373985842</th>\n",
       "      <th>0.4707477391785799</th>\n",
       "      <th>0.2865889005278199</th>\n",
       "      <th>0.0966272431834859</th>\n",
       "      <th>0.1977892200859747</th>\n",
       "      <th>0.2753795102028558</th>\n",
       "      <th>...</th>\n",
       "      <th>0.7088748208313427</th>\n",
       "      <th>0.1294118931429382</th>\n",
       "      <th>0.8746186743734583</th>\n",
       "      <th>0.0973485524047321</th>\n",
       "      <th>0.2968674553355868</th>\n",
       "      <th>0.0.94</th>\n",
       "      <th>0.5380459729529323</th>\n",
       "      <th>0.2629923281866915</th>\n",
       "      <th>0.4297671622465255</th>\n",
       "      <th>0.0.95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.708650</td>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.813089</td>\n",
       "      <td>0.680699</td>\n",
       "      <td>0.545733</td>\n",
       "      <td>0.572325</td>\n",
       "      <td>0.697678</td>\n",
       "      <td>0.842246</td>\n",
       "      <td>0.474116</td>\n",
       "      <td>0.704920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.804933</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.843018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.608175</td>\n",
       "      <td>0.662687</td>\n",
       "      <td>0.595088</td>\n",
       "      <td>0.573621</td>\n",
       "      <td>0.679507</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.836337</td>\n",
       "      <td>0.724553</td>\n",
       "      <td>0.922874</td>\n",
       "      <td>0.349111</td>\n",
       "      <td>0.514206</td>\n",
       "      <td>0.372578</td>\n",
       "      <td>0.746362</td>\n",
       "      <td>0.622469</td>\n",
       "      <td>0.501846</td>\n",
       "      <td>0.734891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.710397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.592317</td>\n",
       "      <td>0.176802</td>\n",
       "      <td>0.825509</td>\n",
       "      <td>0.166148</td>\n",
       "      <td>0.224444</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.489098</td>\n",
       "      <td>0.636059</td>\n",
       "      <td>0.714290</td>\n",
       "      <td>0.630977</td>\n",
       "      <td>0.069949</td>\n",
       "      <td>0.370207</td>\n",
       "      <td>0.785841</td>\n",
       "      <td>0.417892</td>\n",
       "      <td>0.811071</td>\n",
       "      <td>0.595715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.615886</td>\n",
       "      <td>0.156727</td>\n",
       "      <td>0.817037</td>\n",
       "      <td>0.192973</td>\n",
       "      <td>0.574707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.862929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.429698</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.477439</td>\n",
       "      <td>0.393592</td>\n",
       "      <td>0.607329</td>\n",
       "      <td>0.198135</td>\n",
       "      <td>0.550840</td>\n",
       "      <td>0.245304</td>\n",
       "      <td>0.717082</td>\n",
       "      <td>0.389015</td>\n",
       "      <td>0.608288</td>\n",
       "      <td>0.590069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.601839</td>\n",
       "      <td>0.056409</td>\n",
       "      <td>0.753295</td>\n",
       "      <td>0.153588</td>\n",
       "      <td>0.498053</td>\n",
       "      <td>0.138553</td>\n",
       "      <td>0.739054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.218294</td>\n",
       "      <td>0.471895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.758234</td>\n",
       "      <td>0.549616</td>\n",
       "      <td>0.792716</td>\n",
       "      <td>0.529613</td>\n",
       "      <td>0.210826</td>\n",
       "      <td>0.430117</td>\n",
       "      <td>0.766417</td>\n",
       "      <td>0.452830</td>\n",
       "      <td>0.812468</td>\n",
       "      <td>0.704500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.850681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.749091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.553597</td>\n",
       "      <td>0.093133</td>\n",
       "      <td>0.954716</td>\n",
       "      <td>0.413251</td>\n",
       "      <td>0.623621</td>\n",
       "      <td>0.060177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.799387</td>\n",
       "      <td>0.731254</td>\n",
       "      <td>0.918431</td>\n",
       "      <td>0.336832</td>\n",
       "      <td>0.343084</td>\n",
       "      <td>0.259899</td>\n",
       "      <td>0.845376</td>\n",
       "      <td>0.465362</td>\n",
       "      <td>0.722849</td>\n",
       "      <td>0.705281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.558242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.803681</td>\n",
       "      <td>0.139736</td>\n",
       "      <td>0.550429</td>\n",
       "      <td>0.390241</td>\n",
       "      <td>0.716650</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.168189</td>\n",
       "      <td>0.242301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.409785</td>\n",
       "      <td>0.419107</td>\n",
       "      <td>0.306179</td>\n",
       "      <td>0.119020</td>\n",
       "      <td>0.700423</td>\n",
       "      <td>0.340862</td>\n",
       "      <td>0.337622</td>\n",
       "      <td>0.285221</td>\n",
       "      <td>0.222367</td>\n",
       "      <td>0.235409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.797002</td>\n",
       "      <td>0.137360</td>\n",
       "      <td>0.816745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116475</td>\n",
       "      <td>0.052727</td>\n",
       "      <td>0.282079</td>\n",
       "      <td>0.304016</td>\n",
       "      <td>0.434091</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.446395</td>\n",
       "      <td>0.897199</td>\n",
       "      <td>0.701386</td>\n",
       "      <td>0.279609</td>\n",
       "      <td>0.136374</td>\n",
       "      <td>0.446684</td>\n",
       "      <td>0.746707</td>\n",
       "      <td>0.120624</td>\n",
       "      <td>0.777950</td>\n",
       "      <td>0.692773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.543753</td>\n",
       "      <td>0.273617</td>\n",
       "      <td>0.718979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.593554</td>\n",
       "      <td>0.683562</td>\n",
       "      <td>0.887407</td>\n",
       "      <td>0.306006</td>\n",
       "      <td>0.351222</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.338903</td>\n",
       "      <td>0.581646</td>\n",
       "      <td>0.574051</td>\n",
       "      <td>0.240783</td>\n",
       "      <td>0.494790</td>\n",
       "      <td>0.491801</td>\n",
       "      <td>0.608010</td>\n",
       "      <td>0.733429</td>\n",
       "      <td>0.085286</td>\n",
       "      <td>0.211625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.816701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.550066</td>\n",
       "      <td>0.099365</td>\n",
       "      <td>0.324379</td>\n",
       "      <td>0.332649</td>\n",
       "      <td>0.577956</td>\n",
       "      <td>0.081455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.564102</td>\n",
       "      <td>0.412766</td>\n",
       "      <td>0.679223</td>\n",
       "      <td>0.524488</td>\n",
       "      <td>0.666483</td>\n",
       "      <td>0.316955</td>\n",
       "      <td>0.425785</td>\n",
       "      <td>0.400328</td>\n",
       "      <td>0.229244</td>\n",
       "      <td>0.414751</td>\n",
       "      <td>...</td>\n",
       "      <td>0.682179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.743574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408166</td>\n",
       "      <td>0.104054</td>\n",
       "      <td>0.366036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290851</td>\n",
       "      <td>0.068311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>301 rows × 1745 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0.377954420035459  0.3005685724575208  0.2730827836798193  \\\n",
       "0             0.708650            0.767016            0.813089   \n",
       "1             0.836337            0.724553            0.922874   \n",
       "2             0.489098            0.636059            0.714290   \n",
       "3             0.477439            0.393592            0.607329   \n",
       "4             0.758234            0.549616            0.792716   \n",
       "..                 ...                 ...                 ...   \n",
       "296           0.799387            0.731254            0.918431   \n",
       "297           0.409785            0.419107            0.306179   \n",
       "298           0.446395            0.897199            0.701386   \n",
       "299           0.338903            0.581646            0.574051   \n",
       "300           0.564102            0.412766            0.679223   \n",
       "\n",
       "     0.1161187365375847  0.4779390373985842  0.4707477391785799  \\\n",
       "0              0.680699            0.545733            0.572325   \n",
       "1              0.349111            0.514206            0.372578   \n",
       "2              0.630977            0.069949            0.370207   \n",
       "3              0.198135            0.550840            0.245304   \n",
       "4              0.529613            0.210826            0.430117   \n",
       "..                  ...                 ...                 ...   \n",
       "296            0.336832            0.343084            0.259899   \n",
       "297            0.119020            0.700423            0.340862   \n",
       "298            0.279609            0.136374            0.446684   \n",
       "299            0.240783            0.494790            0.491801   \n",
       "300            0.524488            0.666483            0.316955   \n",
       "\n",
       "     0.2865889005278199  0.0966272431834859  0.1977892200859747  \\\n",
       "0              0.697678            0.842246            0.474116   \n",
       "1              0.746362            0.622469            0.501846   \n",
       "2              0.785841            0.417892            0.811071   \n",
       "3              0.717082            0.389015            0.608288   \n",
       "4              0.766417            0.452830            0.812468   \n",
       "..                  ...                 ...                 ...   \n",
       "296            0.845376            0.465362            0.722849   \n",
       "297            0.337622            0.285221            0.222367   \n",
       "298            0.746707            0.120624            0.777950   \n",
       "299            0.608010            0.733429            0.085286   \n",
       "300            0.425785            0.400328            0.229244   \n",
       "\n",
       "     0.2753795102028558  ...  0.7088748208313427  0.1294118931429382  \\\n",
       "0              0.704920  ...            0.804933            0.000000   \n",
       "1              0.734891  ...            0.629348            0.000000   \n",
       "2              0.595715  ...            0.615886            0.156727   \n",
       "3              0.590069  ...            0.601839            0.056409   \n",
       "4              0.704500  ...            0.850681            0.000000   \n",
       "..                  ...  ...                 ...                 ...   \n",
       "296            0.705281  ...            0.558242            0.000000   \n",
       "297            0.235409  ...            0.797002            0.137360   \n",
       "298            0.692773  ...            0.543753            0.273617   \n",
       "299            0.211625  ...            0.807453            0.000000   \n",
       "300            0.414751  ...            0.682179            0.000000   \n",
       "\n",
       "     0.8746186743734583  0.0973485524047321  0.2968674553355868    0.0.94  \\\n",
       "0              0.843018            0.000000            0.608175  0.662687   \n",
       "1              0.710397            0.000000            0.592317  0.176802   \n",
       "2              0.817037            0.192973            0.574707  0.000000   \n",
       "3              0.753295            0.153588            0.498053  0.138553   \n",
       "4              0.749091            0.000000            0.553597  0.093133   \n",
       "..                  ...                 ...                 ...       ...   \n",
       "296            0.803681            0.139736            0.550429  0.390241   \n",
       "297            0.816745            0.000000            0.116475  0.052727   \n",
       "298            0.718979            0.000000            0.593554  0.683562   \n",
       "299            0.816701            0.000000            0.550066  0.099365   \n",
       "300            0.743574            0.000000            0.408166  0.104054   \n",
       "\n",
       "     0.5380459729529323  0.2629923281866915  0.4297671622465255    0.0.95  \n",
       "0              0.595088            0.573621            0.679507  0.000000  \n",
       "1              0.825509            0.166148            0.224444  0.000000  \n",
       "2              0.862929            0.000000            0.429698  0.000000  \n",
       "3              0.739054            0.000000            0.218294  0.471895  \n",
       "4              0.954716            0.413251            0.623621  0.060177  \n",
       "..                  ...                 ...                 ...       ...  \n",
       "296            0.716650            0.000000            0.168189  0.242301  \n",
       "297            0.282079            0.304016            0.434091  0.000000  \n",
       "298            0.887407            0.306006            0.351222  0.000000  \n",
       "299            0.324379            0.332649            0.577956  0.081455  \n",
       "300            0.366036            0.000000            0.290851  0.068311  \n",
       "\n",
       "[301 rows x 1745 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取制表符分隔的CSV文件\n",
    "RNAseq_train = pd.read_csv('../../DATA/BLCA_parting/2_tr.csv')\n",
    "# 显示 DataFrame\n",
    "RNAseq_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff830335",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:42.171292Z",
     "iopub.status.busy": "2025-05-09T13:26:42.170995Z",
     "iopub.status.idle": "2025-05-09T13:26:42.227075Z",
     "shell.execute_reply": "2025-05-09T13:26:42.226620Z"
    },
    "papermill": {
     "duration": 0.070113,
     "end_time": "2025-05-09T13:26:42.227811",
     "exception": false,
     "start_time": "2025-05-09T13:26:42.157698",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.9005624112059554</th>\n",
       "      <th>0.5990745692054172</th>\n",
       "      <th>0.7037518514177332</th>\n",
       "      <th>0.3540163199849064</th>\n",
       "      <th>0.5065347857473791</th>\n",
       "      <th>0.366018824798762</th>\n",
       "      <th>0.5739812262913282</th>\n",
       "      <th>0.5450923958105004</th>\n",
       "      <th>0.1545521416086865</th>\n",
       "      <th>0.7565278757939309</th>\n",
       "      <th>...</th>\n",
       "      <th>0.6094123268036311</th>\n",
       "      <th>0.0.43</th>\n",
       "      <th>0.8143071915458833</th>\n",
       "      <th>0.0.44</th>\n",
       "      <th>0.6996318203766299</th>\n",
       "      <th>0.5053215970544241</th>\n",
       "      <th>0.7397337664630486</th>\n",
       "      <th>0.0.45</th>\n",
       "      <th>0.3709449929478134</th>\n",
       "      <th>0.7210442364951085</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.296763</td>\n",
       "      <td>0.614091</td>\n",
       "      <td>0.406726</td>\n",
       "      <td>0.418628</td>\n",
       "      <td>0.221946</td>\n",
       "      <td>0.356436</td>\n",
       "      <td>0.258754</td>\n",
       "      <td>0.156134</td>\n",
       "      <td>0.186783</td>\n",
       "      <td>0.168306</td>\n",
       "      <td>...</td>\n",
       "      <td>0.737183</td>\n",
       "      <td>0.362351</td>\n",
       "      <td>0.455971</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.216800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.690976</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.298841</td>\n",
       "      <td>0.598496</td>\n",
       "      <td>0.566390</td>\n",
       "      <td>0.633547</td>\n",
       "      <td>0.303725</td>\n",
       "      <td>0.638496</td>\n",
       "      <td>0.271771</td>\n",
       "      <td>0.483022</td>\n",
       "      <td>0.283602</td>\n",
       "      <td>0.290725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.749032</td>\n",
       "      <td>0.145198</td>\n",
       "      <td>0.861862</td>\n",
       "      <td>0.395339</td>\n",
       "      <td>0.661743</td>\n",
       "      <td>0.127527</td>\n",
       "      <td>0.296485</td>\n",
       "      <td>0.548807</td>\n",
       "      <td>0.784157</td>\n",
       "      <td>0.181806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.389895</td>\n",
       "      <td>0.633211</td>\n",
       "      <td>0.440238</td>\n",
       "      <td>0.583054</td>\n",
       "      <td>0.598905</td>\n",
       "      <td>0.756300</td>\n",
       "      <td>0.416368</td>\n",
       "      <td>0.302929</td>\n",
       "      <td>0.087810</td>\n",
       "      <td>0.533251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.712183</td>\n",
       "      <td>0.498553</td>\n",
       "      <td>0.710674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363774</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.516445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.581285</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.410995</td>\n",
       "      <td>0.435171</td>\n",
       "      <td>0.572187</td>\n",
       "      <td>0.211201</td>\n",
       "      <td>0.419886</td>\n",
       "      <td>0.474694</td>\n",
       "      <td>0.252143</td>\n",
       "      <td>0.108940</td>\n",
       "      <td>0.461106</td>\n",
       "      <td>0.482530</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600239</td>\n",
       "      <td>0.363650</td>\n",
       "      <td>0.854082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.592196</td>\n",
       "      <td>0.065384</td>\n",
       "      <td>0.472759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.537955</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.605432</td>\n",
       "      <td>0.721814</td>\n",
       "      <td>0.916524</td>\n",
       "      <td>0.462494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.196283</td>\n",
       "      <td>0.779200</td>\n",
       "      <td>0.329356</td>\n",
       "      <td>0.759804</td>\n",
       "      <td>0.819772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.485726</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.701771</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.727743</td>\n",
       "      <td>0.089269</td>\n",
       "      <td>0.936584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.233254</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.533469</td>\n",
       "      <td>0.499896</td>\n",
       "      <td>0.490959</td>\n",
       "      <td>0.120238</td>\n",
       "      <td>0.445001</td>\n",
       "      <td>0.721120</td>\n",
       "      <td>0.122588</td>\n",
       "      <td>0.316071</td>\n",
       "      <td>0.353617</td>\n",
       "      <td>0.372783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629025</td>\n",
       "      <td>0.362067</td>\n",
       "      <td>0.815986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.335013</td>\n",
       "      <td>0.053456</td>\n",
       "      <td>0.262332</td>\n",
       "      <td>0.307707</td>\n",
       "      <td>0.415640</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.409586</td>\n",
       "      <td>0.876933</td>\n",
       "      <td>0.546251</td>\n",
       "      <td>0.187853</td>\n",
       "      <td>0.294599</td>\n",
       "      <td>0.215861</td>\n",
       "      <td>0.847451</td>\n",
       "      <td>0.104004</td>\n",
       "      <td>0.227327</td>\n",
       "      <td>0.739486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.580972</td>\n",
       "      <td>0.171912</td>\n",
       "      <td>0.713535</td>\n",
       "      <td>0.106771</td>\n",
       "      <td>0.430091</td>\n",
       "      <td>0.084954</td>\n",
       "      <td>0.583860</td>\n",
       "      <td>0.115302</td>\n",
       "      <td>0.328724</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.381935</td>\n",
       "      <td>0.617660</td>\n",
       "      <td>0.563428</td>\n",
       "      <td>0.442086</td>\n",
       "      <td>0.111592</td>\n",
       "      <td>0.364031</td>\n",
       "      <td>0.749830</td>\n",
       "      <td>0.387519</td>\n",
       "      <td>0.557642</td>\n",
       "      <td>0.217976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.721011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.717155</td>\n",
       "      <td>0.128708</td>\n",
       "      <td>0.608040</td>\n",
       "      <td>0.406225</td>\n",
       "      <td>0.486397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.464774</td>\n",
       "      <td>0.987739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.311886</td>\n",
       "      <td>0.847446</td>\n",
       "      <td>0.495349</td>\n",
       "      <td>0.710949</td>\n",
       "      <td>0.172253</td>\n",
       "      <td>0.236928</td>\n",
       "      <td>0.735460</td>\n",
       "      <td>0.471042</td>\n",
       "      <td>0.515579</td>\n",
       "      <td>0.389796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.708743</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.843952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.692736</td>\n",
       "      <td>0.337629</td>\n",
       "      <td>0.640858</td>\n",
       "      <td>0.239206</td>\n",
       "      <td>0.344702</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.184234</td>\n",
       "      <td>0.090622</td>\n",
       "      <td>0.380311</td>\n",
       "      <td>0.108407</td>\n",
       "      <td>0.416826</td>\n",
       "      <td>0.532816</td>\n",
       "      <td>0.091865</td>\n",
       "      <td>0.221340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.857907</td>\n",
       "      <td>0.053014</td>\n",
       "      <td>0.756886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.555438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.508908</td>\n",
       "      <td>0.466716</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1745 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0.9005624112059554  0.5990745692054172  0.7037518514177332  \\\n",
       "0             0.296763            0.614091            0.406726   \n",
       "1             0.298841            0.598496            0.566390   \n",
       "2             0.389895            0.633211            0.440238   \n",
       "3             0.410995            0.435171            0.572187   \n",
       "4             0.605432            0.721814            0.916524   \n",
       "..                 ...                 ...                 ...   \n",
       "95            0.533469            0.499896            0.490959   \n",
       "96            0.409586            0.876933            0.546251   \n",
       "97            0.381935            0.617660            0.563428   \n",
       "98            0.311886            0.847446            0.495349   \n",
       "99            0.184234            0.090622            0.380311   \n",
       "\n",
       "    0.3540163199849064  0.5065347857473791  0.366018824798762  \\\n",
       "0             0.418628            0.221946           0.356436   \n",
       "1             0.633547            0.303725           0.638496   \n",
       "2             0.583054            0.598905           0.756300   \n",
       "3             0.211201            0.419886           0.474694   \n",
       "4             0.462494            0.000000           0.196283   \n",
       "..                 ...                 ...                ...   \n",
       "95            0.120238            0.445001           0.721120   \n",
       "96            0.187853            0.294599           0.215861   \n",
       "97            0.442086            0.111592           0.364031   \n",
       "98            0.710949            0.172253           0.236928   \n",
       "99            0.108407            0.416826           0.532816   \n",
       "\n",
       "    0.5739812262913282  0.5450923958105004  0.1545521416086865  \\\n",
       "0             0.258754            0.156134            0.186783   \n",
       "1             0.271771            0.483022            0.283602   \n",
       "2             0.416368            0.302929            0.087810   \n",
       "3             0.252143            0.108940            0.461106   \n",
       "4             0.779200            0.329356            0.759804   \n",
       "..                 ...                 ...                 ...   \n",
       "95            0.122588            0.316071            0.353617   \n",
       "96            0.847451            0.104004            0.227327   \n",
       "97            0.749830            0.387519            0.557642   \n",
       "98            0.735460            0.471042            0.515579   \n",
       "99            0.091865            0.221340            0.000000   \n",
       "\n",
       "    0.7565278757939309  ...  0.6094123268036311    0.0.43  0.8143071915458833  \\\n",
       "0             0.168306  ...            0.737183  0.362351            0.455971   \n",
       "1             0.290725  ...            0.749032  0.145198            0.861862   \n",
       "2             0.533251  ...            0.712183  0.498553            0.710674   \n",
       "3             0.482530  ...            0.600239  0.363650            0.854082   \n",
       "4             0.819772  ...            0.485726  0.000000            0.701771   \n",
       "..                 ...  ...                 ...       ...                 ...   \n",
       "95            0.372783  ...            0.629025  0.362067            0.815986   \n",
       "96            0.739486  ...            0.580972  0.171912            0.713535   \n",
       "97            0.217976  ...            0.721011  0.000000            0.717155   \n",
       "98            0.389796  ...            0.708743  0.000000            0.843952   \n",
       "99            0.185333  ...            0.857907  0.053014            0.756886   \n",
       "\n",
       "      0.0.44  0.6996318203766299  0.5053215970544241  0.7397337664630486  \\\n",
       "0   0.000000            0.269179            0.000000            0.216800   \n",
       "1   0.395339            0.661743            0.127527            0.296485   \n",
       "2   0.000000            0.363774            0.000000            0.516445   \n",
       "3   0.000000            0.592196            0.065384            0.472759   \n",
       "4   0.000000            0.727743            0.089269            0.936584   \n",
       "..       ...                 ...                 ...                 ...   \n",
       "95  0.000000            0.335013            0.053456            0.262332   \n",
       "96  0.106771            0.430091            0.084954            0.583860   \n",
       "97  0.128708            0.608040            0.406225            0.486397   \n",
       "98  0.000000            0.692736            0.337629            0.640858   \n",
       "99  0.000000            0.555438            0.000000            0.000000   \n",
       "\n",
       "      0.0.45  0.3709449929478134  0.7210442364951085  \n",
       "0   0.000000            0.690976            0.000000  \n",
       "1   0.548807            0.784157            0.181806  \n",
       "2   0.000000            0.581285            0.000000  \n",
       "3   0.000000            0.537955            0.000000  \n",
       "4   0.000000            0.233254            0.000000  \n",
       "..       ...                 ...                 ...  \n",
       "95  0.307707            0.415640            0.000000  \n",
       "96  0.115302            0.328724            0.000000  \n",
       "97  0.000000            0.464774            0.987739  \n",
       "98  0.239206            0.344702            0.000000  \n",
       "99  0.508908            0.466716            0.000000  \n",
       "\n",
       "[100 rows x 1745 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取制表符分隔的CSV文件\n",
    "RNAseq_test = pd.read_csv('../../DATA/BLCA_parting/2_te.csv')\n",
    "# 显示 DataFrame\n",
    "RNAseq_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bcf918a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:42.245903Z",
     "iopub.status.busy": "2025-05-09T13:26:42.245612Z",
     "iopub.status.idle": "2025-05-09T13:26:42.247775Z",
     "shell.execute_reply": "2025-05-09T13:26:42.247370Z"
    },
    "papermill": {
     "duration": 0.014671,
     "end_time": "2025-05-09T13:26:42.248404",
     "exception": false,
     "start_time": "2025-05-09T13:26:42.233733",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 读取制表符分隔的CSV文件\n",
    "# RNAseq_val = pd.read_csv('../../DATA/BLCA_parting/RNAseq_val.csv')\n",
    "# # 显示 DataFrame\n",
    "# RNAseq_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fb4c79b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:42.260658Z",
     "iopub.status.busy": "2025-05-09T13:26:42.260403Z",
     "iopub.status.idle": "2025-05-09T13:26:42.266729Z",
     "shell.execute_reply": "2025-05-09T13:26:42.266274Z"
    },
    "papermill": {
     "duration": 0.013316,
     "end_time": "2025-05-09T13:26:42.267438",
     "exception": false,
     "start_time": "2025-05-09T13:26:42.254122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1\n",
       "2    102\n",
       "1     99\n",
       "3     98\n",
       "0      2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用np.load()函数加载.npy文件\n",
    "labels_train = pd.read_csv('../../DATA/BLCA_parting/labels_tr.csv')\n",
    "labels_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36706f05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:42.287033Z",
     "iopub.status.busy": "2025-05-09T13:26:42.286745Z",
     "iopub.status.idle": "2025-05-09T13:26:42.292974Z",
     "shell.execute_reply": "2025-05-09T13:26:42.292519Z"
    },
    "papermill": {
     "duration": 0.020445,
     "end_time": "2025-05-09T13:26:42.293662",
     "exception": false,
     "start_time": "2025-05-09T13:26:42.273217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3\n",
       "2    36\n",
       "3    34\n",
       "1    30\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用np.load()函数加载.npy文件\n",
    "labels_test = pd.read_csv('../../DATA/BLCA_parting/labels_te.csv')\n",
    "labels_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e068632b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:42.314565Z",
     "iopub.status.busy": "2025-05-09T13:26:42.314276Z",
     "iopub.status.idle": "2025-05-09T13:26:42.316619Z",
     "shell.execute_reply": "2025-05-09T13:26:42.316170Z"
    },
    "papermill": {
     "duration": 0.017947,
     "end_time": "2025-05-09T13:26:42.317332",
     "exception": false,
     "start_time": "2025-05-09T13:26:42.299385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 使用np.load()函数加载.npy文件\n",
    "# labels_val = pd.read_csv('../../DATA/BLCA_parting/labels_val.csv')\n",
    "# labels_val.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "813d1549",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:42.332406Z",
     "iopub.status.busy": "2025-05-09T13:26:42.332137Z",
     "iopub.status.idle": "2025-05-09T13:26:42.335646Z",
     "shell.execute_reply": "2025-05-09T13:26:42.335211Z"
    },
    "papermill": {
     "duration": 0.012959,
     "end_time": "2025-05-09T13:26:42.336348",
     "exception": false,
     "start_time": "2025-05-09T13:26:42.323389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dim = len(np.unique(labels_train))\n",
    "label_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aae8d01b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:42.348619Z",
     "iopub.status.busy": "2025-05-09T13:26:42.348356Z",
     "iopub.status.idle": "2025-05-09T13:26:42.388323Z",
     "shell.execute_reply": "2025-05-09T13:26:42.387775Z"
    },
    "papermill": {
     "duration": 0.046827,
     "end_time": "2025-05-09T13:26:42.389003",
     "exception": false,
     "start_time": "2025-05-09T13:26:42.342176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 使用 GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a28be32d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:42.401627Z",
     "iopub.status.busy": "2025-05-09T13:26:42.401373Z",
     "iopub.status.idle": "2025-05-09T13:26:42.652725Z",
     "shell.execute_reply": "2025-05-09T13:26:42.652072Z"
    },
    "papermill": {
     "duration": 0.258535,
     "end_time": "2025-05-09T13:26:42.653600",
     "exception": false,
     "start_time": "2025-05-09T13:26:42.395065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 0 已转换为 Tensor，并移动到 cuda:0\n",
      "DataFrame 1 已转换为 Tensor，并移动到 cuda:0\n",
      "DataFrame 2 已转换为 Tensor，并移动到 cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 调用函数，将这些 numpy 数组转换为 tensors\n",
    "train_tensors = batch_dataframe_to_tensors(Methylation_train, miRNASeq_train, RNAseq_train)\n",
    "\n",
    "# 现在 tensors 是一个包含这些转换后张量的列表\n",
    "Methylation_train = train_tensors[0]\n",
    "miRNASeq_train = train_tensors[1]\n",
    "RNAseq_train = train_tensors[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a23632e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:42.672939Z",
     "iopub.status.busy": "2025-05-09T13:26:42.672587Z",
     "iopub.status.idle": "2025-05-09T13:26:42.685489Z",
     "shell.execute_reply": "2025-05-09T13:26:42.684971Z"
    },
    "papermill": {
     "duration": 0.023414,
     "end_time": "2025-05-09T13:26:42.686250",
     "exception": false,
     "start_time": "2025-05-09T13:26:42.662836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 0 已转换为 Tensor，并移动到 cuda:0\n",
      "DataFrame 1 已转换为 Tensor，并移动到 cuda:0\n",
      "DataFrame 2 已转换为 Tensor，并移动到 cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 调用函数，将这些 numpy 数组转换为 tensors\n",
    "test_tensors = batch_dataframe_to_tensors(Methylation_test, miRNASeq_test, RNAseq_test)\n",
    "\n",
    "# 现在 tensors 是一个包含这些转换后张量的列表\n",
    "Methylation_test = test_tensors[0]\n",
    "miRNASeq_test = test_tensors[1]\n",
    "RNAseq_test = test_tensors[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03468a79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:42.705653Z",
     "iopub.status.busy": "2025-05-09T13:26:42.705129Z",
     "iopub.status.idle": "2025-05-09T13:26:42.707898Z",
     "shell.execute_reply": "2025-05-09T13:26:42.707453Z"
    },
    "papermill": {
     "duration": 0.013265,
     "end_time": "2025-05-09T13:26:42.708659",
     "exception": false,
     "start_time": "2025-05-09T13:26:42.695394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 调用函数，将这些 numpy 数组转换为 tensors\n",
    "# val_tensors = batch_dataframe_to_tensors(Methylation_val, miRNASeq_val, RNAseq_val)\n",
    "\n",
    "# # 现在 tensors 是一个包含这些转换后张量的列表\n",
    "# Methylation_val = val_tensors[0]\n",
    "# miRNASeq_val = val_tensors[1]\n",
    "# RNAseq_val = val_tensors[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8860b5d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:42.727964Z",
     "iopub.status.busy": "2025-05-09T13:26:42.727541Z",
     "iopub.status.idle": "2025-05-09T13:26:42.737219Z",
     "shell.execute_reply": "2025-05-09T13:26:42.736776Z"
    },
    "papermill": {
     "duration": 0.020105,
     "end_time": "2025-05-09T13:26:42.737971",
     "exception": false,
     "start_time": "2025-05-09T13:26:42.717866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [3],\n",
       "        [3],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [0],\n",
       "        [3],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [3],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [3],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [3],\n",
       "        [3],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [3],\n",
       "        [3],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [3],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [3],\n",
       "        [3],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [0],\n",
       "        [3],\n",
       "        [1],\n",
       "        [2]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将DataFrame的值转换为Tensor\n",
    "labels_tr_tensor = torch.LongTensor(labels_train.values)\n",
    "labels_tr_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e593db4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:42.758751Z",
     "iopub.status.busy": "2025-05-09T13:26:42.758395Z",
     "iopub.status.idle": "2025-05-09T13:26:42.761536Z",
     "shell.execute_reply": "2025-05-09T13:26:42.761094Z"
    },
    "papermill": {
     "duration": 0.014942,
     "end_time": "2025-05-09T13:26:42.762290",
     "exception": false,
     "start_time": "2025-05-09T13:26:42.747348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "onehot_labels_tr_tensor = one_hot_tensor(labels_tr_tensor, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0203bbc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:42.783251Z",
     "iopub.status.busy": "2025-05-09T13:26:42.782825Z",
     "iopub.status.idle": "2025-05-09T13:26:42.788314Z",
     "shell.execute_reply": "2025-05-09T13:26:42.787885Z"
    },
    "papermill": {
     "duration": 0.016096,
     "end_time": "2025-05-09T13:26:42.789091",
     "exception": false,
     "start_time": "2025-05-09T13:26:42.772995",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [3],\n",
       "        [3],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [3],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [1]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将DataFrame的值转换为Tensor\n",
    "labels_te_tensor = torch.LongTensor(labels_test.values)\n",
    "labels_te_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0b725ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:42.806564Z",
     "iopub.status.busy": "2025-05-09T13:26:42.806279Z",
     "iopub.status.idle": "2025-05-09T13:26:42.812224Z",
     "shell.execute_reply": "2025-05-09T13:26:42.811884Z"
    },
    "papermill": {
     "duration": 0.017306,
     "end_time": "2025-05-09T13:26:42.812831",
     "exception": false,
     "start_time": "2025-05-09T13:26:42.795525",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [3],\n",
       "        [3],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [0],\n",
       "        [3],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [3],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [3],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [3],\n",
       "        [3],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [3],\n",
       "        [3],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [3],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [3],\n",
       "        [3],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [3],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [0],\n",
       "        [3],\n",
       "        [1],\n",
       "        [2]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将DataFrame的值转换为Tensor\n",
    "labels_tr_tensor = torch.LongTensor(labels_train.values)\n",
    "labels_tr_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9b7910e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:42.829478Z",
     "iopub.status.busy": "2025-05-09T13:26:42.829139Z",
     "iopub.status.idle": "2025-05-09T13:26:42.831196Z",
     "shell.execute_reply": "2025-05-09T13:26:42.830849Z"
    },
    "papermill": {
     "duration": 0.012278,
     "end_time": "2025-05-09T13:26:42.831797",
     "exception": false,
     "start_time": "2025-05-09T13:26:42.819519",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 将DataFrame的值转换为Tensor\n",
    "# labels_val_tensor = torch.LongTensor(labels_val.values)\n",
    "# labels_val_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a22e9c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:42.850924Z",
     "iopub.status.busy": "2025-05-09T13:26:42.850608Z",
     "iopub.status.idle": "2025-05-09T13:26:42.853866Z",
     "shell.execute_reply": "2025-05-09T13:26:42.853519Z"
    },
    "papermill": {
     "duration": 0.014548,
     "end_time": "2025-05-09T13:26:42.854450",
     "exception": false,
     "start_time": "2025-05-09T13:26:42.839902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义数据集\n",
    "train_dataset = TensorDataset(Methylation_train, miRNASeq_train, RNAseq_train, labels_tr_tensor)\n",
    "test_dataset = TensorDataset(Methylation_test, miRNASeq_test, RNAseq_test, labels_te_tensor)\n",
    "# val_dataset = TensorDataset(Methylation_val, miRNASeq_val, RNAseq_val, labels_val_tensor)\n",
    "\n",
    "# 定义 DataLoader，使用自定义采样器\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)  # 测试集一般不需要打乱顺序\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)  # 验证集也一般不需要重复采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bdfa98b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:42.873535Z",
     "iopub.status.busy": "2025-05-09T13:26:42.873223Z",
     "iopub.status.idle": "2025-05-09T13:26:42.883574Z",
     "shell.execute_reply": "2025-05-09T13:26:42.883126Z"
    },
    "papermill": {
     "duration": 0.023257,
     "end_time": "2025-05-09T13:26:42.884283",
     "exception": false,
     "start_time": "2025-05-09T13:26:42.861026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: 101 samples\n",
      "Batch 2: 101 samples\n",
      "Batch 3: 99 samples\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_loader):\n",
    "    print(f\"Batch {i + 1}: {len(batch[0])} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6130e7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:42.899098Z",
     "iopub.status.busy": "2025-05-09T13:26:42.898815Z",
     "iopub.status.idle": "2025-05-09T13:26:42.903385Z",
     "shell.execute_reply": "2025-05-09T13:26:42.902992Z"
    },
    "papermill": {
     "duration": 0.012627,
     "end_time": "2025-05-09T13:26:42.904023",
     "exception": false,
     "start_time": "2025-05-09T13:26:42.891396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: 100 samples\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(test_loader):\n",
    "    print(f\"Batch {i + 1}: {len(batch[0])} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f1cdddd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:42.918149Z",
     "iopub.status.busy": "2025-05-09T13:26:42.917902Z",
     "iopub.status.idle": "2025-05-09T13:26:42.919836Z",
     "shell.execute_reply": "2025-05-09T13:26:42.919516Z"
    },
    "papermill": {
     "duration": 0.009614,
     "end_time": "2025-05-09T13:26:42.920415",
     "exception": false,
     "start_time": "2025-05-09T13:26:42.910801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i, batch in enumerate(val_loader):\n",
    "#     print(f\"Batch {i + 1}: {len(batch[0])} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "409b569a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:42.934493Z",
     "iopub.status.busy": "2025-05-09T13:26:42.934312Z",
     "iopub.status.idle": "2025-05-09T13:26:42.943843Z",
     "shell.execute_reply": "2025-05-09T13:26:42.943492Z"
    },
    "papermill": {
     "duration": 0.017236,
     "end_time": "2025-05-09T13:26:42.944415",
     "exception": false,
     "start_time": "2025-05-09T13:26:42.927179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from typing import Dict, Tuple, List\n",
    "import gc\n",
    "\n",
    "def plot_metrics(lr: float, depth: int, heads: int, dim_head: int, beta: float, adj_parameter: int, dropout: float) -> None:\n",
    "    \"\"\"生成四分类训练指标图表\"\"\"\n",
    "    \n",
    "    # ================== 配置参数 ==================\n",
    "    SAVE_DIR = '../../result/BLCA_parting/'\n",
    "    FILE_FORMAT = 'pdf'\n",
    "    DPI = 600\n",
    "    FONT_CONFIG = {\n",
    "        'font.family': 'DejaVu Sans',\n",
    "        'font.size': 12,\n",
    "        'axes.titlesize': 14,\n",
    "        'axes.labelsize': 12,\n",
    "        'legend.fontsize': 10\n",
    "    }\n",
    "    STYLE_CONFIG = {\n",
    "        'train': {'color': '#E74C3C', 'ls': '-', 'lw': 2.5, 'alpha': 0.9},\n",
    "        'val': {'color': '#3498DB', 'ls': '-', 'lw': 2.5, 'alpha': 0.9},\n",
    "        'test': {'color': '#2ECC71', 'ls': '-', 'lw': 2.5, 'alpha': 0.9}\n",
    "    }\n",
    "    \n",
    "    # ================== 初始化设置 ==================\n",
    "    rcParams.update(FONT_CONFIG)\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    param_id = f\"lr{lr:.0e}_depth{depth}heads{heads}dim_head{dim_head}adj_parameter{adj_parameter}dropout{dropout}beta{beta}\".replace('.', '').replace('+', '')\n",
    "\n",
    "    try:\n",
    "        # ================== 创建画布 ==================\n",
    "        fig = plt.figure(figsize=(18, 12), dpi=DPI, facecolor='white')\n",
    "        fig.suptitle(f'Training Metrics ({param_id})', y=1.02, fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # ================== 子图配置 ==================\n",
    "        axes = [\n",
    "            fig.add_subplot(221),  # Loss\n",
    "            fig.add_subplot(222),  # Accuracy\n",
    "            fig.add_subplot(223),  # F1 Weighted\n",
    "            fig.add_subplot(224)   # F1 Macro\n",
    "        ]\n",
    "        \n",
    "        metric_data = [\n",
    "            ('Loss', (train_losses, test_losses), 'log' if max(train_losses) > 1e3 else 'linear'),\n",
    "            ('Accuracy', (train_accuracies, test_accuracies), 'linear'),\n",
    "            ('F1 Weighted', (train_f1_weighted_scores, test_f1_weighted_scores), 'linear'),\n",
    "            ('F1 Macro', (train_f1_macro_scores, test_f1_macro_scores), 'linear')\n",
    "        ]\n",
    "\n",
    "        # ================== 绘制图表 ==================\n",
    "        for ax, (title, (train, test), scale) in zip(axes, metric_data):\n",
    "            ax.plot(train, label='Train', **STYLE_CONFIG['train'])\n",
    "            # ax.plot(val, label='Val', **STYLE_CONFIG['val'])\n",
    "            ax.plot(test, label='Test', **STYLE_CONFIG['test'])\n",
    "            \n",
    "            ax.set_title(title, pad=15, fontweight='semibold')\n",
    "            ax.set_xlabel('Epochs', labelpad=10)\n",
    "            ax.set_ylabel(title, labelpad=10)\n",
    "            ax.grid(True, which='both', ls='--', alpha=0.4)\n",
    "            ax.set_yscale(scale)\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            \n",
    "            if ax == axes[0]:\n",
    "                handles, labels = ax.get_legend_handles_labels()\n",
    "                fig.legend(handles, labels, loc='upper center', \n",
    "                         bbox_to_anchor=(0.5, 1.0), ncol=3, frameon=False)\n",
    "\n",
    "        # ================== 布局优化 ==================\n",
    "        plt.tight_layout(pad=3.0, w_pad=2.0, h_pad=2.0)\n",
    "        \n",
    "        # ================== 保存图像 ==================\n",
    "        save_path = os.path.join(SAVE_DIR, f'metrics_{param_id}.{FILE_FORMAT}')\n",
    "        plt.savefig(\n",
    "            save_path,\n",
    "            dpi=DPI,\n",
    "            bbox_inches='tight',\n",
    "            facecolor=fig.get_facecolor(),\n",
    "            edgecolor='none',\n",
    "            transparent=False,\n",
    "            metadata={'CreationDate': None}\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"图表生成失败: {str(e)}\")\n",
    "    finally:\n",
    "        plt.close(fig)\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84bfd89f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:42.958475Z",
     "iopub.status.busy": "2025-05-09T13:26:42.958205Z",
     "iopub.status.idle": "2025-05-09T13:26:42.963006Z",
     "shell.execute_reply": "2025-05-09T13:26:42.962666Z"
    },
    "papermill": {
     "duration": 0.012409,
     "end_time": "2025-05-09T13:26:42.963551",
     "exception": false,
     "start_time": "2025-05-09T13:26:42.951142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(data_loader, model, optimizer, adj_parameter, output_attentions=False):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_probs, all_labels = [], []\n",
    "\n",
    "    for batch in data_loader:\n",
    "        Methylation_batch, miRNASeq_batch, RNAseq_batch, labels_batch = [\n",
    "            tensor.to(device) for tensor in batch\n",
    "        ]\n",
    "\n",
    "        # 生成三维邻接张量\n",
    "        graph_train = np.stack([\n",
    "            gen_trte_adj_mat(Methylation_batch.cpu(), adj_parameter),\n",
    "            gen_trte_adj_mat(miRNASeq_batch.cpu(), adj_parameter),\n",
    "            gen_trte_adj_mat(RNAseq_batch.cpu(), adj_parameter)\n",
    "        ], axis=-1)\n",
    "        graph_train = torch.from_numpy(graph_train).float().to(device).permute(0, 2, 1)\n",
    "\n",
    "        # 梯度更新流程\n",
    "        optimizer.zero_grad()\n",
    "        ci_loss, ci = model(\n",
    "            Methylation_batch, \n",
    "            miRNASeq_batch,\n",
    "            RNAseq_batch,\n",
    "            graph_train,\n",
    "            labels_batch,\n",
    "            output_attentions=output_attentions\n",
    "        )\n",
    "        ci_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 结果收集\n",
    "        total_loss += ci_loss.item() * labels_batch.size(0)\n",
    "        all_probs.append(F.softmax(ci, dim=1).detach().cpu().numpy())\n",
    "        all_labels.append(labels_batch.detach().cpu().numpy().flatten())\n",
    "\n",
    "    return (\n",
    "        total_loss / len(data_loader.dataset),\n",
    "        np.concatenate(all_probs),\n",
    "        np.concatenate(all_labels)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e66ebda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:42.983559Z",
     "iopub.status.busy": "2025-05-09T13:26:42.983335Z",
     "iopub.status.idle": "2025-05-09T13:26:42.985682Z",
     "shell.execute_reply": "2025-05-09T13:26:42.985352Z"
    },
    "papermill": {
     "duration": 0.014604,
     "end_time": "2025-05-09T13:26:42.986244",
     "exception": false,
     "start_time": "2025-05-09T13:26:42.971640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def val_epoch(data_loader, model, adj_parameter, output_attentions=False):\n",
    "#     model.eval()\n",
    "#     total_loss = 0.0\n",
    "#     all_labels, all_probs = [], []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in data_loader:\n",
    "#             # 数据预处理\n",
    "#             Methylation_batch, miRNASeq_batch, RNAseq_batch, labels_batch = [\n",
    "#                 x.to(device) for x in batch\n",
    "#             ]\n",
    "#             # print(miRNASeq_batch.shape) #torch.Size([32, 217])\n",
    "#             # 生成三维邻接张量\n",
    "#             graph_val = torch.from_numpy(np.stack([\n",
    "#                 gen_trte_adj_mat(d.cpu(), adj_parameter) \n",
    "#                 for d in [Methylation_batch, miRNASeq_batch, RNAseq_batch]\n",
    "#             ], axis=-1)).float().to(device).permute(0,2,1)\n",
    "\n",
    "#             # 前向计算\n",
    "#             ci_loss, ci = model(\n",
    "#                 Methylation_batch, \n",
    "#                 miRNASeq_batch,\n",
    "#                 RNAseq_batch,\n",
    "#                 graph_val,\n",
    "#                 labels_batch,\n",
    "#                 output_attentions=output_attentions\n",
    "#             )\n",
    "\n",
    "#             # 累计结果\n",
    "#             total_loss += ci_loss.item() * labels_batch.size(0)\n",
    "#             all_probs.append(F.softmax(ci, 1).detach().cpu().numpy())\n",
    "#             all_labels.append(labels_batch.detach().cpu().numpy().flatten())\n",
    "\n",
    "#     return (\n",
    "#         total_loss / len(data_loader.dataset),\n",
    "#         np.concatenate(all_probs),\n",
    "#         np.concatenate(all_labels)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a309d1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:43.004922Z",
     "iopub.status.busy": "2025-05-09T13:26:43.004706Z",
     "iopub.status.idle": "2025-05-09T13:26:43.009310Z",
     "shell.execute_reply": "2025-05-09T13:26:43.008971Z"
    },
    "papermill": {
     "duration": 0.016932,
     "end_time": "2025-05-09T13:26:43.009879",
     "exception": false,
     "start_time": "2025-05-09T13:26:42.992947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_epoch(data_loader, model, adj_parameter, output_attentions=False):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_probs, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            # 数据预处理 (保持原始变量名)\n",
    "            Methylation_batch, miRNASeq_batch, RNAseq_batch, labels_test_tensor = [\n",
    "                tensor.to(device) for tensor in batch\n",
    "            ]\n",
    "\n",
    "            # 生成邻接矩阵 (保持原始逻辑但更紧凑)\n",
    "            graph_test = torch.from_numpy(\n",
    "                np.stack([\n",
    "                    gen_trte_adj_mat(Methylation_batch.cpu(), adj_parameter),\n",
    "                    gen_trte_adj_mat(miRNASeq_batch.cpu(), adj_parameter),\n",
    "                    gen_trte_adj_mat(RNAseq_batch.cpu(), adj_parameter)\n",
    "                ], axis=-1)\n",
    "            ).float().to(device).permute(0, 2, 1)\n",
    "\n",
    "            # 前向计算\n",
    "            loss, logits = model(\n",
    "                Methylation_batch,\n",
    "                miRNASeq_batch,\n",
    "                RNAseq_batch,\n",
    "                graph_test,\n",
    "                labels_test_tensor,\n",
    "                output_attentions=output_attentions\n",
    "            )\n",
    "\n",
    "            # 结果收集\n",
    "            total_loss += loss.item() * labels_test_tensor.size(0)\n",
    "            all_probs.append(F.softmax(logits, dim=1).detach().cpu().numpy())\n",
    "            all_labels.append(labels_test_tensor.detach().cpu().numpy().flatten())\n",
    "\n",
    "    return (\n",
    "        total_loss / len(data_loader.dataset),\n",
    "        np.concatenate(all_probs),\n",
    "        np.concatenate(all_labels)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a07e129",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:43.035437Z",
     "iopub.status.busy": "2025-05-09T13:26:43.035256Z",
     "iopub.status.idle": "2025-05-09T13:26:43.045804Z",
     "shell.execute_reply": "2025-05-09T13:26:43.045458Z"
    },
    "papermill": {
     "duration": 0.024427,
     "end_time": "2025-05-09T13:26:43.046347",
     "exception": false,
     "start_time": "2025-05-09T13:26:43.021920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 用于保存指标数据的列表（移除了AUC相关指标）\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "test_losses = []\n",
    "val_accuracies = []\n",
    "val_f1_weighted_scores = []\n",
    "val_f1_macro_scores = []\n",
    "test_accuracies = []\n",
    "test_f1_weighted_scores = []\n",
    "test_f1_macro_scores = []\n",
    "train_accuracies = []\n",
    "train_f1_weighted_scores = []\n",
    "train_f1_macro_scores = []\n",
    "\n",
    "def train_test(\n",
    "        Methylation_train,\n",
    "        miRNASeq_train,\n",
    "        RNAseq_train,\n",
    "        lr,\n",
    "        depth, \n",
    "        heads, \n",
    "        dim_head,\n",
    "        beta, \n",
    "        attn_dropout,\n",
    "        ff_dropout, \n",
    "        classifier_dropout,\n",
    "        adj_parameter,\n",
    "        num_epoch_pretrain\n",
    "):\n",
    "    # 初始化最佳指标跟踪器（以val_f1_macro为选择标准）\n",
    "    best_metrics = {\n",
    "        'epoch': -1,\n",
    "        # 'val_f1_macro': -1.0,\n",
    "        'test_acc': 0.0,\n",
    "        'test_f1_weighted': 0.0,\n",
    "        'test_f1_macro': 0.0,\n",
    "    }\n",
    "    \n",
    "    DEPTH = depth\n",
    "    HEAD = heads\n",
    "    HEAD_DIM = dim_head\n",
    "\n",
    "    classifier_input = [Methylation_train.shape[1], miRNASeq_train.shape[1], RNAseq_train.shape[1]]\n",
    "    classifier_dim = [\n",
    "        [classifier_input[0], 300, 200],\n",
    "        [classifier_input[1], 300, 200],\n",
    "        [classifier_input[2], 300, 200]\n",
    "    ]\n",
    "    col_dim = 32\n",
    "\n",
    "    # 确定 row_dim 和 embeding_num\n",
    "    if num_view <= 2:\n",
    "        embeding = True\n",
    "        embeding_num = 32\n",
    "        row_dim = embeding_num\n",
    "    else:\n",
    "        embeding = False\n",
    "        row_dim = num_view\n",
    "        embeding_num = 32\n",
    "    \n",
    "    model = pathformer_model(\n",
    "        row_dim=row_dim,\n",
    "        col_dim=col_dim,\n",
    "        depth=DEPTH,\n",
    "        heads=HEAD,\n",
    "        dim_head=HEAD_DIM,\n",
    "        classifier_input=classifier_input,\n",
    "        classifier_dim=classifier_dim,\n",
    "        label_dim=label_dim,\n",
    "        embeding=embeding,\n",
    "        embeding_num=embeding_num,\n",
    "        beta=beta,\n",
    "        attn_dropout=attn_dropout,\n",
    "        ff_dropout=ff_dropout,\n",
    "        classifier_dropout=classifier_dropout,\n",
    "        num_view=num_view,\n",
    "        dim_hvcdn=dim_hvcdn\n",
    "    )\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model.to(device)\n",
    "\n",
    "    logging.info(\"\\nPretrain GCNs...\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    logging.info(\"\\nTraining...\")\n",
    "    for epoch in range(num_epoch_pretrain + 1):\n",
    "        # 训练阶段\n",
    "        train_loss, train_probs, labels_train_tensor = train_epoch(train_loader, model, optimizer, adj_parameter, output_attentions=False)\n",
    "        train_acc = accuracy_score(labels_train_tensor, train_probs.argmax(1))\n",
    "        train_f1_weighted = f1_score(labels_train_tensor, train_probs.argmax(1), average='weighted')\n",
    "        train_f1_macro = f1_score(labels_train_tensor, train_probs.argmax(1), average='macro')\n",
    "\n",
    "        logging.info(f\"Train Epoch {epoch}, Loss: {train_loss}\")\n",
    "        logging.info(f\"Train Epoch {epoch}, ACC: {train_acc:.3f}\")\n",
    "        logging.info(f\"Train Epoch {epoch}, F1 Weighted: {train_f1_weighted:.3f}\")\n",
    "        logging.info(f\"Train Epoch {epoch}, F1 Macro: {train_f1_macro:.3f}\")\n",
    "        \n",
    "        # 验证阶段\n",
    "        # val_loss, val_probs, labels_validation_tensor = val_epoch(val_loader, model, adj_parameter, output_attentions=False)\n",
    "        # val_acc = accuracy_score(labels_validation_tensor, val_probs.argmax(1))\n",
    "        # val_f1_weighted = f1_score(labels_validation_tensor, val_probs.argmax(1), average='weighted')\n",
    "        # val_f1_macro = f1_score(labels_validation_tensor, val_probs.argmax(1), average='macro')\n",
    "        \n",
    "        # logging.info(f\"Val Epoch {epoch}, Loss: {val_loss}\")\n",
    "        # logging.info(f\"Val Epoch {epoch}, ACC: {val_acc:.3f}\")\n",
    "        # logging.info(f\"Val Epoch {epoch}, F1 Weighted: {val_f1_weighted:.3f}\")\n",
    "        # logging.info(f\"Val Epoch {epoch}, F1 Macro: {val_f1_macro:.3f}\")\n",
    "        \n",
    "        # 测试阶段\n",
    "        test_loss, test_probs, labels_test_tensor = test_epoch(test_loader, model, adj_parameter, output_attentions=False)\n",
    "        test_acc = accuracy_score(labels_test_tensor, test_probs.argmax(1))\n",
    "        test_f1_weighted = f1_score(labels_test_tensor, test_probs.argmax(1), average='weighted')\n",
    "        test_f1_macro = f1_score(labels_test_tensor, test_probs.argmax(1), average='macro')\n",
    "        \n",
    "        logging.info(f\"Test Epoch {epoch}, Loss: {test_loss}\")\n",
    "        logging.info(f\"Test Epoch {epoch}, ACC: {test_acc:.3f}\")\n",
    "        logging.info(f\"Test Epoch {epoch}, F1 Weighted: {test_f1_weighted:.3f}\")\n",
    "        logging.info(f\"Test Epoch {epoch}, F1 Macro: {test_f1_macro:.3f}\")\n",
    "\n",
    "        # 更新最佳指标（以val_f1_macro为选择标准）\n",
    "        if test_f1_weighted > best_metrics['test_f1_weighted'] or \\\n",
    "           (test_f1_weighted == best_metrics['test_f1_weighted'] and test_f1_macro > best_metrics['test_f1_macro']):\n",
    "            best_metrics.update({\n",
    "                'epoch': epoch,\n",
    "                # 'val_f1_macro': val_f1_macro,\n",
    "                'test_acc': test_acc,\n",
    "                'test_f1_weighted': test_f1_weighted,\n",
    "                'test_f1_macro': test_f1_macro\n",
    "            })\n",
    "\n",
    "        logging.info(\"\\n\")\n",
    "\n",
    "        # 保存训练指标\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        train_f1_weighted_scores.append(train_f1_weighted)\n",
    "        train_f1_macro_scores.append(train_f1_macro)\n",
    "        \n",
    "        # val_losses.append(val_loss)\n",
    "        # val_accuracies.append(val_acc)\n",
    "        # val_f1_weighted_scores.append(val_f1_weighted)\n",
    "        # val_f1_macro_scores.append(val_f1_macro)\n",
    "        \n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_acc)\n",
    "        test_f1_weighted_scores.append(test_f1_weighted)\n",
    "        test_f1_macro_scores.append(test_f1_macro)\n",
    "\n",
    "        # 绘制并保存指标图（需调整绘图函数）\n",
    "        plot_metrics(lr, depth, heads, dim_head, beta, adj_parameter, dropout=attn_dropout)\n",
    "\n",
    "    return (\n",
    "        best_metrics['test_acc'],\n",
    "        best_metrics['test_f1_weighted'],\n",
    "        best_metrics['test_f1_macro']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e819324a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:43.062247Z",
     "iopub.status.busy": "2025-05-09T13:26:43.062067Z",
     "iopub.status.idle": "2025-05-09T13:26:43.065845Z",
     "shell.execute_reply": "2025-05-09T13:26:43.065514Z"
    },
    "papermill": {
     "duration": 0.013235,
     "end_time": "2025-05-09T13:26:43.066404",
     "exception": false,
     "start_time": "2025-05-09T13:26:43.053169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 用于保存指标数据的列表\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "# test_losses = []\n",
    "# val_accuracies = []\n",
    "# val_f1_scores = []\n",
    "# val_auc_scores = []\n",
    "# test_accuracies = []\n",
    "# test_f1_scores = []\n",
    "# test_auc_scores = []\n",
    "# train_accuracies = []\n",
    "# train_f1_scores = []\n",
    "# train_auc_scores = []\n",
    "# train_c_index = []\n",
    "# val_c_index = []\n",
    "# test_c_index = []\n",
    "\n",
    "# def train_test(\n",
    "#         Methylation_train,\n",
    "#         Methylation_test,\n",
    "#         miRNASeq_train,\n",
    "#         miRNASeq_test,\n",
    "#         RNAseq_train,\n",
    "#         RNAseq_test,\n",
    "#         label_train,\n",
    "#         label_test, \n",
    "#         labels_tr_tensor,\n",
    "#         lr,\n",
    "#         depth, \n",
    "#         heads, \n",
    "#         dim_head,\n",
    "#         beta, \n",
    "#         attn_dropout,\n",
    "#         ff_dropout, \n",
    "#         classifier_dropout,\n",
    "#         adj_parameter,\n",
    "#         num_epoch_pretrain\n",
    "# ):\n",
    "    \n",
    "#     DEPTH = depth\n",
    "#     HEAD = heads\n",
    "#     HEAD_DIM = dim_head\n",
    "\n",
    "#     classifier_input = [Methylation_train.shape[1], miRNASeq_train.shape[1], RNAseq_train.shape[1]]\n",
    "#     classifier_dim = [\n",
    "#         [classifier_input[0], 300, 200],\n",
    "#         [classifier_input[1], 300, 200],\n",
    "#         [classifier_input[2], 300, 200]\n",
    "#     ]\n",
    "#     col_dim = 71\n",
    "\n",
    "#     # 确定 row_dim 和 embeding_num\n",
    "#     if num_view <= 2:\n",
    "#         embeding = True\n",
    "#         embeding_num = 71\n",
    "#         row_dim = embeding_num  # 请根据需要修改此处\n",
    "#     else:\n",
    "#         embeding = False\n",
    "#         row_dim = num_view\n",
    "#         embeding_num = 71\n",
    "    \n",
    "#     model = pathformer_model(\n",
    "#         row_dim=row_dim,\n",
    "#         col_dim=col_dim,\n",
    "#         depth=DEPTH,\n",
    "#         heads=HEAD,\n",
    "#         dim_head=HEAD_DIM,\n",
    "#         classifier_input=classifier_input,\n",
    "#         classifier_dim=classifier_dim,\n",
    "#         label_dim=label_dim,\n",
    "#         embeding=embeding,\n",
    "#         embeding_num=embeding_num,\n",
    "#         beta=beta,\n",
    "#         attn_dropout=attn_dropout,\n",
    "#         ff_dropout=ff_dropout,\n",
    "#         classifier_dropout=classifier_dropout,\n",
    "#         num_view=num_view,\n",
    "#         dim_hvcdn=dim_hvcdn\n",
    "#     )\n",
    "\n",
    "#     if torch.cuda.is_available():\n",
    "#         model.to(device)\n",
    "\n",
    "#     print(\"\\nPretrain GCNs...\")\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#     print(\"\\nTraining...\")\n",
    "#     for epoch in range(num_epoch_pretrain + 1):\n",
    "#         # 每一轮都进行训练\n",
    "#         train_loss, train_probs, labels_train_tensor = train_epoch(train_loader, model, optimizer, adj_parameter, epoch, output_attentions=False)\n",
    "#         train_acc = accuracy_score(labels_train_tensor, train_probs.argmax(1))\n",
    "#         train_f1 = f1_score(labels_train_tensor, train_probs.argmax(1))\n",
    "#         train_auc = roc_auc_score(labels_train_tensor, train_probs[:, 1])\n",
    "# #         print(f\"Train Epoch {epoch}, Loss: {train_loss}\")\n",
    "# #         print(f\"Train Epoch {epoch}, ACC: Train ACC: {train_acc:.3f}\")\n",
    "# #         print(f\"Train Epoch {epoch}, f1: Train F1: {train_f1:.3f}\")\n",
    "# #         print(f\"Train Epoch {epoch}, AUC: Train AUC: {train_auc:.3f}\")\n",
    "        \n",
    "#         # 每一轮都进行验证\n",
    "#         val_loss, val_probs, labels_val_tensor = val_epoch(val_loader, model, adj_parameter, epoch, output_attentions=False)\n",
    "#         val_acc = accuracy_score(labels_val_tensor, val_probs.argmax(1))\n",
    "#         val_f1 = f1_score(labels_val_tensor, val_probs.argmax(1))\n",
    "#         val_auc = roc_auc_score(labels_val_tensor, val_probs[:, 1])\n",
    "# #         print(f\"val Epoch {epoch}, Loss: {val_loss}\")\n",
    "# #         print(f\"val Epoch {epoch}, ACC: val ACC: {val_acc:.3f}\")\n",
    "# #         print(f\"val Epoch {epoch}, f1: val F1: {val_f1:.3f}\")\n",
    "# #         print(f\"val Epoch {epoch}, AUC: val AUC: {val_auc:.3f}\")\n",
    "        \n",
    "#         # 每一轮都进行测试\n",
    "#         test_loss, test_probs, labels_test_tensor = test_epoch(test_loader, model, adj_parameter, epoch, output_attentions=False)\n",
    "#         test_acc = accuracy_score(labels_test_tensor, test_probs.argmax(1))\n",
    "#         test_f1 = f1_score(labels_test_tensor, test_probs.argmax(1))\n",
    "#         test_auc = roc_auc_score(labels_test_tensor, test_probs[:, 1])\n",
    "# #         print(f\"test Epoch {epoch}, Loss: {test_loss}\")\n",
    "# #         print(f\"test Epoch {epoch}, ACC: test ACC: {test_acc:.3f}\")\n",
    "# #         print(f\"test Epoch {epoch}, f1: test F1: {test_f1:.3f}\")\n",
    "# #         print(f\"test Epoch {epoch}, AUC: test AUC: {test_auc:.3f}\")\n",
    "\n",
    "#         print(\"\\n\")\n",
    "\n",
    "#         # 保存训练指标\n",
    "#         train_losses.append(train_loss)\n",
    "#         train_accuracies.append(train_acc)\n",
    "#         train_f1_scores.append(train_f1)\n",
    "#         train_auc_scores.append(train_auc)\n",
    "        \n",
    "#         val_losses.append(val_loss)\n",
    "#         val_accuracies.append(val_acc)\n",
    "#         val_f1_scores.append(val_f1)\n",
    "#         val_auc_scores.append(val_auc)\n",
    "        \n",
    "#         test_losses.append(test_loss)\n",
    "#         test_accuracies.append(test_acc)\n",
    "#         test_f1_scores.append(test_f1)\n",
    "#         test_auc_scores.append(test_auc)\n",
    "\n",
    "#         # 绘制并保存指标图\n",
    "#         plot_metrics(lr, depth, heads, dim_head, beta, dropout=attn_dropout)\n",
    "\n",
    "#     # 返回最终的预测结果和指标\n",
    "#     final_predictions = test_probs.argmax(1)\n",
    "#     return final_predictions, test_acc, test_f1, test_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dccd421e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T13:26:43.086212Z",
     "iopub.status.busy": "2025-05-09T13:26:43.086034Z"
    },
    "papermill": {
     "duration": 469.783531,
     "end_time": "2025-05-09T13:34:32.859918",
     "exception": false,
     "start_time": "2025-05-09T13:26:43.076387",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "# 配置日志和路径\n",
    "LOG_FILE = '../../result/BLCA_parting/training_log_CMFM.txt'\n",
    "RESULT_FILE = '../../result/BLCA_parting/mogat_training_results.xlsx'\n",
    "os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=LOG_FILE,\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(message)s',\n",
    "    filemode='w'  # 每次运行覆盖旧日志\n",
    ")\n",
    "\n",
    "# 超参数配置（便于维护和修改）\n",
    "HP_CONFIG = {\n",
    "    'lr': [0.0001, 0.001, 0.01],\n",
    "    'depth': [2, 3, 4, 6],\n",
    "    'heads': [4, 6, 8],\n",
    "    'dim_head': [12, 16, 32, 64],\n",
    "    'dropout': [0.3],\n",
    "    'beta': [1],\n",
    "    'adj_parameter': [2, 4, 6, 8, 10]\n",
    "}\n",
    "\n",
    "# 计算总组合数\n",
    "total_combinations = (\n",
    "    len(HP_CONFIG['lr']) *\n",
    "    len(HP_CONFIG['depth']) *\n",
    "    len(HP_CONFIG['heads']) *\n",
    "    len(HP_CONFIG['dim_head']) *\n",
    "    len(HP_CONFIG['dropout']) *\n",
    "    len(HP_CONFIG['beta']) *\n",
    "    len(HP_CONFIG['adj_parameter'])\n",
    ")\n",
    "\n",
    "# 结果缓存配置\n",
    "BATCH_SIZE = 5  # 每5个结果写入一次文件\n",
    "results_cache = []\n",
    "\n",
    "def save_results(batch: list, force_save: bool = False):\n",
    "    \"\"\"批量保存结果到文件\"\"\"\n",
    "    if not force_save and len(batch) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        df = pd.DataFrame(batch)\n",
    "        header = not os.path.exists(RESULT_FILE)\n",
    "        \n",
    "        with pd.ExcelWriter(\n",
    "            RESULT_FILE,\n",
    "            mode='a' if os.path.exists(RESULT_FILE) else 'w',\n",
    "            engine='openpyxl',\n",
    "            if_sheet_exists='overlay' if os.path.exists(RESULT_FILE) else None\n",
    "        ) as writer:\n",
    "            df.to_excel(\n",
    "                writer,\n",
    "                index=False,\n",
    "                sheet_name='Results',\n",
    "                header=header,\n",
    "                startrow=writer.sheets['Results'].max_row if not header else 0\n",
    "            )\n",
    "        batch.clear()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"保存结果失败: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# 主训练循环\n",
    "current_combination = 0\n",
    "start_total = time.time()\n",
    "\n",
    "try:\n",
    "    for lr in HP_CONFIG['lr']:\n",
    "        for depth in HP_CONFIG['depth']:\n",
    "            for heads in HP_CONFIG['heads']:\n",
    "                for dim_head in HP_CONFIG['dim_head']:\n",
    "                    for dropout in HP_CONFIG['dropout']:\n",
    "                        for beta in HP_CONFIG['beta']:\n",
    "                            for adj_param in HP_CONFIG['adj_parameter']:\n",
    "                                current_combination += 1\n",
    "                                start_iter = time.time()\n",
    "                                \n",
    "                                # 日志记录当前参数\n",
    "                                param_str = (\n",
    "                                    f\"lr={lr}, depth={depth}, heads={heads}, \"\n",
    "                                    f\"dim_head={dim_head}, dropout={dropout}, \"\n",
    "                                    f\"beta={beta}, adj_parameter={adj_param} \"\n",
    "                                    f\"({current_combination}/{total_combinations})\"\n",
    "                                )\n",
    "                                logging.info(f\"开始训练: {param_str}\")\n",
    "\n",
    "                                try:\n",
    "                                    # 执行训练（假设train_test返回acc, f1_weighted, f1_macro）\n",
    "                                    acc, f1_weighted, f1_macro = train_test(  # 修改变量接收\n",
    "                                        Methylation_train=Methylation_train,\n",
    "                                        miRNASeq_train=miRNASeq_train,\n",
    "                                        RNAseq_train=RNAseq_train,\n",
    "                                        lr=lr,\n",
    "                                        depth=depth,\n",
    "                                        heads=heads,\n",
    "                                        dim_head=dim_head,\n",
    "                                        beta=beta,\n",
    "                                        attn_dropout=dropout,\n",
    "                                        ff_dropout=dropout,\n",
    "                                        classifier_dropout=dropout,\n",
    "                                        adj_parameter=adj_param,\n",
    "                                        num_epoch_pretrain=1500\n",
    "                                    )\n",
    "\n",
    "                                    # 缓存结果（修改键名）\n",
    "                                    results_cache.append({\n",
    "                                        'lr': lr,\n",
    "                                        'depth': depth,\n",
    "                                        'heads': heads,\n",
    "                                        'dim_head': dim_head,\n",
    "                                        'dropout': dropout,\n",
    "                                        'beta': beta,\n",
    "                                        'adj_parameter': adj_param,\n",
    "                                        'accuracy': acc,\n",
    "                                        'f1_weighted': f1_weighted,  # 修改键名\n",
    "                                        'f1_macro': f1_macro          # 新增键\n",
    "                                    })\n",
    "\n",
    "                                    # 定期保存\n",
    "                                    save_results(results_cache)\n",
    "\n",
    "                                    # 记录成功信息（修改日志输出）\n",
    "                                    elapsed = time.time() - start_iter\n",
    "                                    logging.info(\n",
    "                                        f\"完成: {param_str} | \"\n",
    "                                        f\"耗时: {elapsed:.1f}s | \"\n",
    "                                        f\"结果: ACC={acc:.4f}, F1_weighted={f1_weighted:.4f}, F1_macro={f1_macro:.4f}\"  # 修改指标名称\n",
    "                                    )\n",
    "\n",
    "                                except torch.cuda.OutOfMemoryError:\n",
    "                                    logging.warning(f\"显存不足，跳过组合: {param_str}\")\n",
    "                                    torch.cuda.empty_cache()\n",
    "                                except Exception as e:\n",
    "                                    logging.error(f\"训练失败: {param_str} | 错误: {str(e)}\")\n",
    "                                    continue\n",
    "                                # 用于保存指标数据的列表（移除了AUC相关指标）\n",
    "                                train_losses = []\n",
    "                                val_losses = []\n",
    "                                test_losses = []\n",
    "                                val_accuracies = []\n",
    "                                val_f1_weighted_scores = []\n",
    "                                val_f1_macro_scores = []\n",
    "                                test_accuracies = []\n",
    "                                test_f1_weighted_scores = []\n",
    "                                test_f1_macro_scores = []\n",
    "                                train_accuracies = []\n",
    "                                train_f1_weighted_scores = []\n",
    "                                train_f1_macro_scores = []\n",
    "\n",
    "    # 最后强制保存剩余结果\n",
    "    save_results(results_cache, force_save=True)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logging.warning(\"用户中断训练，保存已完成的实验结果...\")\n",
    "    save_results(results_cache, force_save=True)\n",
    "finally:\n",
    "    total_time = time.time() - start_total\n",
    "    logging.info(f\"全部完成! 总耗时: {total_time/3600:.2f} 小时\")\n",
    "\n",
    "print(\"实验完成，结果保存在:\", RESULT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7c5f32",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import time\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "# # 创建一个 DataFrame 用于存储结果\n",
    "# results_df = pd.DataFrame(columns=['lr', 'depth', 'heads', 'dim_head', 'dropout', 'beta', 'accuracy', 'f1_score', 'auc'])\n",
    "\n",
    "# # 设置单一的超参数组合\n",
    "# lr = 0.0001  # 你想要的学习率\n",
    "# depth = 6  # 你想要的层数\n",
    "# heads = 4  # 你想要的注意力头数\n",
    "# dim_head = 32  # 你想要的注意力头的维度\n",
    "# dropout = 0.5  # 你想要的 dropout 比例\n",
    "# beta = 1  # 你想要的 beta 值\n",
    "\n",
    "# # 训练参数组合\n",
    "# start_time = time.time()  # 记录开始时间\n",
    "# print(f\"训练参数：lr={lr}, depth={depth}, heads={heads}, \"\n",
    "#       f\"dim_head={dim_head}, dropout={dropout}, beta={beta}\")\n",
    "\n",
    "# # 训练函数假设已定义，返回最后的预测和各项指标\n",
    "# try:\n",
    "#     last_predictions, acc, f1, auc = train_test(\n",
    "#         Methylation_train=Methylation_train,\n",
    "#         Methylation_test=Methylation_test,\n",
    "#         miRNASeq_train=miRNASeq_train,\n",
    "#         miRNASeq_test=miRNASeq_test,\n",
    "#         RNAseq_train=RNAseq_train,\n",
    "#         RNAseq_test=RNAseq_test,\n",
    "#         label_train=labels_train,\n",
    "#         label_test=labels_test,\n",
    "#         labels_tr_tensor=labels_tr_tensor,\n",
    "#         lr=lr,\n",
    "#         depth=depth,\n",
    "#         heads=heads,\n",
    "#         dim_head=dim_head,\n",
    "#         beta=beta,\n",
    "#         attn_dropout=dropout,\n",
    "#         ff_dropout=dropout,\n",
    "#         classifier_dropout=dropout,\n",
    "#         num_epoch_pretrain=2000,\n",
    "#     )\n",
    "\n",
    "#     # 清空列表以便下一个超参数组合使用\n",
    "#     train_losses = []\n",
    "#     val_losses = []\n",
    "#     test_losses = []\n",
    "    \n",
    "#     train_accuracies = []\n",
    "#     val_accuracies = []\n",
    "#     test_accuracies = []\n",
    "    \n",
    "#     train_f1_scores = []\n",
    "#     val_f1_scores = []\n",
    "#     test_f1_scores = []\n",
    "    \n",
    "#     train_auc_scores = []\n",
    "#     val_auc_scores = []\n",
    "#     test_auc_scores = []\n",
    "    \n",
    "#     # 检查最后的预测结果是否全部一致\n",
    "#     if last_predictions is not None:\n",
    "#         last_predictions_tensor = torch.tensor(last_predictions)  # 转换为张量\n",
    "#         if not torch.all(last_predictions_tensor == last_predictions_tensor[0]):\n",
    "#             print(\"最后一轮的预测结果不一致，保存该参数组合和评价指标。\")\n",
    "            \n",
    "#             # 保存结果到 DataFrame\n",
    "#             result = {\n",
    "#                 'lr': lr,\n",
    "#                 'depth': depth,\n",
    "#                 'heads': heads,\n",
    "#                 'dim_head': dim_head,\n",
    "#                 'dropout': dropout,\n",
    "#                 'beta': beta,\n",
    "#                 'accuracy': acc,\n",
    "#                 'f1_score': f1,\n",
    "#                 'auc': auc\n",
    "#             }\n",
    "\n",
    "#             result_df = pd.DataFrame([result])\n",
    "\n",
    "#             # 确保 result_df 不为空且不全为 NA\n",
    "#             if result_df.notnull().any().any():\n",
    "#                 results_df = pd.concat([results_df, result_df], ignore_index=True)\n",
    "\n",
    "#                 # 检查 Excel 文件是否存在，如果不存在则创建\n",
    "#                 if not os.path.isfile('../../result/BLCA_parting/mogat_training_results.xlsx'):\n",
    "#                     results_df.to_excel('../../result/BLCA_parting/mogat_training_results.xlsx', index=False, sheet_name='Results')\n",
    "#                 else:\n",
    "#                     # 立即写入 Excel 文件\n",
    "#                     with pd.ExcelWriter('../../result/BLCA_parting/mogat_training_results.xlsx', mode='a', engine='openpyxl', if_sheet_exists='overlay') as writer:\n",
    "#                         results_df.to_excel(writer, index=False, sheet_name='Results', startrow=writer.sheets['Results'].max_row)\n",
    "#         else:\n",
    "#             print(\"最后一轮的预测结果全部一致，不保存该参数组合。\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"训练参数组合 lr={lr}, depth={depth}, heads={heads}, dim_head={dim_head}, dropout={dropout}, beta={beta} 时发生错误: {e}\")\n",
    "\n",
    "# elapsed_time = time.time() - start_time\n",
    "# print(f\"训练完成，耗时: {elapsed_time:.2f} 秒\")\n",
    "\n",
    "# # 最终保存（如果需要）\n",
    "# if not results_df.empty:\n",
    "#     results_df.to_excel('../../result/BLCA_parting/mogat_training_results.xlsx', index=False, sheet_name='Results')\n",
    "#     print(\"参数组合和评价指标已保存为 mogat_training_results.xlsx 文件。\")\n",
    "# else:\n",
    "#     print(\"没有符合条件的参数组合，未保存任何结果。\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 476.66576,
   "end_time": "2025-05-09T13:34:33.858832",
   "environment_variables": {},
   "exception": null,
   "input_path": "train_test_CMFM.ipynb",
   "output_path": "output_CMFM.ipynb",
   "parameters": {},
   "start_time": "2025-05-09T13:26:37.193072",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}