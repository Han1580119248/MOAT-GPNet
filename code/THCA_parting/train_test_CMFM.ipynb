{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b87dbca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "import torch.nn.functional as F\n",
    "import nbimporter\n",
    "from network import *\n",
    "import os\n",
    "from utils import *\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, ConcatDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66ceb4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446fb3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class=4\n",
    "num_epoch=2500\n",
    "test_inverval = 50\n",
    "num_view = 3\n",
    "dim_hvcdn= 64\n",
    "batch_size = 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bf9054",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.108356</td>\n",
       "      <td>0.111206</td>\n",
       "      <td>0.212663</td>\n",
       "      <td>0.441164</td>\n",
       "      <td>0.263589</td>\n",
       "      <td>0.373276</td>\n",
       "      <td>0.215142</td>\n",
       "      <td>0.231430</td>\n",
       "      <td>0.415421</td>\n",
       "      <td>0.524971</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563012</td>\n",
       "      <td>0.116748</td>\n",
       "      <td>0.637430</td>\n",
       "      <td>0.042472</td>\n",
       "      <td>0.473440</td>\n",
       "      <td>0.252532</td>\n",
       "      <td>0.519429</td>\n",
       "      <td>0.432116</td>\n",
       "      <td>0.350196</td>\n",
       "      <td>0.163122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.095593</td>\n",
       "      <td>0.092424</td>\n",
       "      <td>0.067180</td>\n",
       "      <td>0.450420</td>\n",
       "      <td>0.283063</td>\n",
       "      <td>0.323043</td>\n",
       "      <td>0.227279</td>\n",
       "      <td>0.196455</td>\n",
       "      <td>0.401015</td>\n",
       "      <td>0.523737</td>\n",
       "      <td>...</td>\n",
       "      <td>0.419127</td>\n",
       "      <td>0.103232</td>\n",
       "      <td>0.566139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.461772</td>\n",
       "      <td>0.219299</td>\n",
       "      <td>0.321675</td>\n",
       "      <td>0.352588</td>\n",
       "      <td>0.311547</td>\n",
       "      <td>0.142307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.155456</td>\n",
       "      <td>0.121434</td>\n",
       "      <td>0.104714</td>\n",
       "      <td>0.471631</td>\n",
       "      <td>0.324398</td>\n",
       "      <td>0.355628</td>\n",
       "      <td>0.241474</td>\n",
       "      <td>0.209778</td>\n",
       "      <td>0.406169</td>\n",
       "      <td>0.520879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.459746</td>\n",
       "      <td>0.149027</td>\n",
       "      <td>0.670796</td>\n",
       "      <td>0.075482</td>\n",
       "      <td>0.537622</td>\n",
       "      <td>0.272693</td>\n",
       "      <td>0.466133</td>\n",
       "      <td>0.420018</td>\n",
       "      <td>0.332122</td>\n",
       "      <td>0.206836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.165101</td>\n",
       "      <td>0.152276</td>\n",
       "      <td>0.113309</td>\n",
       "      <td>0.466523</td>\n",
       "      <td>0.298085</td>\n",
       "      <td>0.335604</td>\n",
       "      <td>0.218271</td>\n",
       "      <td>0.169460</td>\n",
       "      <td>0.420195</td>\n",
       "      <td>0.510951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.519247</td>\n",
       "      <td>0.120110</td>\n",
       "      <td>0.674506</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.489361</td>\n",
       "      <td>0.290446</td>\n",
       "      <td>0.497281</td>\n",
       "      <td>0.351045</td>\n",
       "      <td>0.333715</td>\n",
       "      <td>0.170886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.125337</td>\n",
       "      <td>0.134063</td>\n",
       "      <td>0.079028</td>\n",
       "      <td>0.468523</td>\n",
       "      <td>0.201158</td>\n",
       "      <td>0.349485</td>\n",
       "      <td>0.275529</td>\n",
       "      <td>0.211588</td>\n",
       "      <td>0.435993</td>\n",
       "      <td>0.462107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.502319</td>\n",
       "      <td>0.139415</td>\n",
       "      <td>0.587636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.446979</td>\n",
       "      <td>0.214085</td>\n",
       "      <td>0.465480</td>\n",
       "      <td>0.330512</td>\n",
       "      <td>0.300085</td>\n",
       "      <td>0.175081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.100223</td>\n",
       "      <td>0.029615</td>\n",
       "      <td>0.138830</td>\n",
       "      <td>0.377822</td>\n",
       "      <td>0.209048</td>\n",
       "      <td>0.439149</td>\n",
       "      <td>0.206836</td>\n",
       "      <td>0.306878</td>\n",
       "      <td>0.454488</td>\n",
       "      <td>0.502880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500366</td>\n",
       "      <td>0.126615</td>\n",
       "      <td>0.601225</td>\n",
       "      <td>0.158056</td>\n",
       "      <td>0.571851</td>\n",
       "      <td>0.304004</td>\n",
       "      <td>0.468395</td>\n",
       "      <td>0.514099</td>\n",
       "      <td>0.322872</td>\n",
       "      <td>0.305926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.139415</td>\n",
       "      <td>0.092424</td>\n",
       "      <td>0.129764</td>\n",
       "      <td>0.456725</td>\n",
       "      <td>0.302057</td>\n",
       "      <td>0.373519</td>\n",
       "      <td>0.183568</td>\n",
       "      <td>0.199217</td>\n",
       "      <td>0.393360</td>\n",
       "      <td>0.540179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.512199</td>\n",
       "      <td>0.131005</td>\n",
       "      <td>0.624044</td>\n",
       "      <td>0.022105</td>\n",
       "      <td>0.465087</td>\n",
       "      <td>0.234854</td>\n",
       "      <td>0.439694</td>\n",
       "      <td>0.417872</td>\n",
       "      <td>0.288097</td>\n",
       "      <td>0.175540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.109788</td>\n",
       "      <td>0.140578</td>\n",
       "      <td>0.123398</td>\n",
       "      <td>0.483855</td>\n",
       "      <td>0.241474</td>\n",
       "      <td>0.337932</td>\n",
       "      <td>0.275997</td>\n",
       "      <td>0.238201</td>\n",
       "      <td>0.392509</td>\n",
       "      <td>0.498019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.514852</td>\n",
       "      <td>0.109073</td>\n",
       "      <td>0.599984</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.471568</td>\n",
       "      <td>0.308950</td>\n",
       "      <td>0.480523</td>\n",
       "      <td>0.342632</td>\n",
       "      <td>0.265855</td>\n",
       "      <td>0.170412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0.109788</td>\n",
       "      <td>0.055338</td>\n",
       "      <td>0.109788</td>\n",
       "      <td>0.475411</td>\n",
       "      <td>0.292759</td>\n",
       "      <td>0.379695</td>\n",
       "      <td>0.280820</td>\n",
       "      <td>0.255240</td>\n",
       "      <td>0.380854</td>\n",
       "      <td>0.525627</td>\n",
       "      <td>...</td>\n",
       "      <td>0.491298</td>\n",
       "      <td>0.101735</td>\n",
       "      <td>0.628378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499588</td>\n",
       "      <td>0.293383</td>\n",
       "      <td>0.396511</td>\n",
       "      <td>0.390899</td>\n",
       "      <td>0.320813</td>\n",
       "      <td>0.241768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0.068125</td>\n",
       "      <td>0.034420</td>\n",
       "      <td>0.124693</td>\n",
       "      <td>0.394418</td>\n",
       "      <td>0.149573</td>\n",
       "      <td>0.243230</td>\n",
       "      <td>0.218614</td>\n",
       "      <td>0.181821</td>\n",
       "      <td>0.390250</td>\n",
       "      <td>0.407043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.415604</td>\n",
       "      <td>0.068125</td>\n",
       "      <td>0.516207</td>\n",
       "      <td>0.025907</td>\n",
       "      <td>0.430470</td>\n",
       "      <td>0.213376</td>\n",
       "      <td>0.462173</td>\n",
       "      <td>0.349912</td>\n",
       "      <td>0.241474</td>\n",
       "      <td>0.165592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.108356  0.111206  0.212663  0.441164  0.263589  0.373276  0.215142   \n",
       "1    0.095593  0.092424  0.067180  0.450420  0.283063  0.323043  0.227279   \n",
       "2    0.155456  0.121434  0.104714  0.471631  0.324398  0.355628  0.241474   \n",
       "3    0.165101  0.152276  0.113309  0.466523  0.298085  0.335604  0.218271   \n",
       "4    0.125337  0.134063  0.079028  0.468523  0.201158  0.349485  0.275529   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "205  0.100223  0.029615  0.138830  0.377822  0.209048  0.439149  0.206836   \n",
       "206  0.139415  0.092424  0.129764  0.456725  0.302057  0.373519  0.183568   \n",
       "207  0.109788  0.140578  0.123398  0.483855  0.241474  0.337932  0.275997   \n",
       "208  0.109788  0.055338  0.109788  0.475411  0.292759  0.379695  0.280820   \n",
       "209  0.068125  0.034420  0.124693  0.394418  0.149573  0.243230  0.218614   \n",
       "\n",
       "            7         8         9  ...       190       191       192  \\\n",
       "0    0.231430  0.415421  0.524971  ...  0.563012  0.116748  0.637430   \n",
       "1    0.196455  0.401015  0.523737  ...  0.419127  0.103232  0.566139   \n",
       "2    0.209778  0.406169  0.520879  ...  0.459746  0.149027  0.670796   \n",
       "3    0.169460  0.420195  0.510951  ...  0.519247  0.120110  0.674506   \n",
       "4    0.211588  0.435993  0.462107  ...  0.502319  0.139415  0.587636   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "205  0.306878  0.454488  0.502880  ...  0.500366  0.126615  0.601225   \n",
       "206  0.199217  0.393360  0.540179  ...  0.512199  0.131005  0.624044   \n",
       "207  0.238201  0.392509  0.498019  ...  0.514852  0.109073  0.599984   \n",
       "208  0.255240  0.380854  0.525627  ...  0.491298  0.101735  0.628378   \n",
       "209  0.181821  0.390250  0.407043  ...  0.415604  0.068125  0.516207   \n",
       "\n",
       "          193       194       195       196       197       198       199  \n",
       "0    0.042472  0.473440  0.252532  0.519429  0.432116  0.350196  0.163122  \n",
       "1    0.000000  0.461772  0.219299  0.321675  0.352588  0.311547  0.142307  \n",
       "2    0.075482  0.537622  0.272693  0.466133  0.420018  0.332122  0.206836  \n",
       "3    0.000000  0.489361  0.290446  0.497281  0.351045  0.333715  0.170886  \n",
       "4    0.000000  0.446979  0.214085  0.465480  0.330512  0.300085  0.175081  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "205  0.158056  0.571851  0.304004  0.468395  0.514099  0.322872  0.305926  \n",
       "206  0.022105  0.465087  0.234854  0.439694  0.417872  0.288097  0.175540  \n",
       "207  0.000000  0.471568  0.308950  0.480523  0.342632  0.265855  0.170412  \n",
       "208  0.000000  0.499588  0.293383  0.396511  0.390899  0.320813  0.241768  \n",
       "209  0.025907  0.430470  0.213376  0.462173  0.349912  0.241474  0.165592  \n",
       "\n",
       "[210 rows x 200 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取制表符分隔的CSV文件\n",
    "Methylation_train = pd.read_csv('../../DATA/THCA_parting/1_tr.csv')\n",
    "# 显示 DataFrame\n",
    "Methylation_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96898d81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.029615</td>\n",
       "      <td>0.025907</td>\n",
       "      <td>0.110499</td>\n",
       "      <td>0.393784</td>\n",
       "      <td>0.227603</td>\n",
       "      <td>0.294212</td>\n",
       "      <td>0.184434</td>\n",
       "      <td>0.119444</td>\n",
       "      <td>0.445864</td>\n",
       "      <td>0.481583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.456308</td>\n",
       "      <td>0.022105</td>\n",
       "      <td>0.430470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.390250</td>\n",
       "      <td>0.204968</td>\n",
       "      <td>0.463638</td>\n",
       "      <td>0.321331</td>\n",
       "      <td>0.243810</td>\n",
       "      <td>0.164609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.165101</td>\n",
       "      <td>0.083345</td>\n",
       "      <td>0.058388</td>\n",
       "      <td>0.476021</td>\n",
       "      <td>0.300283</td>\n",
       "      <td>0.385066</td>\n",
       "      <td>0.285053</td>\n",
       "      <td>0.202311</td>\n",
       "      <td>0.393148</td>\n",
       "      <td>0.497334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.572936</td>\n",
       "      <td>0.161620</td>\n",
       "      <td>0.724728</td>\n",
       "      <td>0.069999</td>\n",
       "      <td>0.519657</td>\n",
       "      <td>0.298085</td>\n",
       "      <td>0.441087</td>\n",
       "      <td>0.439383</td>\n",
       "      <td>0.350196</td>\n",
       "      <td>0.244100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.128512</td>\n",
       "      <td>0.139415</td>\n",
       "      <td>0.176454</td>\n",
       "      <td>0.484605</td>\n",
       "      <td>0.274825</td>\n",
       "      <td>0.362444</td>\n",
       "      <td>0.271494</td>\n",
       "      <td>0.234237</td>\n",
       "      <td>0.429391</td>\n",
       "      <td>0.531031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.498962</td>\n",
       "      <td>0.138830</td>\n",
       "      <td>0.687974</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.502421</td>\n",
       "      <td>0.270044</td>\n",
       "      <td>0.402720</td>\n",
       "      <td>0.428051</td>\n",
       "      <td>0.326075</td>\n",
       "      <td>0.161620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.138830</td>\n",
       "      <td>0.140578</td>\n",
       "      <td>0.061377</td>\n",
       "      <td>0.510517</td>\n",
       "      <td>0.343673</td>\n",
       "      <td>0.355628</td>\n",
       "      <td>0.269313</td>\n",
       "      <td>0.162122</td>\n",
       "      <td>0.475838</td>\n",
       "      <td>0.581641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.498596</td>\n",
       "      <td>0.188270</td>\n",
       "      <td>0.632206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.477054</td>\n",
       "      <td>0.236385</td>\n",
       "      <td>0.366821</td>\n",
       "      <td>0.417692</td>\n",
       "      <td>0.355217</td>\n",
       "      <td>0.211588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.044695</td>\n",
       "      <td>0.088366</td>\n",
       "      <td>0.079901</td>\n",
       "      <td>0.380044</td>\n",
       "      <td>0.191186</td>\n",
       "      <td>0.285930</td>\n",
       "      <td>0.174621</td>\n",
       "      <td>0.173696</td>\n",
       "      <td>0.340225</td>\n",
       "      <td>0.502268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.463240</td>\n",
       "      <td>0.051172</td>\n",
       "      <td>0.524663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.439538</td>\n",
       "      <td>0.207207</td>\n",
       "      <td>0.541598</td>\n",
       "      <td>0.422831</td>\n",
       "      <td>0.263589</td>\n",
       "      <td>0.114003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.071849</td>\n",
       "      <td>0.052224</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.394629</td>\n",
       "      <td>0.275060</td>\n",
       "      <td>0.323383</td>\n",
       "      <td>0.189943</td>\n",
       "      <td>0.213020</td>\n",
       "      <td>0.447939</td>\n",
       "      <td>0.497545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563792</td>\n",
       "      <td>0.071849</td>\n",
       "      <td>0.430305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.443608</td>\n",
       "      <td>0.223343</td>\n",
       "      <td>0.433582</td>\n",
       "      <td>0.418949</td>\n",
       "      <td>0.323383</td>\n",
       "      <td>0.168982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.112611</td>\n",
       "      <td>0.085876</td>\n",
       "      <td>0.079028</td>\n",
       "      <td>0.430966</td>\n",
       "      <td>0.335291</td>\n",
       "      <td>0.307634</td>\n",
       "      <td>0.267841</td>\n",
       "      <td>0.198825</td>\n",
       "      <td>0.396926</td>\n",
       "      <td>0.465676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.475104</td>\n",
       "      <td>0.108356</td>\n",
       "      <td>0.611999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.467881</td>\n",
       "      <td>0.235161</td>\n",
       "      <td>0.407623</td>\n",
       "      <td>0.346898</td>\n",
       "      <td>0.283508</td>\n",
       "      <td>0.174621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.103232</td>\n",
       "      <td>0.131622</td>\n",
       "      <td>0.136466</td>\n",
       "      <td>0.505653</td>\n",
       "      <td>0.215142</td>\n",
       "      <td>0.403416</td>\n",
       "      <td>0.290869</td>\n",
       "      <td>0.210505</td>\n",
       "      <td>0.432361</td>\n",
       "      <td>0.578153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410961</td>\n",
       "      <td>0.079028</td>\n",
       "      <td>0.590601</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.496486</td>\n",
       "      <td>0.277622</td>\n",
       "      <td>0.484317</td>\n",
       "      <td>0.425079</td>\n",
       "      <td>0.332601</td>\n",
       "      <td>0.153875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.173696</td>\n",
       "      <td>0.062360</td>\n",
       "      <td>0.094017</td>\n",
       "      <td>0.466133</td>\n",
       "      <td>0.201158</td>\n",
       "      <td>0.340225</td>\n",
       "      <td>0.194853</td>\n",
       "      <td>0.163619</td>\n",
       "      <td>0.418859</td>\n",
       "      <td>0.426019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.535348</td>\n",
       "      <td>0.119444</td>\n",
       "      <td>0.562944</td>\n",
       "      <td>0.015543</td>\n",
       "      <td>0.448233</td>\n",
       "      <td>0.228890</td>\n",
       "      <td>0.497966</td>\n",
       "      <td>0.385514</td>\n",
       "      <td>0.319947</td>\n",
       "      <td>0.225324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.074582</td>\n",
       "      <td>0.058388</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.426869</td>\n",
       "      <td>0.242647</td>\n",
       "      <td>0.316252</td>\n",
       "      <td>0.216888</td>\n",
       "      <td>0.199607</td>\n",
       "      <td>0.366821</td>\n",
       "      <td>0.533039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.452718</td>\n",
       "      <td>0.084194</td>\n",
       "      <td>0.568974</td>\n",
       "      <td>0.044695</td>\n",
       "      <td>0.494669</td>\n",
       "      <td>0.260513</td>\n",
       "      <td>0.501705</td>\n",
       "      <td>0.383259</td>\n",
       "      <td>0.249496</td>\n",
       "      <td>0.190773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0   0.029615  0.025907  0.110499  0.393784  0.227603  0.294212  0.184434   \n",
       "1   0.165101  0.083345  0.058388  0.476021  0.300283  0.385066  0.285053   \n",
       "2   0.128512  0.139415  0.176454  0.484605  0.274825  0.362444  0.271494   \n",
       "3   0.138830  0.140578  0.061377  0.510517  0.343673  0.355628  0.269313   \n",
       "4   0.044695  0.088366  0.079901  0.380044  0.191186  0.285930  0.174621   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "66  0.071849  0.052224  0.081633  0.394629  0.275060  0.323383  0.189943   \n",
       "67  0.112611  0.085876  0.079028  0.430966  0.335291  0.307634  0.267841   \n",
       "68  0.103232  0.131622  0.136466  0.505653  0.215142  0.403416  0.290869   \n",
       "69  0.173696  0.062360  0.094017  0.466133  0.201158  0.340225  0.194853   \n",
       "70  0.074582  0.058388  0.081633  0.426869  0.242647  0.316252  0.216888   \n",
       "\n",
       "           7         8         9  ...       190       191       192       193  \\\n",
       "0   0.119444  0.445864  0.481583  ...  0.456308  0.022105  0.430470  0.000000   \n",
       "1   0.202311  0.393148  0.497334  ...  0.572936  0.161620  0.724728  0.069999   \n",
       "2   0.234237  0.429391  0.531031  ...  0.498962  0.138830  0.687974  0.000000   \n",
       "3   0.162122  0.475838  0.581641  ...  0.498596  0.188270  0.632206  0.000000   \n",
       "4   0.173696  0.340225  0.502268  ...  0.463240  0.051172  0.524663  0.000000   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "66  0.213020  0.447939  0.497545  ...  0.563792  0.071849  0.430305  0.000000   \n",
       "67  0.198825  0.396926  0.465676  ...  0.475104  0.108356  0.611999  0.000000   \n",
       "68  0.210505  0.432361  0.578153  ...  0.410961  0.079028  0.590601  0.000000   \n",
       "69  0.163619  0.418859  0.426019  ...  0.535348  0.119444  0.562944  0.015543   \n",
       "70  0.199607  0.366821  0.533039  ...  0.452718  0.084194  0.568974  0.044695   \n",
       "\n",
       "         194       195       196       197       198       199  \n",
       "0   0.390250  0.204968  0.463638  0.321331  0.243810  0.164609  \n",
       "1   0.519657  0.298085  0.441087  0.439383  0.350196  0.244100  \n",
       "2   0.502421  0.270044  0.402720  0.428051  0.326075  0.161620  \n",
       "3   0.477054  0.236385  0.366821  0.417692  0.355217  0.211588  \n",
       "4   0.439538  0.207207  0.541598  0.422831  0.263589  0.114003  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "66  0.443608  0.223343  0.433582  0.418949  0.323383  0.168982  \n",
       "67  0.467881  0.235161  0.407623  0.346898  0.283508  0.174621  \n",
       "68  0.496486  0.277622  0.484317  0.425079  0.332601  0.153875  \n",
       "69  0.448233  0.228890  0.497966  0.385514  0.319947  0.225324  \n",
       "70  0.494669  0.260513  0.501705  0.383259  0.249496  0.190773  \n",
       "\n",
       "[71 rows x 200 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取制表符分隔的CSV文件\n",
    "Methylation_test = pd.read_csv('../../DATA/THCA_parting/1_te.csv')\n",
    "# 显示 DataFrame\n",
    "Methylation_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658d5fc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.141733</td>\n",
       "      <td>0.106910</td>\n",
       "      <td>0.153344</td>\n",
       "      <td>0.493644</td>\n",
       "      <td>0.257636</td>\n",
       "      <td>0.364777</td>\n",
       "      <td>0.219981</td>\n",
       "      <td>0.252259</td>\n",
       "      <td>0.440933</td>\n",
       "      <td>0.539385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.582180</td>\n",
       "      <td>0.122747</td>\n",
       "      <td>0.552270</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.472881</td>\n",
       "      <td>0.277853</td>\n",
       "      <td>0.450420</td>\n",
       "      <td>0.411714</td>\n",
       "      <td>0.346607</td>\n",
       "      <td>0.226955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.028390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025907</td>\n",
       "      <td>0.412089</td>\n",
       "      <td>0.339159</td>\n",
       "      <td>0.326409</td>\n",
       "      <td>0.222676</td>\n",
       "      <td>0.185724</td>\n",
       "      <td>0.374486</td>\n",
       "      <td>0.467301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.458928</td>\n",
       "      <td>0.080770</td>\n",
       "      <td>0.555985</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.453428</td>\n",
       "      <td>0.246680</td>\n",
       "      <td>0.334347</td>\n",
       "      <td>0.376282</td>\n",
       "      <td>0.275763</td>\n",
       "      <td>0.107634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.174621</td>\n",
       "      <td>0.119444</td>\n",
       "      <td>0.144015</td>\n",
       "      <td>0.472943</td>\n",
       "      <td>0.271975</td>\n",
       "      <td>0.361000</td>\n",
       "      <td>0.229209</td>\n",
       "      <td>0.221335</td>\n",
       "      <td>0.429724</td>\n",
       "      <td>0.526628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.536246</td>\n",
       "      <td>0.159083</td>\n",
       "      <td>0.623637</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.497386</td>\n",
       "      <td>0.282840</td>\n",
       "      <td>0.459338</td>\n",
       "      <td>0.405583</td>\n",
       "      <td>0.337160</td>\n",
       "      <td>0.222342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.090002</td>\n",
       "      <td>0.090814</td>\n",
       "      <td>0.120774</td>\n",
       "      <td>0.400915</td>\n",
       "      <td>0.336384</td>\n",
       "      <td>0.306688</td>\n",
       "      <td>0.187426</td>\n",
       "      <td>0.226630</td>\n",
       "      <td>0.313737</td>\n",
       "      <td>0.474120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.463836</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.578060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501140</td>\n",
       "      <td>0.267594</td>\n",
       "      <td>0.528654</td>\n",
       "      <td>0.419395</td>\n",
       "      <td>0.273644</td>\n",
       "      <td>0.161116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.076376</td>\n",
       "      <td>0.142307</td>\n",
       "      <td>0.124047</td>\n",
       "      <td>0.425849</td>\n",
       "      <td>0.137060</td>\n",
       "      <td>0.304390</td>\n",
       "      <td>0.171828</td>\n",
       "      <td>0.135269</td>\n",
       "      <td>0.431048</td>\n",
       "      <td>0.441856</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446534</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.564433</td>\n",
       "      <td>0.051172</td>\n",
       "      <td>0.414318</td>\n",
       "      <td>0.232996</td>\n",
       "      <td>0.479574</td>\n",
       "      <td>0.380276</td>\n",
       "      <td>0.285492</td>\n",
       "      <td>0.148478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.094807</td>\n",
       "      <td>0.082492</td>\n",
       "      <td>0.042472</td>\n",
       "      <td>0.443077</td>\n",
       "      <td>0.211947</td>\n",
       "      <td>0.317140</td>\n",
       "      <td>0.168021</td>\n",
       "      <td>0.188690</td>\n",
       "      <td>0.397753</td>\n",
       "      <td>0.453853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.503894</td>\n",
       "      <td>0.108356</td>\n",
       "      <td>0.410110</td>\n",
       "      <td>0.007266</td>\n",
       "      <td>0.416697</td>\n",
       "      <td>0.219640</td>\n",
       "      <td>0.508766</td>\n",
       "      <td>0.398576</td>\n",
       "      <td>0.322018</td>\n",
       "      <td>0.154404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.132236</td>\n",
       "      <td>0.106181</td>\n",
       "      <td>0.060387</td>\n",
       "      <td>0.449694</td>\n",
       "      <td>0.173696</td>\n",
       "      <td>0.311363</td>\n",
       "      <td>0.216193</td>\n",
       "      <td>0.189526</td>\n",
       "      <td>0.367328</td>\n",
       "      <td>0.497913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.474982</td>\n",
       "      <td>0.116748</td>\n",
       "      <td>0.634347</td>\n",
       "      <td>0.018202</td>\n",
       "      <td>0.449184</td>\n",
       "      <td>0.206464</td>\n",
       "      <td>0.509352</td>\n",
       "      <td>0.358338</td>\n",
       "      <td>0.268579</td>\n",
       "      <td>0.142307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.094017</td>\n",
       "      <td>0.109788</td>\n",
       "      <td>0.089186</td>\n",
       "      <td>0.492175</td>\n",
       "      <td>0.180054</td>\n",
       "      <td>0.393041</td>\n",
       "      <td>0.249496</td>\n",
       "      <td>0.314460</td>\n",
       "      <td>0.467042</td>\n",
       "      <td>0.501088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578979</td>\n",
       "      <td>0.117427</td>\n",
       "      <td>0.656068</td>\n",
       "      <td>0.034420</td>\n",
       "      <td>0.463372</td>\n",
       "      <td>0.264348</td>\n",
       "      <td>0.453144</td>\n",
       "      <td>0.342183</td>\n",
       "      <td>0.305735</td>\n",
       "      <td>0.196056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.052224</td>\n",
       "      <td>0.114694</td>\n",
       "      <td>0.053269</td>\n",
       "      <td>0.441395</td>\n",
       "      <td>0.206836</td>\n",
       "      <td>0.331480</td>\n",
       "      <td>0.254972</td>\n",
       "      <td>0.185724</td>\n",
       "      <td>0.399495</td>\n",
       "      <td>0.506450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.478019</td>\n",
       "      <td>0.106181</td>\n",
       "      <td>0.626859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.445341</td>\n",
       "      <td>0.184434</td>\n",
       "      <td>0.417602</td>\n",
       "      <td>0.375327</td>\n",
       "      <td>0.273169</td>\n",
       "      <td>0.100981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.091621</td>\n",
       "      <td>0.107634</td>\n",
       "      <td>0.106910</td>\n",
       "      <td>0.429057</td>\n",
       "      <td>0.135269</td>\n",
       "      <td>0.332282</td>\n",
       "      <td>0.167539</td>\n",
       "      <td>0.332761</td>\n",
       "      <td>0.364906</td>\n",
       "      <td>0.501705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.520879</td>\n",
       "      <td>0.082492</td>\n",
       "      <td>0.620043</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.484490</td>\n",
       "      <td>0.280594</td>\n",
       "      <td>0.469863</td>\n",
       "      <td>0.376876</td>\n",
       "      <td>0.274590</td>\n",
       "      <td>0.203835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0   0.141733  0.106910  0.153344  0.493644  0.257636  0.364777  0.219981   \n",
       "1   0.028390  0.000000  0.025907  0.412089  0.339159  0.326409  0.222676   \n",
       "2   0.174621  0.119444  0.144015  0.472943  0.271975  0.361000  0.229209   \n",
       "3   0.090002  0.090814  0.120774  0.400915  0.336384  0.306688  0.187426   \n",
       "4   0.076376  0.142307  0.124047  0.425849  0.137060  0.304390  0.171828   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "65  0.094807  0.082492  0.042472  0.443077  0.211947  0.317140  0.168021   \n",
       "66  0.132236  0.106181  0.060387  0.449694  0.173696  0.311363  0.216193   \n",
       "67  0.094017  0.109788  0.089186  0.492175  0.180054  0.393041  0.249496   \n",
       "68  0.052224  0.114694  0.053269  0.441395  0.206836  0.331480  0.254972   \n",
       "69  0.091621  0.107634  0.106910  0.429057  0.135269  0.332282  0.167539   \n",
       "\n",
       "           7         8         9  ...       190       191       192       193  \\\n",
       "0   0.252259  0.440933  0.539385  ...  0.582180  0.122747  0.552270  0.000000   \n",
       "1   0.185724  0.374486  0.467301  ...  0.458928  0.080770  0.555985  0.000000   \n",
       "2   0.221335  0.429724  0.526628  ...  0.536246  0.159083  0.623637  0.000000   \n",
       "3   0.226630  0.313737  0.474120  ...  0.463836  0.081633  0.578060  0.000000   \n",
       "4   0.135269  0.431048  0.441856  ...  0.446534  0.081633  0.564433  0.051172   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "65  0.188690  0.397753  0.453853  ...  0.503894  0.108356  0.410110  0.007266   \n",
       "66  0.189526  0.367328  0.497913  ...  0.474982  0.116748  0.634347  0.018202   \n",
       "67  0.314460  0.467042  0.501088  ...  0.578979  0.117427  0.656068  0.034420   \n",
       "68  0.185724  0.399495  0.506450  ...  0.478019  0.106181  0.626859  0.000000   \n",
       "69  0.332761  0.364906  0.501705  ...  0.520879  0.082492  0.620043  0.000000   \n",
       "\n",
       "         194       195       196       197       198       199  \n",
       "0   0.472881  0.277853  0.450420  0.411714  0.346607  0.226955  \n",
       "1   0.453428  0.246680  0.334347  0.376282  0.275763  0.107634  \n",
       "2   0.497386  0.282840  0.459338  0.405583  0.337160  0.222342  \n",
       "3   0.501140  0.267594  0.528654  0.419395  0.273644  0.161116  \n",
       "4   0.414318  0.232996  0.479574  0.380276  0.285492  0.148478  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "65  0.416697  0.219640  0.508766  0.398576  0.322018  0.154404  \n",
       "66  0.449184  0.206464  0.509352  0.358338  0.268579  0.142307  \n",
       "67  0.463372  0.264348  0.453144  0.342183  0.305735  0.196056  \n",
       "68  0.445341  0.184434  0.417602  0.375327  0.273169  0.100981  \n",
       "69  0.484490  0.280594  0.469863  0.376876  0.274590  0.203835  \n",
       "\n",
       "[70 rows x 200 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 读取制表符分隔的CSV文件\n",
    "# Methylation_val = pd.read_csv('../../DATA/THCA_parting/methylation_val.csv')\n",
    "# # 显示 DataFrame\n",
    "# Methylation_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e4836d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.536138</td>\n",
       "      <td>0.493898</td>\n",
       "      <td>0.313622</td>\n",
       "      <td>0.673057</td>\n",
       "      <td>0.347664</td>\n",
       "      <td>0.882575</td>\n",
       "      <td>0.525694</td>\n",
       "      <td>0.302910</td>\n",
       "      <td>0.463281</td>\n",
       "      <td>0.524695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.571598</td>\n",
       "      <td>0.190252</td>\n",
       "      <td>0.610287</td>\n",
       "      <td>0.434482</td>\n",
       "      <td>0.640451</td>\n",
       "      <td>0.505441</td>\n",
       "      <td>0.577362</td>\n",
       "      <td>0.312103</td>\n",
       "      <td>0.392741</td>\n",
       "      <td>0.308848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.681927</td>\n",
       "      <td>0.643106</td>\n",
       "      <td>0.333751</td>\n",
       "      <td>0.773326</td>\n",
       "      <td>0.416641</td>\n",
       "      <td>0.874332</td>\n",
       "      <td>0.567864</td>\n",
       "      <td>0.436028</td>\n",
       "      <td>0.528897</td>\n",
       "      <td>0.615744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605867</td>\n",
       "      <td>0.159838</td>\n",
       "      <td>0.734076</td>\n",
       "      <td>0.591776</td>\n",
       "      <td>0.711390</td>\n",
       "      <td>0.622655</td>\n",
       "      <td>0.512113</td>\n",
       "      <td>0.265614</td>\n",
       "      <td>0.460598</td>\n",
       "      <td>0.404959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.526849</td>\n",
       "      <td>0.501926</td>\n",
       "      <td>0.268437</td>\n",
       "      <td>0.636161</td>\n",
       "      <td>0.338596</td>\n",
       "      <td>0.826629</td>\n",
       "      <td>0.540336</td>\n",
       "      <td>0.320623</td>\n",
       "      <td>0.375533</td>\n",
       "      <td>0.525848</td>\n",
       "      <td>...</td>\n",
       "      <td>0.587036</td>\n",
       "      <td>0.203297</td>\n",
       "      <td>0.570025</td>\n",
       "      <td>0.499156</td>\n",
       "      <td>0.581459</td>\n",
       "      <td>0.512106</td>\n",
       "      <td>0.524621</td>\n",
       "      <td>0.310605</td>\n",
       "      <td>0.321102</td>\n",
       "      <td>0.327286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.622823</td>\n",
       "      <td>0.551429</td>\n",
       "      <td>0.347272</td>\n",
       "      <td>0.742341</td>\n",
       "      <td>0.392350</td>\n",
       "      <td>0.912176</td>\n",
       "      <td>0.501390</td>\n",
       "      <td>0.408903</td>\n",
       "      <td>0.493463</td>\n",
       "      <td>0.573354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.663959</td>\n",
       "      <td>0.269806</td>\n",
       "      <td>0.674698</td>\n",
       "      <td>0.533805</td>\n",
       "      <td>0.692656</td>\n",
       "      <td>0.477031</td>\n",
       "      <td>0.499395</td>\n",
       "      <td>0.371578</td>\n",
       "      <td>0.453525</td>\n",
       "      <td>0.378615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.579327</td>\n",
       "      <td>0.519563</td>\n",
       "      <td>0.284819</td>\n",
       "      <td>0.687902</td>\n",
       "      <td>0.328320</td>\n",
       "      <td>0.868325</td>\n",
       "      <td>0.607898</td>\n",
       "      <td>0.362169</td>\n",
       "      <td>0.443423</td>\n",
       "      <td>0.509004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.654633</td>\n",
       "      <td>0.223976</td>\n",
       "      <td>0.558896</td>\n",
       "      <td>0.528972</td>\n",
       "      <td>0.682052</td>\n",
       "      <td>0.480867</td>\n",
       "      <td>0.498257</td>\n",
       "      <td>0.361563</td>\n",
       "      <td>0.385183</td>\n",
       "      <td>0.296327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.574634</td>\n",
       "      <td>0.572532</td>\n",
       "      <td>0.303439</td>\n",
       "      <td>0.702631</td>\n",
       "      <td>0.386823</td>\n",
       "      <td>0.849504</td>\n",
       "      <td>0.595399</td>\n",
       "      <td>0.400728</td>\n",
       "      <td>0.454928</td>\n",
       "      <td>0.526443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684901</td>\n",
       "      <td>0.271815</td>\n",
       "      <td>0.602635</td>\n",
       "      <td>0.529764</td>\n",
       "      <td>0.663177</td>\n",
       "      <td>0.605549</td>\n",
       "      <td>0.563902</td>\n",
       "      <td>0.326938</td>\n",
       "      <td>0.415784</td>\n",
       "      <td>0.337581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.578494</td>\n",
       "      <td>0.538260</td>\n",
       "      <td>0.273763</td>\n",
       "      <td>0.716966</td>\n",
       "      <td>0.354288</td>\n",
       "      <td>0.858739</td>\n",
       "      <td>0.565641</td>\n",
       "      <td>0.381254</td>\n",
       "      <td>0.461594</td>\n",
       "      <td>0.534191</td>\n",
       "      <td>...</td>\n",
       "      <td>0.609258</td>\n",
       "      <td>0.221354</td>\n",
       "      <td>0.628558</td>\n",
       "      <td>0.499733</td>\n",
       "      <td>0.649465</td>\n",
       "      <td>0.487716</td>\n",
       "      <td>0.544537</td>\n",
       "      <td>0.328057</td>\n",
       "      <td>0.385530</td>\n",
       "      <td>0.295312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.613273</td>\n",
       "      <td>0.533456</td>\n",
       "      <td>0.339734</td>\n",
       "      <td>0.667563</td>\n",
       "      <td>0.309254</td>\n",
       "      <td>0.833195</td>\n",
       "      <td>0.554292</td>\n",
       "      <td>0.313353</td>\n",
       "      <td>0.416181</td>\n",
       "      <td>0.492470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.554084</td>\n",
       "      <td>0.195447</td>\n",
       "      <td>0.628684</td>\n",
       "      <td>0.513073</td>\n",
       "      <td>0.638695</td>\n",
       "      <td>0.499231</td>\n",
       "      <td>0.515983</td>\n",
       "      <td>0.235825</td>\n",
       "      <td>0.358591</td>\n",
       "      <td>0.298488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0.535862</td>\n",
       "      <td>0.655576</td>\n",
       "      <td>0.248294</td>\n",
       "      <td>0.714125</td>\n",
       "      <td>0.369420</td>\n",
       "      <td>0.905228</td>\n",
       "      <td>0.588485</td>\n",
       "      <td>0.369132</td>\n",
       "      <td>0.425094</td>\n",
       "      <td>0.527233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.660287</td>\n",
       "      <td>0.227530</td>\n",
       "      <td>0.652750</td>\n",
       "      <td>0.515958</td>\n",
       "      <td>0.646835</td>\n",
       "      <td>0.488626</td>\n",
       "      <td>0.510308</td>\n",
       "      <td>0.312804</td>\n",
       "      <td>0.407930</td>\n",
       "      <td>0.341037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0.552051</td>\n",
       "      <td>0.428435</td>\n",
       "      <td>0.265753</td>\n",
       "      <td>0.657925</td>\n",
       "      <td>0.303306</td>\n",
       "      <td>0.908999</td>\n",
       "      <td>0.598477</td>\n",
       "      <td>0.310019</td>\n",
       "      <td>0.471984</td>\n",
       "      <td>0.483266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.602165</td>\n",
       "      <td>0.221014</td>\n",
       "      <td>0.603092</td>\n",
       "      <td>0.457148</td>\n",
       "      <td>0.650390</td>\n",
       "      <td>0.425369</td>\n",
       "      <td>0.495715</td>\n",
       "      <td>0.340218</td>\n",
       "      <td>0.367332</td>\n",
       "      <td>0.300648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.536138  0.493898  0.313622  0.673057  0.347664  0.882575  0.525694   \n",
       "1    0.681927  0.643106  0.333751  0.773326  0.416641  0.874332  0.567864   \n",
       "2    0.526849  0.501926  0.268437  0.636161  0.338596  0.826629  0.540336   \n",
       "3    0.622823  0.551429  0.347272  0.742341  0.392350  0.912176  0.501390   \n",
       "4    0.579327  0.519563  0.284819  0.687902  0.328320  0.868325  0.607898   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "205  0.574634  0.572532  0.303439  0.702631  0.386823  0.849504  0.595399   \n",
       "206  0.578494  0.538260  0.273763  0.716966  0.354288  0.858739  0.565641   \n",
       "207  0.613273  0.533456  0.339734  0.667563  0.309254  0.833195  0.554292   \n",
       "208  0.535862  0.655576  0.248294  0.714125  0.369420  0.905228  0.588485   \n",
       "209  0.552051  0.428435  0.265753  0.657925  0.303306  0.908999  0.598477   \n",
       "\n",
       "            7         8         9  ...       190       191       192  \\\n",
       "0    0.302910  0.463281  0.524695  ...  0.571598  0.190252  0.610287   \n",
       "1    0.436028  0.528897  0.615744  ...  0.605867  0.159838  0.734076   \n",
       "2    0.320623  0.375533  0.525848  ...  0.587036  0.203297  0.570025   \n",
       "3    0.408903  0.493463  0.573354  ...  0.663959  0.269806  0.674698   \n",
       "4    0.362169  0.443423  0.509004  ...  0.654633  0.223976  0.558896   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "205  0.400728  0.454928  0.526443  ...  0.684901  0.271815  0.602635   \n",
       "206  0.381254  0.461594  0.534191  ...  0.609258  0.221354  0.628558   \n",
       "207  0.313353  0.416181  0.492470  ...  0.554084  0.195447  0.628684   \n",
       "208  0.369132  0.425094  0.527233  ...  0.660287  0.227530  0.652750   \n",
       "209  0.310019  0.471984  0.483266  ...  0.602165  0.221014  0.603092   \n",
       "\n",
       "          193       194       195       196       197       198       199  \n",
       "0    0.434482  0.640451  0.505441  0.577362  0.312103  0.392741  0.308848  \n",
       "1    0.591776  0.711390  0.622655  0.512113  0.265614  0.460598  0.404959  \n",
       "2    0.499156  0.581459  0.512106  0.524621  0.310605  0.321102  0.327286  \n",
       "3    0.533805  0.692656  0.477031  0.499395  0.371578  0.453525  0.378615  \n",
       "4    0.528972  0.682052  0.480867  0.498257  0.361563  0.385183  0.296327  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "205  0.529764  0.663177  0.605549  0.563902  0.326938  0.415784  0.337581  \n",
       "206  0.499733  0.649465  0.487716  0.544537  0.328057  0.385530  0.295312  \n",
       "207  0.513073  0.638695  0.499231  0.515983  0.235825  0.358591  0.298488  \n",
       "208  0.515958  0.646835  0.488626  0.510308  0.312804  0.407930  0.341037  \n",
       "209  0.457148  0.650390  0.425369  0.495715  0.340218  0.367332  0.300648  \n",
       "\n",
       "[210 rows x 200 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取制表符分隔的CSV文件\n",
    "miRNASeq_train = pd.read_csv('../../DATA/THCA_parting/3_tr.csv')\n",
    "# 显示 DataFrame\n",
    "miRNASeq_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f744b2a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.636477</td>\n",
       "      <td>0.582610</td>\n",
       "      <td>0.311742</td>\n",
       "      <td>0.686883</td>\n",
       "      <td>0.363409</td>\n",
       "      <td>0.916336</td>\n",
       "      <td>0.598365</td>\n",
       "      <td>0.338654</td>\n",
       "      <td>0.397808</td>\n",
       "      <td>0.542641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.652009</td>\n",
       "      <td>0.149287</td>\n",
       "      <td>0.642376</td>\n",
       "      <td>0.550381</td>\n",
       "      <td>0.706561</td>\n",
       "      <td>0.538832</td>\n",
       "      <td>0.509110</td>\n",
       "      <td>0.312026</td>\n",
       "      <td>0.463783</td>\n",
       "      <td>0.329504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.537868</td>\n",
       "      <td>0.560139</td>\n",
       "      <td>0.293288</td>\n",
       "      <td>0.674952</td>\n",
       "      <td>0.355516</td>\n",
       "      <td>0.865736</td>\n",
       "      <td>0.629423</td>\n",
       "      <td>0.340724</td>\n",
       "      <td>0.436495</td>\n",
       "      <td>0.522952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.588638</td>\n",
       "      <td>0.127681</td>\n",
       "      <td>0.611870</td>\n",
       "      <td>0.508268</td>\n",
       "      <td>0.615549</td>\n",
       "      <td>0.526583</td>\n",
       "      <td>0.558372</td>\n",
       "      <td>0.299117</td>\n",
       "      <td>0.370165</td>\n",
       "      <td>0.340715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.588184</td>\n",
       "      <td>0.563345</td>\n",
       "      <td>0.275554</td>\n",
       "      <td>0.730194</td>\n",
       "      <td>0.371459</td>\n",
       "      <td>0.935580</td>\n",
       "      <td>0.592646</td>\n",
       "      <td>0.362311</td>\n",
       "      <td>0.479582</td>\n",
       "      <td>0.568918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.649749</td>\n",
       "      <td>0.233179</td>\n",
       "      <td>0.636508</td>\n",
       "      <td>0.517827</td>\n",
       "      <td>0.655792</td>\n",
       "      <td>0.561696</td>\n",
       "      <td>0.541594</td>\n",
       "      <td>0.312743</td>\n",
       "      <td>0.431669</td>\n",
       "      <td>0.380083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.647425</td>\n",
       "      <td>0.574234</td>\n",
       "      <td>0.350328</td>\n",
       "      <td>0.741731</td>\n",
       "      <td>0.375318</td>\n",
       "      <td>0.831933</td>\n",
       "      <td>0.586606</td>\n",
       "      <td>0.365600</td>\n",
       "      <td>0.413820</td>\n",
       "      <td>0.543183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.581911</td>\n",
       "      <td>0.161881</td>\n",
       "      <td>0.635631</td>\n",
       "      <td>0.546277</td>\n",
       "      <td>0.692837</td>\n",
       "      <td>0.494645</td>\n",
       "      <td>0.508547</td>\n",
       "      <td>0.279187</td>\n",
       "      <td>0.443970</td>\n",
       "      <td>0.337727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.560996</td>\n",
       "      <td>0.547512</td>\n",
       "      <td>0.278014</td>\n",
       "      <td>0.737094</td>\n",
       "      <td>0.351183</td>\n",
       "      <td>0.917013</td>\n",
       "      <td>0.587173</td>\n",
       "      <td>0.380527</td>\n",
       "      <td>0.476647</td>\n",
       "      <td>0.544561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.554261</td>\n",
       "      <td>0.210073</td>\n",
       "      <td>0.669128</td>\n",
       "      <td>0.539841</td>\n",
       "      <td>0.652562</td>\n",
       "      <td>0.494985</td>\n",
       "      <td>0.505819</td>\n",
       "      <td>0.311685</td>\n",
       "      <td>0.420646</td>\n",
       "      <td>0.344907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.572572</td>\n",
       "      <td>0.535556</td>\n",
       "      <td>0.312320</td>\n",
       "      <td>0.688941</td>\n",
       "      <td>0.332178</td>\n",
       "      <td>0.875345</td>\n",
       "      <td>0.565469</td>\n",
       "      <td>0.389930</td>\n",
       "      <td>0.432463</td>\n",
       "      <td>0.529903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.604858</td>\n",
       "      <td>0.147871</td>\n",
       "      <td>0.609722</td>\n",
       "      <td>0.504016</td>\n",
       "      <td>0.656599</td>\n",
       "      <td>0.434673</td>\n",
       "      <td>0.479017</td>\n",
       "      <td>0.282312</td>\n",
       "      <td>0.401255</td>\n",
       "      <td>0.334039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.570569</td>\n",
       "      <td>0.495804</td>\n",
       "      <td>0.378278</td>\n",
       "      <td>0.740705</td>\n",
       "      <td>0.337366</td>\n",
       "      <td>0.857559</td>\n",
       "      <td>0.628539</td>\n",
       "      <td>0.322244</td>\n",
       "      <td>0.425449</td>\n",
       "      <td>0.485416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.613666</td>\n",
       "      <td>0.252531</td>\n",
       "      <td>0.609251</td>\n",
       "      <td>0.499607</td>\n",
       "      <td>0.639922</td>\n",
       "      <td>0.493636</td>\n",
       "      <td>0.508596</td>\n",
       "      <td>0.326887</td>\n",
       "      <td>0.368404</td>\n",
       "      <td>0.290804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.596292</td>\n",
       "      <td>0.529825</td>\n",
       "      <td>0.333195</td>\n",
       "      <td>0.675292</td>\n",
       "      <td>0.336263</td>\n",
       "      <td>0.893040</td>\n",
       "      <td>0.554459</td>\n",
       "      <td>0.340824</td>\n",
       "      <td>0.503864</td>\n",
       "      <td>0.526884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.597962</td>\n",
       "      <td>0.249464</td>\n",
       "      <td>0.646282</td>\n",
       "      <td>0.452249</td>\n",
       "      <td>0.659332</td>\n",
       "      <td>0.464961</td>\n",
       "      <td>0.562734</td>\n",
       "      <td>0.326897</td>\n",
       "      <td>0.368100</td>\n",
       "      <td>0.310360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.575455</td>\n",
       "      <td>0.464524</td>\n",
       "      <td>0.308397</td>\n",
       "      <td>0.669859</td>\n",
       "      <td>0.340807</td>\n",
       "      <td>0.862595</td>\n",
       "      <td>0.590233</td>\n",
       "      <td>0.303528</td>\n",
       "      <td>0.437918</td>\n",
       "      <td>0.529272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.648281</td>\n",
       "      <td>0.170990</td>\n",
       "      <td>0.551651</td>\n",
       "      <td>0.497183</td>\n",
       "      <td>0.635714</td>\n",
       "      <td>0.465937</td>\n",
       "      <td>0.561572</td>\n",
       "      <td>0.405034</td>\n",
       "      <td>0.320404</td>\n",
       "      <td>0.297863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.568467</td>\n",
       "      <td>0.459542</td>\n",
       "      <td>0.266515</td>\n",
       "      <td>0.672673</td>\n",
       "      <td>0.358430</td>\n",
       "      <td>0.883900</td>\n",
       "      <td>0.620222</td>\n",
       "      <td>0.316499</td>\n",
       "      <td>0.412256</td>\n",
       "      <td>0.518178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.571615</td>\n",
       "      <td>0.173356</td>\n",
       "      <td>0.591903</td>\n",
       "      <td>0.473995</td>\n",
       "      <td>0.625035</td>\n",
       "      <td>0.484707</td>\n",
       "      <td>0.511129</td>\n",
       "      <td>0.351608</td>\n",
       "      <td>0.356850</td>\n",
       "      <td>0.305910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0   0.636477  0.582610  0.311742  0.686883  0.363409  0.916336  0.598365   \n",
       "1   0.537868  0.560139  0.293288  0.674952  0.355516  0.865736  0.629423   \n",
       "2   0.588184  0.563345  0.275554  0.730194  0.371459  0.935580  0.592646   \n",
       "3   0.647425  0.574234  0.350328  0.741731  0.375318  0.831933  0.586606   \n",
       "4   0.560996  0.547512  0.278014  0.737094  0.351183  0.917013  0.587173   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "66  0.572572  0.535556  0.312320  0.688941  0.332178  0.875345  0.565469   \n",
       "67  0.570569  0.495804  0.378278  0.740705  0.337366  0.857559  0.628539   \n",
       "68  0.596292  0.529825  0.333195  0.675292  0.336263  0.893040  0.554459   \n",
       "69  0.575455  0.464524  0.308397  0.669859  0.340807  0.862595  0.590233   \n",
       "70  0.568467  0.459542  0.266515  0.672673  0.358430  0.883900  0.620222   \n",
       "\n",
       "           7         8         9  ...       190       191       192       193  \\\n",
       "0   0.338654  0.397808  0.542641  ...  0.652009  0.149287  0.642376  0.550381   \n",
       "1   0.340724  0.436495  0.522952  ...  0.588638  0.127681  0.611870  0.508268   \n",
       "2   0.362311  0.479582  0.568918  ...  0.649749  0.233179  0.636508  0.517827   \n",
       "3   0.365600  0.413820  0.543183  ...  0.581911  0.161881  0.635631  0.546277   \n",
       "4   0.380527  0.476647  0.544561  ...  0.554261  0.210073  0.669128  0.539841   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "66  0.389930  0.432463  0.529903  ...  0.604858  0.147871  0.609722  0.504016   \n",
       "67  0.322244  0.425449  0.485416  ...  0.613666  0.252531  0.609251  0.499607   \n",
       "68  0.340824  0.503864  0.526884  ...  0.597962  0.249464  0.646282  0.452249   \n",
       "69  0.303528  0.437918  0.529272  ...  0.648281  0.170990  0.551651  0.497183   \n",
       "70  0.316499  0.412256  0.518178  ...  0.571615  0.173356  0.591903  0.473995   \n",
       "\n",
       "         194       195       196       197       198       199  \n",
       "0   0.706561  0.538832  0.509110  0.312026  0.463783  0.329504  \n",
       "1   0.615549  0.526583  0.558372  0.299117  0.370165  0.340715  \n",
       "2   0.655792  0.561696  0.541594  0.312743  0.431669  0.380083  \n",
       "3   0.692837  0.494645  0.508547  0.279187  0.443970  0.337727  \n",
       "4   0.652562  0.494985  0.505819  0.311685  0.420646  0.344907  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "66  0.656599  0.434673  0.479017  0.282312  0.401255  0.334039  \n",
       "67  0.639922  0.493636  0.508596  0.326887  0.368404  0.290804  \n",
       "68  0.659332  0.464961  0.562734  0.326897  0.368100  0.310360  \n",
       "69  0.635714  0.465937  0.561572  0.405034  0.320404  0.297863  \n",
       "70  0.625035  0.484707  0.511129  0.351608  0.356850  0.305910  \n",
       "\n",
       "[71 rows x 200 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取制表符分隔的CSV文件\n",
    "miRNASeq_test = pd.read_csv('../../DATA/THCA_parting/3_te.csv')\n",
    "# 显示 DataFrame\n",
    "miRNASeq_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1220144",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.552139</td>\n",
       "      <td>0.559067</td>\n",
       "      <td>0.285569</td>\n",
       "      <td>0.718629</td>\n",
       "      <td>0.390478</td>\n",
       "      <td>0.833890</td>\n",
       "      <td>0.539042</td>\n",
       "      <td>0.403945</td>\n",
       "      <td>0.461322</td>\n",
       "      <td>0.546504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.613052</td>\n",
       "      <td>0.203944</td>\n",
       "      <td>0.637093</td>\n",
       "      <td>0.555595</td>\n",
       "      <td>0.659333</td>\n",
       "      <td>0.533907</td>\n",
       "      <td>0.526485</td>\n",
       "      <td>0.326518</td>\n",
       "      <td>0.441615</td>\n",
       "      <td>0.345450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.580002</td>\n",
       "      <td>0.529317</td>\n",
       "      <td>0.329242</td>\n",
       "      <td>0.738768</td>\n",
       "      <td>0.388183</td>\n",
       "      <td>0.904719</td>\n",
       "      <td>0.580431</td>\n",
       "      <td>0.392392</td>\n",
       "      <td>0.485232</td>\n",
       "      <td>0.548183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.686109</td>\n",
       "      <td>0.231781</td>\n",
       "      <td>0.617823</td>\n",
       "      <td>0.544666</td>\n",
       "      <td>0.661086</td>\n",
       "      <td>0.507549</td>\n",
       "      <td>0.544031</td>\n",
       "      <td>0.303962</td>\n",
       "      <td>0.447308</td>\n",
       "      <td>0.336100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.561408</td>\n",
       "      <td>0.513636</td>\n",
       "      <td>0.315951</td>\n",
       "      <td>0.673154</td>\n",
       "      <td>0.318711</td>\n",
       "      <td>0.922501</td>\n",
       "      <td>0.553679</td>\n",
       "      <td>0.307780</td>\n",
       "      <td>0.455743</td>\n",
       "      <td>0.537512</td>\n",
       "      <td>...</td>\n",
       "      <td>0.580173</td>\n",
       "      <td>0.162286</td>\n",
       "      <td>0.585571</td>\n",
       "      <td>0.492083</td>\n",
       "      <td>0.634090</td>\n",
       "      <td>0.512798</td>\n",
       "      <td>0.561921</td>\n",
       "      <td>0.270619</td>\n",
       "      <td>0.345588</td>\n",
       "      <td>0.301247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.549919</td>\n",
       "      <td>0.462672</td>\n",
       "      <td>0.340425</td>\n",
       "      <td>0.611309</td>\n",
       "      <td>0.316472</td>\n",
       "      <td>0.861999</td>\n",
       "      <td>0.563730</td>\n",
       "      <td>0.292498</td>\n",
       "      <td>0.400651</td>\n",
       "      <td>0.465073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.590594</td>\n",
       "      <td>0.206259</td>\n",
       "      <td>0.554519</td>\n",
       "      <td>0.459943</td>\n",
       "      <td>0.650416</td>\n",
       "      <td>0.446624</td>\n",
       "      <td>0.499137</td>\n",
       "      <td>0.285911</td>\n",
       "      <td>0.324034</td>\n",
       "      <td>0.295996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.567981</td>\n",
       "      <td>0.555092</td>\n",
       "      <td>0.297521</td>\n",
       "      <td>0.694480</td>\n",
       "      <td>0.385651</td>\n",
       "      <td>0.943919</td>\n",
       "      <td>0.565570</td>\n",
       "      <td>0.351122</td>\n",
       "      <td>0.397546</td>\n",
       "      <td>0.525114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.621068</td>\n",
       "      <td>0.232070</td>\n",
       "      <td>0.579691</td>\n",
       "      <td>0.507443</td>\n",
       "      <td>0.663793</td>\n",
       "      <td>0.517530</td>\n",
       "      <td>0.549106</td>\n",
       "      <td>0.347407</td>\n",
       "      <td>0.421452</td>\n",
       "      <td>0.346779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.580880</td>\n",
       "      <td>0.497651</td>\n",
       "      <td>0.258057</td>\n",
       "      <td>0.701615</td>\n",
       "      <td>0.336419</td>\n",
       "      <td>0.864107</td>\n",
       "      <td>0.596609</td>\n",
       "      <td>0.327971</td>\n",
       "      <td>0.499936</td>\n",
       "      <td>0.541747</td>\n",
       "      <td>...</td>\n",
       "      <td>0.639236</td>\n",
       "      <td>0.218292</td>\n",
       "      <td>0.619704</td>\n",
       "      <td>0.499202</td>\n",
       "      <td>0.675908</td>\n",
       "      <td>0.486819</td>\n",
       "      <td>0.553745</td>\n",
       "      <td>0.334531</td>\n",
       "      <td>0.430577</td>\n",
       "      <td>0.311705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.604719</td>\n",
       "      <td>0.491897</td>\n",
       "      <td>0.274434</td>\n",
       "      <td>0.656380</td>\n",
       "      <td>0.357742</td>\n",
       "      <td>0.839752</td>\n",
       "      <td>0.502004</td>\n",
       "      <td>0.357297</td>\n",
       "      <td>0.431040</td>\n",
       "      <td>0.520637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.620938</td>\n",
       "      <td>0.178357</td>\n",
       "      <td>0.582983</td>\n",
       "      <td>0.498561</td>\n",
       "      <td>0.620178</td>\n",
       "      <td>0.492380</td>\n",
       "      <td>0.497586</td>\n",
       "      <td>0.306908</td>\n",
       "      <td>0.383063</td>\n",
       "      <td>0.311347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.600014</td>\n",
       "      <td>0.562987</td>\n",
       "      <td>0.249852</td>\n",
       "      <td>0.701769</td>\n",
       "      <td>0.402879</td>\n",
       "      <td>0.927926</td>\n",
       "      <td>0.572836</td>\n",
       "      <td>0.419111</td>\n",
       "      <td>0.478940</td>\n",
       "      <td>0.564210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.632291</td>\n",
       "      <td>0.264023</td>\n",
       "      <td>0.658148</td>\n",
       "      <td>0.526569</td>\n",
       "      <td>0.683633</td>\n",
       "      <td>0.515043</td>\n",
       "      <td>0.483666</td>\n",
       "      <td>0.328925</td>\n",
       "      <td>0.417604</td>\n",
       "      <td>0.359054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.580941</td>\n",
       "      <td>0.550428</td>\n",
       "      <td>0.367881</td>\n",
       "      <td>0.691000</td>\n",
       "      <td>0.394067</td>\n",
       "      <td>0.853389</td>\n",
       "      <td>0.531384</td>\n",
       "      <td>0.400435</td>\n",
       "      <td>0.448872</td>\n",
       "      <td>0.559928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.623368</td>\n",
       "      <td>0.207858</td>\n",
       "      <td>0.662403</td>\n",
       "      <td>0.559818</td>\n",
       "      <td>0.682088</td>\n",
       "      <td>0.512996</td>\n",
       "      <td>0.504750</td>\n",
       "      <td>0.343361</td>\n",
       "      <td>0.445321</td>\n",
       "      <td>0.356560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.535983</td>\n",
       "      <td>0.458513</td>\n",
       "      <td>0.338341</td>\n",
       "      <td>0.631616</td>\n",
       "      <td>0.308944</td>\n",
       "      <td>0.900401</td>\n",
       "      <td>0.633418</td>\n",
       "      <td>0.291481</td>\n",
       "      <td>0.409003</td>\n",
       "      <td>0.519511</td>\n",
       "      <td>...</td>\n",
       "      <td>0.602546</td>\n",
       "      <td>0.242036</td>\n",
       "      <td>0.570299</td>\n",
       "      <td>0.450732</td>\n",
       "      <td>0.624244</td>\n",
       "      <td>0.436177</td>\n",
       "      <td>0.556290</td>\n",
       "      <td>0.307232</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>0.268904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0   0.552139  0.559067  0.285569  0.718629  0.390478  0.833890  0.539042   \n",
       "1   0.580002  0.529317  0.329242  0.738768  0.388183  0.904719  0.580431   \n",
       "2   0.561408  0.513636  0.315951  0.673154  0.318711  0.922501  0.553679   \n",
       "3   0.549919  0.462672  0.340425  0.611309  0.316472  0.861999  0.563730   \n",
       "4   0.567981  0.555092  0.297521  0.694480  0.385651  0.943919  0.565570   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "65  0.580880  0.497651  0.258057  0.701615  0.336419  0.864107  0.596609   \n",
       "66  0.604719  0.491897  0.274434  0.656380  0.357742  0.839752  0.502004   \n",
       "67  0.600014  0.562987  0.249852  0.701769  0.402879  0.927926  0.572836   \n",
       "68  0.580941  0.550428  0.367881  0.691000  0.394067  0.853389  0.531384   \n",
       "69  0.535983  0.458513  0.338341  0.631616  0.308944  0.900401  0.633418   \n",
       "\n",
       "           7         8         9  ...       190       191       192       193  \\\n",
       "0   0.403945  0.461322  0.546504  ...  0.613052  0.203944  0.637093  0.555595   \n",
       "1   0.392392  0.485232  0.548183  ...  0.686109  0.231781  0.617823  0.544666   \n",
       "2   0.307780  0.455743  0.537512  ...  0.580173  0.162286  0.585571  0.492083   \n",
       "3   0.292498  0.400651  0.465073  ...  0.590594  0.206259  0.554519  0.459943   \n",
       "4   0.351122  0.397546  0.525114  ...  0.621068  0.232070  0.579691  0.507443   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "65  0.327971  0.499936  0.541747  ...  0.639236  0.218292  0.619704  0.499202   \n",
       "66  0.357297  0.431040  0.520637  ...  0.620938  0.178357  0.582983  0.498561   \n",
       "67  0.419111  0.478940  0.564210  ...  0.632291  0.264023  0.658148  0.526569   \n",
       "68  0.400435  0.448872  0.559928  ...  0.623368  0.207858  0.662403  0.559818   \n",
       "69  0.291481  0.409003  0.519511  ...  0.602546  0.242036  0.570299  0.450732   \n",
       "\n",
       "         194       195       196       197       198       199  \n",
       "0   0.659333  0.533907  0.526485  0.326518  0.441615  0.345450  \n",
       "1   0.661086  0.507549  0.544031  0.303962  0.447308  0.336100  \n",
       "2   0.634090  0.512798  0.561921  0.270619  0.345588  0.301247  \n",
       "3   0.650416  0.446624  0.499137  0.285911  0.324034  0.295996  \n",
       "4   0.663793  0.517530  0.549106  0.347407  0.421452  0.346779  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "65  0.675908  0.486819  0.553745  0.334531  0.430577  0.311705  \n",
       "66  0.620178  0.492380  0.497586  0.306908  0.383063  0.311347  \n",
       "67  0.683633  0.515043  0.483666  0.328925  0.417604  0.359054  \n",
       "68  0.682088  0.512996  0.504750  0.343361  0.445321  0.356560  \n",
       "69  0.624244  0.436177  0.556290  0.307232  0.303200  0.268904  \n",
       "\n",
       "[70 rows x 200 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 读取制表符分隔的CSV文件\n",
    "# miRNASeq_val = pd.read_csv('../../DATA/THCA_parting/miRNASeq_val.csv')\n",
    "# # 显示 DataFrame\n",
    "# miRNASeq_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d88d1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.798907</td>\n",
       "      <td>0.735724</td>\n",
       "      <td>0.751890</td>\n",
       "      <td>0.831284</td>\n",
       "      <td>0.724953</td>\n",
       "      <td>0.473487</td>\n",
       "      <td>0.944846</td>\n",
       "      <td>0.379073</td>\n",
       "      <td>0.203647</td>\n",
       "      <td>0.685920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.385380</td>\n",
       "      <td>0.292930</td>\n",
       "      <td>0.234979</td>\n",
       "      <td>0.260191</td>\n",
       "      <td>0.253372</td>\n",
       "      <td>0.234694</td>\n",
       "      <td>0.155948</td>\n",
       "      <td>0.299638</td>\n",
       "      <td>0.314865</td>\n",
       "      <td>0.263381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.807236</td>\n",
       "      <td>0.712284</td>\n",
       "      <td>0.730247</td>\n",
       "      <td>0.819646</td>\n",
       "      <td>0.734358</td>\n",
       "      <td>0.477053</td>\n",
       "      <td>0.941102</td>\n",
       "      <td>0.395230</td>\n",
       "      <td>0.284018</td>\n",
       "      <td>0.682869</td>\n",
       "      <td>...</td>\n",
       "      <td>0.365879</td>\n",
       "      <td>0.275455</td>\n",
       "      <td>0.361636</td>\n",
       "      <td>0.284818</td>\n",
       "      <td>0.388235</td>\n",
       "      <td>0.239268</td>\n",
       "      <td>0.352297</td>\n",
       "      <td>0.400933</td>\n",
       "      <td>0.435073</td>\n",
       "      <td>0.266057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.816905</td>\n",
       "      <td>0.724565</td>\n",
       "      <td>0.719784</td>\n",
       "      <td>0.837038</td>\n",
       "      <td>0.748833</td>\n",
       "      <td>0.480831</td>\n",
       "      <td>0.954793</td>\n",
       "      <td>0.365776</td>\n",
       "      <td>0.286031</td>\n",
       "      <td>0.700446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.307314</td>\n",
       "      <td>0.284587</td>\n",
       "      <td>0.250329</td>\n",
       "      <td>0.182719</td>\n",
       "      <td>0.283641</td>\n",
       "      <td>0.233600</td>\n",
       "      <td>0.281007</td>\n",
       "      <td>0.331042</td>\n",
       "      <td>0.377167</td>\n",
       "      <td>0.317765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.793171</td>\n",
       "      <td>0.737116</td>\n",
       "      <td>0.759549</td>\n",
       "      <td>0.841422</td>\n",
       "      <td>0.733765</td>\n",
       "      <td>0.517666</td>\n",
       "      <td>0.919615</td>\n",
       "      <td>0.391978</td>\n",
       "      <td>0.255071</td>\n",
       "      <td>0.666125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.403233</td>\n",
       "      <td>0.223435</td>\n",
       "      <td>0.262910</td>\n",
       "      <td>0.216989</td>\n",
       "      <td>0.191927</td>\n",
       "      <td>0.338639</td>\n",
       "      <td>0.178280</td>\n",
       "      <td>0.252927</td>\n",
       "      <td>0.341936</td>\n",
       "      <td>0.347774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.805424</td>\n",
       "      <td>0.722645</td>\n",
       "      <td>0.767946</td>\n",
       "      <td>0.800873</td>\n",
       "      <td>0.729937</td>\n",
       "      <td>0.502907</td>\n",
       "      <td>0.942959</td>\n",
       "      <td>0.388526</td>\n",
       "      <td>0.260258</td>\n",
       "      <td>0.668824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.351525</td>\n",
       "      <td>0.285677</td>\n",
       "      <td>0.318781</td>\n",
       "      <td>0.271669</td>\n",
       "      <td>0.171380</td>\n",
       "      <td>0.244158</td>\n",
       "      <td>0.271588</td>\n",
       "      <td>0.271920</td>\n",
       "      <td>0.356414</td>\n",
       "      <td>0.345516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.797760</td>\n",
       "      <td>0.737862</td>\n",
       "      <td>0.763707</td>\n",
       "      <td>0.834947</td>\n",
       "      <td>0.716293</td>\n",
       "      <td>0.479883</td>\n",
       "      <td>0.934261</td>\n",
       "      <td>0.425524</td>\n",
       "      <td>0.228912</td>\n",
       "      <td>0.674421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.217326</td>\n",
       "      <td>0.242386</td>\n",
       "      <td>0.208652</td>\n",
       "      <td>0.291812</td>\n",
       "      <td>0.250175</td>\n",
       "      <td>0.261176</td>\n",
       "      <td>0.212059</td>\n",
       "      <td>0.305331</td>\n",
       "      <td>0.330686</td>\n",
       "      <td>0.343881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.807673</td>\n",
       "      <td>0.737630</td>\n",
       "      <td>0.761739</td>\n",
       "      <td>0.830347</td>\n",
       "      <td>0.740090</td>\n",
       "      <td>0.538156</td>\n",
       "      <td>0.941689</td>\n",
       "      <td>0.347578</td>\n",
       "      <td>0.239740</td>\n",
       "      <td>0.675897</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299436</td>\n",
       "      <td>0.245941</td>\n",
       "      <td>0.235716</td>\n",
       "      <td>0.259349</td>\n",
       "      <td>0.228823</td>\n",
       "      <td>0.277467</td>\n",
       "      <td>0.282033</td>\n",
       "      <td>0.316850</td>\n",
       "      <td>0.378633</td>\n",
       "      <td>0.268288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.820332</td>\n",
       "      <td>0.733834</td>\n",
       "      <td>0.758290</td>\n",
       "      <td>0.813778</td>\n",
       "      <td>0.721499</td>\n",
       "      <td>0.445163</td>\n",
       "      <td>0.926757</td>\n",
       "      <td>0.344475</td>\n",
       "      <td>0.138328</td>\n",
       "      <td>0.667812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.401012</td>\n",
       "      <td>0.247518</td>\n",
       "      <td>0.239063</td>\n",
       "      <td>0.216090</td>\n",
       "      <td>0.224356</td>\n",
       "      <td>0.351130</td>\n",
       "      <td>0.230011</td>\n",
       "      <td>0.351508</td>\n",
       "      <td>0.349275</td>\n",
       "      <td>0.376025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0.840917</td>\n",
       "      <td>0.725794</td>\n",
       "      <td>0.765166</td>\n",
       "      <td>0.818968</td>\n",
       "      <td>0.732587</td>\n",
       "      <td>0.496392</td>\n",
       "      <td>0.951199</td>\n",
       "      <td>0.361329</td>\n",
       "      <td>0.236333</td>\n",
       "      <td>0.681058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.279500</td>\n",
       "      <td>0.294944</td>\n",
       "      <td>0.197405</td>\n",
       "      <td>0.290212</td>\n",
       "      <td>0.254256</td>\n",
       "      <td>0.213738</td>\n",
       "      <td>0.268842</td>\n",
       "      <td>0.312756</td>\n",
       "      <td>0.389861</td>\n",
       "      <td>0.287308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>0.793970</td>\n",
       "      <td>0.717991</td>\n",
       "      <td>0.732055</td>\n",
       "      <td>0.819160</td>\n",
       "      <td>0.718018</td>\n",
       "      <td>0.509887</td>\n",
       "      <td>0.945365</td>\n",
       "      <td>0.378045</td>\n",
       "      <td>0.262301</td>\n",
       "      <td>0.675706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.349287</td>\n",
       "      <td>0.307036</td>\n",
       "      <td>0.318678</td>\n",
       "      <td>0.289931</td>\n",
       "      <td>0.243831</td>\n",
       "      <td>0.255417</td>\n",
       "      <td>0.278546</td>\n",
       "      <td>0.356291</td>\n",
       "      <td>0.365217</td>\n",
       "      <td>0.290131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.798907  0.735724  0.751890  0.831284  0.724953  0.473487  0.944846   \n",
       "1    0.807236  0.712284  0.730247  0.819646  0.734358  0.477053  0.941102   \n",
       "2    0.816905  0.724565  0.719784  0.837038  0.748833  0.480831  0.954793   \n",
       "3    0.793171  0.737116  0.759549  0.841422  0.733765  0.517666  0.919615   \n",
       "4    0.805424  0.722645  0.767946  0.800873  0.729937  0.502907  0.942959   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "205  0.797760  0.737862  0.763707  0.834947  0.716293  0.479883  0.934261   \n",
       "206  0.807673  0.737630  0.761739  0.830347  0.740090  0.538156  0.941689   \n",
       "207  0.820332  0.733834  0.758290  0.813778  0.721499  0.445163  0.926757   \n",
       "208  0.840917  0.725794  0.765166  0.818968  0.732587  0.496392  0.951199   \n",
       "209  0.793970  0.717991  0.732055  0.819160  0.718018  0.509887  0.945365   \n",
       "\n",
       "            7         8         9  ...       190       191       192  \\\n",
       "0    0.379073  0.203647  0.685920  ...  0.385380  0.292930  0.234979   \n",
       "1    0.395230  0.284018  0.682869  ...  0.365879  0.275455  0.361636   \n",
       "2    0.365776  0.286031  0.700446  ...  0.307314  0.284587  0.250329   \n",
       "3    0.391978  0.255071  0.666125  ...  0.403233  0.223435  0.262910   \n",
       "4    0.388526  0.260258  0.668824  ...  0.351525  0.285677  0.318781   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "205  0.425524  0.228912  0.674421  ...  0.217326  0.242386  0.208652   \n",
       "206  0.347578  0.239740  0.675897  ...  0.299436  0.245941  0.235716   \n",
       "207  0.344475  0.138328  0.667812  ...  0.401012  0.247518  0.239063   \n",
       "208  0.361329  0.236333  0.681058  ...  0.279500  0.294944  0.197405   \n",
       "209  0.378045  0.262301  0.675706  ...  0.349287  0.307036  0.318678   \n",
       "\n",
       "          193       194       195       196       197       198       199  \n",
       "0    0.260191  0.253372  0.234694  0.155948  0.299638  0.314865  0.263381  \n",
       "1    0.284818  0.388235  0.239268  0.352297  0.400933  0.435073  0.266057  \n",
       "2    0.182719  0.283641  0.233600  0.281007  0.331042  0.377167  0.317765  \n",
       "3    0.216989  0.191927  0.338639  0.178280  0.252927  0.341936  0.347774  \n",
       "4    0.271669  0.171380  0.244158  0.271588  0.271920  0.356414  0.345516  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "205  0.291812  0.250175  0.261176  0.212059  0.305331  0.330686  0.343881  \n",
       "206  0.259349  0.228823  0.277467  0.282033  0.316850  0.378633  0.268288  \n",
       "207  0.216090  0.224356  0.351130  0.230011  0.351508  0.349275  0.376025  \n",
       "208  0.290212  0.254256  0.213738  0.268842  0.312756  0.389861  0.287308  \n",
       "209  0.289931  0.243831  0.255417  0.278546  0.356291  0.365217  0.290131  \n",
       "\n",
       "[210 rows x 200 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取制表符分隔的CSV文件\n",
    "RNAseq_train = pd.read_csv('../../DATA/THCA_parting/2_tr.csv')\n",
    "# 显示 DataFrame\n",
    "RNAseq_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff830335",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.793003</td>\n",
       "      <td>0.709606</td>\n",
       "      <td>0.721879</td>\n",
       "      <td>0.834684</td>\n",
       "      <td>0.753937</td>\n",
       "      <td>0.467375</td>\n",
       "      <td>0.960719</td>\n",
       "      <td>0.352485</td>\n",
       "      <td>0.219769</td>\n",
       "      <td>0.693918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.371620</td>\n",
       "      <td>0.211666</td>\n",
       "      <td>0.277812</td>\n",
       "      <td>0.214386</td>\n",
       "      <td>0.233559</td>\n",
       "      <td>0.354043</td>\n",
       "      <td>0.266821</td>\n",
       "      <td>0.311767</td>\n",
       "      <td>0.400873</td>\n",
       "      <td>0.309220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.811867</td>\n",
       "      <td>0.742295</td>\n",
       "      <td>0.762346</td>\n",
       "      <td>0.846719</td>\n",
       "      <td>0.736210</td>\n",
       "      <td>0.549367</td>\n",
       "      <td>0.940407</td>\n",
       "      <td>0.360581</td>\n",
       "      <td>0.267450</td>\n",
       "      <td>0.695093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.349109</td>\n",
       "      <td>0.240599</td>\n",
       "      <td>0.269812</td>\n",
       "      <td>0.269671</td>\n",
       "      <td>0.237568</td>\n",
       "      <td>0.217153</td>\n",
       "      <td>0.239773</td>\n",
       "      <td>0.314512</td>\n",
       "      <td>0.328268</td>\n",
       "      <td>0.335625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.806733</td>\n",
       "      <td>0.710420</td>\n",
       "      <td>0.741262</td>\n",
       "      <td>0.843361</td>\n",
       "      <td>0.732442</td>\n",
       "      <td>0.531331</td>\n",
       "      <td>0.942780</td>\n",
       "      <td>0.409817</td>\n",
       "      <td>0.288463</td>\n",
       "      <td>0.683201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198458</td>\n",
       "      <td>0.228993</td>\n",
       "      <td>0.220373</td>\n",
       "      <td>0.265692</td>\n",
       "      <td>0.266537</td>\n",
       "      <td>0.231439</td>\n",
       "      <td>0.163615</td>\n",
       "      <td>0.293377</td>\n",
       "      <td>0.305227</td>\n",
       "      <td>0.351944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.814385</td>\n",
       "      <td>0.718509</td>\n",
       "      <td>0.754616</td>\n",
       "      <td>0.839521</td>\n",
       "      <td>0.738961</td>\n",
       "      <td>0.526235</td>\n",
       "      <td>0.943525</td>\n",
       "      <td>0.392392</td>\n",
       "      <td>0.319183</td>\n",
       "      <td>0.697681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.257104</td>\n",
       "      <td>0.260821</td>\n",
       "      <td>0.288695</td>\n",
       "      <td>0.263305</td>\n",
       "      <td>0.332982</td>\n",
       "      <td>0.229277</td>\n",
       "      <td>0.245703</td>\n",
       "      <td>0.327600</td>\n",
       "      <td>0.355241</td>\n",
       "      <td>0.322626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.815538</td>\n",
       "      <td>0.727702</td>\n",
       "      <td>0.745215</td>\n",
       "      <td>0.829479</td>\n",
       "      <td>0.738741</td>\n",
       "      <td>0.504837</td>\n",
       "      <td>0.947324</td>\n",
       "      <td>0.366882</td>\n",
       "      <td>0.167566</td>\n",
       "      <td>0.688812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.371046</td>\n",
       "      <td>0.264504</td>\n",
       "      <td>0.263607</td>\n",
       "      <td>0.316706</td>\n",
       "      <td>0.245949</td>\n",
       "      <td>0.246698</td>\n",
       "      <td>0.286809</td>\n",
       "      <td>0.314163</td>\n",
       "      <td>0.347649</td>\n",
       "      <td>0.276571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.804041</td>\n",
       "      <td>0.699481</td>\n",
       "      <td>0.708341</td>\n",
       "      <td>0.833917</td>\n",
       "      <td>0.734879</td>\n",
       "      <td>0.539785</td>\n",
       "      <td>0.940506</td>\n",
       "      <td>0.407475</td>\n",
       "      <td>0.317361</td>\n",
       "      <td>0.702691</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124130</td>\n",
       "      <td>0.362786</td>\n",
       "      <td>0.315348</td>\n",
       "      <td>0.303023</td>\n",
       "      <td>0.231527</td>\n",
       "      <td>0.192564</td>\n",
       "      <td>0.323248</td>\n",
       "      <td>0.410385</td>\n",
       "      <td>0.418990</td>\n",
       "      <td>0.263463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.828682</td>\n",
       "      <td>0.723210</td>\n",
       "      <td>0.750118</td>\n",
       "      <td>0.848689</td>\n",
       "      <td>0.758602</td>\n",
       "      <td>0.500481</td>\n",
       "      <td>0.949241</td>\n",
       "      <td>0.356482</td>\n",
       "      <td>0.234229</td>\n",
       "      <td>0.687243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.343994</td>\n",
       "      <td>0.243092</td>\n",
       "      <td>0.292688</td>\n",
       "      <td>0.249129</td>\n",
       "      <td>0.280585</td>\n",
       "      <td>0.284343</td>\n",
       "      <td>0.279401</td>\n",
       "      <td>0.343331</td>\n",
       "      <td>0.337790</td>\n",
       "      <td>0.233913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.820888</td>\n",
       "      <td>0.726444</td>\n",
       "      <td>0.744190</td>\n",
       "      <td>0.827705</td>\n",
       "      <td>0.734209</td>\n",
       "      <td>0.474903</td>\n",
       "      <td>0.942679</td>\n",
       "      <td>0.347207</td>\n",
       "      <td>0.246817</td>\n",
       "      <td>0.683196</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323588</td>\n",
       "      <td>0.226388</td>\n",
       "      <td>0.287337</td>\n",
       "      <td>0.258655</td>\n",
       "      <td>0.232294</td>\n",
       "      <td>0.283436</td>\n",
       "      <td>0.257268</td>\n",
       "      <td>0.321768</td>\n",
       "      <td>0.374917</td>\n",
       "      <td>0.289345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.808246</td>\n",
       "      <td>0.759955</td>\n",
       "      <td>0.747215</td>\n",
       "      <td>0.824923</td>\n",
       "      <td>0.719057</td>\n",
       "      <td>0.441630</td>\n",
       "      <td>0.924886</td>\n",
       "      <td>0.404335</td>\n",
       "      <td>0.293269</td>\n",
       "      <td>0.665009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.368334</td>\n",
       "      <td>0.224143</td>\n",
       "      <td>0.320211</td>\n",
       "      <td>0.258251</td>\n",
       "      <td>0.193026</td>\n",
       "      <td>0.262592</td>\n",
       "      <td>0.221907</td>\n",
       "      <td>0.313015</td>\n",
       "      <td>0.320002</td>\n",
       "      <td>0.387227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.802172</td>\n",
       "      <td>0.734491</td>\n",
       "      <td>0.730934</td>\n",
       "      <td>0.828447</td>\n",
       "      <td>0.729173</td>\n",
       "      <td>0.522198</td>\n",
       "      <td>0.928823</td>\n",
       "      <td>0.367595</td>\n",
       "      <td>0.277991</td>\n",
       "      <td>0.666874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288559</td>\n",
       "      <td>0.263654</td>\n",
       "      <td>0.228567</td>\n",
       "      <td>0.222840</td>\n",
       "      <td>0.156867</td>\n",
       "      <td>0.211428</td>\n",
       "      <td>0.245665</td>\n",
       "      <td>0.359005</td>\n",
       "      <td>0.375175</td>\n",
       "      <td>0.314360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0   0.793003  0.709606  0.721879  0.834684  0.753937  0.467375  0.960719   \n",
       "1   0.811867  0.742295  0.762346  0.846719  0.736210  0.549367  0.940407   \n",
       "2   0.806733  0.710420  0.741262  0.843361  0.732442  0.531331  0.942780   \n",
       "3   0.814385  0.718509  0.754616  0.839521  0.738961  0.526235  0.943525   \n",
       "4   0.815538  0.727702  0.745215  0.829479  0.738741  0.504837  0.947324   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "66  0.804041  0.699481  0.708341  0.833917  0.734879  0.539785  0.940506   \n",
       "67  0.828682  0.723210  0.750118  0.848689  0.758602  0.500481  0.949241   \n",
       "68  0.820888  0.726444  0.744190  0.827705  0.734209  0.474903  0.942679   \n",
       "69  0.808246  0.759955  0.747215  0.824923  0.719057  0.441630  0.924886   \n",
       "70  0.802172  0.734491  0.730934  0.828447  0.729173  0.522198  0.928823   \n",
       "\n",
       "           7         8         9  ...       190       191       192       193  \\\n",
       "0   0.352485  0.219769  0.693918  ...  0.371620  0.211666  0.277812  0.214386   \n",
       "1   0.360581  0.267450  0.695093  ...  0.349109  0.240599  0.269812  0.269671   \n",
       "2   0.409817  0.288463  0.683201  ...  0.198458  0.228993  0.220373  0.265692   \n",
       "3   0.392392  0.319183  0.697681  ...  0.257104  0.260821  0.288695  0.263305   \n",
       "4   0.366882  0.167566  0.688812  ...  0.371046  0.264504  0.263607  0.316706   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "66  0.407475  0.317361  0.702691  ...  0.124130  0.362786  0.315348  0.303023   \n",
       "67  0.356482  0.234229  0.687243  ...  0.343994  0.243092  0.292688  0.249129   \n",
       "68  0.347207  0.246817  0.683196  ...  0.323588  0.226388  0.287337  0.258655   \n",
       "69  0.404335  0.293269  0.665009  ...  0.368334  0.224143  0.320211  0.258251   \n",
       "70  0.367595  0.277991  0.666874  ...  0.288559  0.263654  0.228567  0.222840   \n",
       "\n",
       "         194       195       196       197       198       199  \n",
       "0   0.233559  0.354043  0.266821  0.311767  0.400873  0.309220  \n",
       "1   0.237568  0.217153  0.239773  0.314512  0.328268  0.335625  \n",
       "2   0.266537  0.231439  0.163615  0.293377  0.305227  0.351944  \n",
       "3   0.332982  0.229277  0.245703  0.327600  0.355241  0.322626  \n",
       "4   0.245949  0.246698  0.286809  0.314163  0.347649  0.276571  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "66  0.231527  0.192564  0.323248  0.410385  0.418990  0.263463  \n",
       "67  0.280585  0.284343  0.279401  0.343331  0.337790  0.233913  \n",
       "68  0.232294  0.283436  0.257268  0.321768  0.374917  0.289345  \n",
       "69  0.193026  0.262592  0.221907  0.313015  0.320002  0.387227  \n",
       "70  0.156867  0.211428  0.245665  0.359005  0.375175  0.314360  \n",
       "\n",
       "[71 rows x 200 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取制表符分隔的CSV文件\n",
    "RNAseq_test = pd.read_csv('../../DATA/THCA_parting/2_te.csv')\n",
    "# 显示 DataFrame\n",
    "RNAseq_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcf918a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.822696</td>\n",
       "      <td>0.749512</td>\n",
       "      <td>0.738373</td>\n",
       "      <td>0.829757</td>\n",
       "      <td>0.725952</td>\n",
       "      <td>0.510500</td>\n",
       "      <td>0.928945</td>\n",
       "      <td>0.401692</td>\n",
       "      <td>0.279266</td>\n",
       "      <td>0.673976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.337899</td>\n",
       "      <td>0.256514</td>\n",
       "      <td>0.289538</td>\n",
       "      <td>0.182423</td>\n",
       "      <td>0.265771</td>\n",
       "      <td>0.254107</td>\n",
       "      <td>0.222156</td>\n",
       "      <td>0.348486</td>\n",
       "      <td>0.386689</td>\n",
       "      <td>0.317780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.807597</td>\n",
       "      <td>0.717130</td>\n",
       "      <td>0.739479</td>\n",
       "      <td>0.836040</td>\n",
       "      <td>0.743131</td>\n",
       "      <td>0.509932</td>\n",
       "      <td>0.938481</td>\n",
       "      <td>0.344084</td>\n",
       "      <td>0.228623</td>\n",
       "      <td>0.689352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.273958</td>\n",
       "      <td>0.292204</td>\n",
       "      <td>0.217053</td>\n",
       "      <td>0.243807</td>\n",
       "      <td>0.287193</td>\n",
       "      <td>0.272302</td>\n",
       "      <td>0.274035</td>\n",
       "      <td>0.366102</td>\n",
       "      <td>0.316983</td>\n",
       "      <td>0.308586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.808222</td>\n",
       "      <td>0.711237</td>\n",
       "      <td>0.751831</td>\n",
       "      <td>0.856981</td>\n",
       "      <td>0.768841</td>\n",
       "      <td>0.529645</td>\n",
       "      <td>0.954614</td>\n",
       "      <td>0.372392</td>\n",
       "      <td>0.335992</td>\n",
       "      <td>0.727246</td>\n",
       "      <td>...</td>\n",
       "      <td>0.379027</td>\n",
       "      <td>0.326835</td>\n",
       "      <td>0.305716</td>\n",
       "      <td>0.280504</td>\n",
       "      <td>0.323942</td>\n",
       "      <td>0.273627</td>\n",
       "      <td>0.195085</td>\n",
       "      <td>0.405561</td>\n",
       "      <td>0.359562</td>\n",
       "      <td>0.338459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.807853</td>\n",
       "      <td>0.739236</td>\n",
       "      <td>0.756555</td>\n",
       "      <td>0.838984</td>\n",
       "      <td>0.749880</td>\n",
       "      <td>0.512322</td>\n",
       "      <td>0.946342</td>\n",
       "      <td>0.361297</td>\n",
       "      <td>0.228889</td>\n",
       "      <td>0.679450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.357983</td>\n",
       "      <td>0.246972</td>\n",
       "      <td>0.261173</td>\n",
       "      <td>0.204835</td>\n",
       "      <td>0.185487</td>\n",
       "      <td>0.220200</td>\n",
       "      <td>0.183395</td>\n",
       "      <td>0.291305</td>\n",
       "      <td>0.310701</td>\n",
       "      <td>0.277985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.807047</td>\n",
       "      <td>0.729387</td>\n",
       "      <td>0.763370</td>\n",
       "      <td>0.818535</td>\n",
       "      <td>0.726867</td>\n",
       "      <td>0.520051</td>\n",
       "      <td>0.940714</td>\n",
       "      <td>0.412594</td>\n",
       "      <td>0.297168</td>\n",
       "      <td>0.677224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.402075</td>\n",
       "      <td>0.205109</td>\n",
       "      <td>0.222423</td>\n",
       "      <td>0.211773</td>\n",
       "      <td>0.076387</td>\n",
       "      <td>0.233315</td>\n",
       "      <td>0.283959</td>\n",
       "      <td>0.347730</td>\n",
       "      <td>0.391543</td>\n",
       "      <td>0.266929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.815769</td>\n",
       "      <td>0.725591</td>\n",
       "      <td>0.742734</td>\n",
       "      <td>0.825777</td>\n",
       "      <td>0.741116</td>\n",
       "      <td>0.514395</td>\n",
       "      <td>0.954288</td>\n",
       "      <td>0.362880</td>\n",
       "      <td>0.302591</td>\n",
       "      <td>0.675439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.350479</td>\n",
       "      <td>0.243489</td>\n",
       "      <td>0.283342</td>\n",
       "      <td>0.271770</td>\n",
       "      <td>0.268658</td>\n",
       "      <td>0.197210</td>\n",
       "      <td>0.208597</td>\n",
       "      <td>0.324356</td>\n",
       "      <td>0.399081</td>\n",
       "      <td>0.329099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.817828</td>\n",
       "      <td>0.735251</td>\n",
       "      <td>0.729180</td>\n",
       "      <td>0.812079</td>\n",
       "      <td>0.723800</td>\n",
       "      <td>0.484208</td>\n",
       "      <td>0.912951</td>\n",
       "      <td>0.364798</td>\n",
       "      <td>0.114950</td>\n",
       "      <td>0.676715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.295223</td>\n",
       "      <td>0.346479</td>\n",
       "      <td>0.293446</td>\n",
       "      <td>0.316083</td>\n",
       "      <td>0.347091</td>\n",
       "      <td>0.259914</td>\n",
       "      <td>0.304735</td>\n",
       "      <td>0.333377</td>\n",
       "      <td>0.409943</td>\n",
       "      <td>0.248880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.815332</td>\n",
       "      <td>0.737245</td>\n",
       "      <td>0.754298</td>\n",
       "      <td>0.844369</td>\n",
       "      <td>0.749922</td>\n",
       "      <td>0.552492</td>\n",
       "      <td>0.947020</td>\n",
       "      <td>0.364876</td>\n",
       "      <td>0.255961</td>\n",
       "      <td>0.707385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329426</td>\n",
       "      <td>0.262290</td>\n",
       "      <td>0.244829</td>\n",
       "      <td>0.186200</td>\n",
       "      <td>0.235827</td>\n",
       "      <td>0.133048</td>\n",
       "      <td>0.184496</td>\n",
       "      <td>0.297047</td>\n",
       "      <td>0.374515</td>\n",
       "      <td>0.217278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.795091</td>\n",
       "      <td>0.698683</td>\n",
       "      <td>0.734965</td>\n",
       "      <td>0.843130</td>\n",
       "      <td>0.741016</td>\n",
       "      <td>0.537043</td>\n",
       "      <td>0.938839</td>\n",
       "      <td>0.355818</td>\n",
       "      <td>0.268145</td>\n",
       "      <td>0.689790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.291312</td>\n",
       "      <td>0.200119</td>\n",
       "      <td>0.177841</td>\n",
       "      <td>0.217369</td>\n",
       "      <td>0.306494</td>\n",
       "      <td>0.246859</td>\n",
       "      <td>0.316634</td>\n",
       "      <td>0.321032</td>\n",
       "      <td>0.332343</td>\n",
       "      <td>0.248622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.811965</td>\n",
       "      <td>0.732277</td>\n",
       "      <td>0.761629</td>\n",
       "      <td>0.818184</td>\n",
       "      <td>0.737511</td>\n",
       "      <td>0.544076</td>\n",
       "      <td>0.924104</td>\n",
       "      <td>0.398830</td>\n",
       "      <td>0.186060</td>\n",
       "      <td>0.680776</td>\n",
       "      <td>...</td>\n",
       "      <td>0.345292</td>\n",
       "      <td>0.331032</td>\n",
       "      <td>0.338349</td>\n",
       "      <td>0.202437</td>\n",
       "      <td>0.230945</td>\n",
       "      <td>0.174608</td>\n",
       "      <td>0.229505</td>\n",
       "      <td>0.382746</td>\n",
       "      <td>0.403005</td>\n",
       "      <td>0.216000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "0   0.822696  0.749512  0.738373  0.829757  0.725952  0.510500  0.928945   \n",
       "1   0.807597  0.717130  0.739479  0.836040  0.743131  0.509932  0.938481   \n",
       "2   0.808222  0.711237  0.751831  0.856981  0.768841  0.529645  0.954614   \n",
       "3   0.807853  0.739236  0.756555  0.838984  0.749880  0.512322  0.946342   \n",
       "4   0.807047  0.729387  0.763370  0.818535  0.726867  0.520051  0.940714   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "65  0.815769  0.725591  0.742734  0.825777  0.741116  0.514395  0.954288   \n",
       "66  0.817828  0.735251  0.729180  0.812079  0.723800  0.484208  0.912951   \n",
       "67  0.815332  0.737245  0.754298  0.844369  0.749922  0.552492  0.947020   \n",
       "68  0.795091  0.698683  0.734965  0.843130  0.741016  0.537043  0.938839   \n",
       "69  0.811965  0.732277  0.761629  0.818184  0.737511  0.544076  0.924104   \n",
       "\n",
       "           7         8         9  ...       190       191       192       193  \\\n",
       "0   0.401692  0.279266  0.673976  ...  0.337899  0.256514  0.289538  0.182423   \n",
       "1   0.344084  0.228623  0.689352  ...  0.273958  0.292204  0.217053  0.243807   \n",
       "2   0.372392  0.335992  0.727246  ...  0.379027  0.326835  0.305716  0.280504   \n",
       "3   0.361297  0.228889  0.679450  ...  0.357983  0.246972  0.261173  0.204835   \n",
       "4   0.412594  0.297168  0.677224  ...  0.402075  0.205109  0.222423  0.211773   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "65  0.362880  0.302591  0.675439  ...  0.350479  0.243489  0.283342  0.271770   \n",
       "66  0.364798  0.114950  0.676715  ...  0.295223  0.346479  0.293446  0.316083   \n",
       "67  0.364876  0.255961  0.707385  ...  0.329426  0.262290  0.244829  0.186200   \n",
       "68  0.355818  0.268145  0.689790  ...  0.291312  0.200119  0.177841  0.217369   \n",
       "69  0.398830  0.186060  0.680776  ...  0.345292  0.331032  0.338349  0.202437   \n",
       "\n",
       "         194       195       196       197       198       199  \n",
       "0   0.265771  0.254107  0.222156  0.348486  0.386689  0.317780  \n",
       "1   0.287193  0.272302  0.274035  0.366102  0.316983  0.308586  \n",
       "2   0.323942  0.273627  0.195085  0.405561  0.359562  0.338459  \n",
       "3   0.185487  0.220200  0.183395  0.291305  0.310701  0.277985  \n",
       "4   0.076387  0.233315  0.283959  0.347730  0.391543  0.266929  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "65  0.268658  0.197210  0.208597  0.324356  0.399081  0.329099  \n",
       "66  0.347091  0.259914  0.304735  0.333377  0.409943  0.248880  \n",
       "67  0.235827  0.133048  0.184496  0.297047  0.374515  0.217278  \n",
       "68  0.306494  0.246859  0.316634  0.321032  0.332343  0.248622  \n",
       "69  0.230945  0.174608  0.229505  0.382746  0.403005  0.216000  \n",
       "\n",
       "[70 rows x 200 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 读取制表符分隔的CSV文件\n",
    "# RNAseq_val = pd.read_csv('../../DATA/THCA_parting/RNAseq_val.csv')\n",
    "# # 显示 DataFrame\n",
    "# RNAseq_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb4c79b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0  \n",
       "1.0    113\n",
       "0.0     97\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用np.load()函数加载.npy文件\n",
    "labels_train = pd.read_csv('../../DATA/THCA_parting/labels_tr.csv')\n",
    "labels_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36706f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0  \n",
       "0.0    36\n",
       "1.0    35\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用np.load()函数加载.npy文件\n",
    "labels_test = pd.read_csv('../../DATA/THCA_parting/labels_te.csv')\n",
    "labels_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e068632b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0  \n",
       "0.0    36\n",
       "1.0    34\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 使用np.load()函数加载.npy文件\n",
    "# labels_val = pd.read_csv('../../DATA/THCA_parting/labels_val.csv')\n",
    "# labels_val.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "813d1549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dim = len(np.unique(labels_train))\n",
    "label_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aae8d01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 使用 GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28be32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 0 已转换为 Tensor，并移动到 cuda:0\n",
      "DataFrame 1 已转换为 Tensor，并移动到 cuda:0\n",
      "DataFrame 2 已转换为 Tensor，并移动到 cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 调用函数，将这些 numpy 数组转换为 tensors\n",
    "train_tensors = batch_dataframe_to_tensors(Methylation_train, miRNASeq_train, RNAseq_train)\n",
    "\n",
    "# 现在 tensors 是一个包含这些转换后张量的列表\n",
    "Methylation_train = train_tensors[0]\n",
    "miRNASeq_train = train_tensors[1]\n",
    "RNAseq_train = train_tensors[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23632e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 0 已转换为 Tensor，并移动到 cuda:0\n",
      "DataFrame 1 已转换为 Tensor，并移动到 cuda:0\n",
      "DataFrame 2 已转换为 Tensor，并移动到 cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 调用函数，将这些 numpy 数组转换为 tensors\n",
    "test_tensors = batch_dataframe_to_tensors(Methylation_test, miRNASeq_test, RNAseq_test)\n",
    "\n",
    "# 现在 tensors 是一个包含这些转换后张量的列表\n",
    "Methylation_test = test_tensors[0]\n",
    "miRNASeq_test = test_tensors[1]\n",
    "RNAseq_test = test_tensors[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03468a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame 0 已转换为 Tensor，并移动到 cuda:0\n",
      "DataFrame 1 已转换为 Tensor，并移动到 cuda:0\n",
      "DataFrame 2 已转换为 Tensor，并移动到 cuda:0\n"
     ]
    }
   ],
   "source": [
    "# # 调用函数，将这些 numpy 数组转换为 tensors\n",
    "# val_tensors = batch_dataframe_to_tensors(Methylation_val, miRNASeq_val, RNAseq_val)\n",
    "\n",
    "# # 现在 tensors 是一个包含这些转换后张量的列表\n",
    "# Methylation_val = val_tensors[0]\n",
    "# miRNASeq_val = val_tensors[1]\n",
    "# RNAseq_val = val_tensors[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8860b5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将DataFrame的值转换为Tensor\n",
    "labels_tr_tensor = torch.LongTensor(labels_train.values)\n",
    "labels_tr_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e593db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_labels_tr_tensor = one_hot_tensor(labels_tr_tensor, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0203bbc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将DataFrame的值转换为Tensor\n",
    "labels_te_tensor = torch.LongTensor(labels_test.values)\n",
    "labels_te_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0b725ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将DataFrame的值转换为Tensor\n",
    "labels_tr_tensor = torch.LongTensor(labels_train.values)\n",
    "labels_tr_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b7910e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 将DataFrame的值转换为Tensor\n",
    "# labels_val_tensor = torch.LongTensor(labels_val.values)\n",
    "# labels_val_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a22e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据集\n",
    "train_dataset = TensorDataset(Methylation_train, miRNASeq_train, RNAseq_train, labels_tr_tensor)\n",
    "test_dataset = TensorDataset(Methylation_test, miRNASeq_test, RNAseq_test, labels_te_tensor)\n",
    "# val_dataset = TensorDataset(Methylation_val, miRNASeq_val, RNAseq_val, labels_val_tensor)\n",
    "\n",
    "# 定义 DataLoader，使用自定义采样器\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)  # 测试集一般不需要打乱顺序\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)  # 验证集也一般不需要重复采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4bdfa98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: 32 samples\n",
      "Batch 2: 32 samples\n",
      "Batch 3: 32 samples\n",
      "Batch 4: 32 samples\n",
      "Batch 5: 32 samples\n",
      "Batch 6: 32 samples\n",
      "Batch 7: 18 samples\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_loader):\n",
    "    print(f\"Batch {i + 1}: {len(batch[0])} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6130e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: 32 samples\n",
      "Batch 2: 32 samples\n",
      "Batch 3: 7 samples\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(test_loader):\n",
    "    print(f\"Batch {i + 1}: {len(batch[0])} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1cdddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: 32 samples\n",
      "Batch 2: 32 samples\n",
      "Batch 3: 6 samples\n"
     ]
    }
   ],
   "source": [
    "# for i, batch in enumerate(val_loader):\n",
    "#     print(f\"Batch {i + 1}: {len(batch[0])} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409b569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from typing import Dict, Tuple, List\n",
    "import gc\n",
    "\n",
    "def plot_metrics(lr: float, depth: int, heads: int, dim_head: int, beta: float, adj_parameter: int, dropout: float) -> None:\n",
    "    \"\"\"生成四分类训练指标图表\"\"\"\n",
    "    \n",
    "    # ================== 配置参数 ==================\n",
    "    SAVE_DIR = '../../result/THCA_parting/'\n",
    "    FILE_FORMAT = 'pdf'\n",
    "    DPI = 600\n",
    "    FONT_CONFIG = {\n",
    "        'font.family': 'DejaVu Sans',\n",
    "        'font.size': 12,\n",
    "        'axes.titlesize': 14,\n",
    "        'axes.labelsize': 12,\n",
    "        'legend.fontsize': 10\n",
    "    }\n",
    "    STYLE_CONFIG = {\n",
    "        'train': {'color': '#E74C3C', 'ls': '-', 'lw': 2.5, 'alpha': 0.9},\n",
    "        'val': {'color': '#3498DB', 'ls': '-', 'lw': 2.5, 'alpha': 0.9},\n",
    "        'test': {'color': '#2ECC71', 'ls': '-', 'lw': 2.5, 'alpha': 0.9}\n",
    "    }\n",
    "    \n",
    "    # ================== 初始化设置 ==================\n",
    "    rcParams.update(FONT_CONFIG)\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    param_id = f\"lr{lr:.0e}_depth{depth}heads{heads}dim_head{dim_head}adj_parameter{adj_parameter}dropout{dropout}beta{beta}\".replace('.', '').replace('+', '')\n",
    "\n",
    "    try:\n",
    "        # ================== 创建画布 ==================\n",
    "        fig = plt.figure(figsize=(18, 12), dpi=DPI, facecolor='white')\n",
    "        fig.suptitle(f'Training Metrics ({param_id})', y=1.02, fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # ================== 子图配置 ==================\n",
    "        axes = [\n",
    "            fig.add_subplot(221),  # Loss\n",
    "            fig.add_subplot(222),  # Accuracy\n",
    "            fig.add_subplot(223),  # F1 Weighted\n",
    "            fig.add_subplot(224)   # F1 Macro\n",
    "        ]\n",
    "        \n",
    "        metric_data = [\n",
    "            ('Loss', (train_losses, test_losses), 'log' if max(train_losses) > 1e3 else 'linear'),\n",
    "            ('Accuracy', (train_accuracies, test_accuracies), 'linear'),\n",
    "            ('F1 Weighted', (train_f1_weighted_scores, test_f1_weighted_scores), 'linear'),\n",
    "            ('F1 Macro', (train_f1_macro_scores, test_f1_macro_scores), 'linear')\n",
    "        ]\n",
    "\n",
    "        # ================== 绘制图表 ==================\n",
    "        for ax, (title, (train, test), scale) in zip(axes, metric_data):\n",
    "            ax.plot(train, label='Train', **STYLE_CONFIG['train'])\n",
    "            # ax.plot(val, label='Val', **STYLE_CONFIG['val'])\n",
    "            ax.plot(test, label='Test', **STYLE_CONFIG['test'])\n",
    "            \n",
    "            ax.set_title(title, pad=15, fontweight='semibold')\n",
    "            ax.set_xlabel('Epochs', labelpad=10)\n",
    "            ax.set_ylabel(title, labelpad=10)\n",
    "            ax.grid(True, which='both', ls='--', alpha=0.4)\n",
    "            ax.set_yscale(scale)\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            \n",
    "            if ax == axes[0]:\n",
    "                handles, labels = ax.get_legend_handles_labels()\n",
    "                fig.legend(handles, labels, loc='upper center', \n",
    "                         bbox_to_anchor=(0.5, 1.0), ncol=3, frameon=False)\n",
    "\n",
    "        # ================== 布局优化 ==================\n",
    "        plt.tight_layout(pad=3.0, w_pad=2.0, h_pad=2.0)\n",
    "        \n",
    "        # ================== 保存图像 ==================\n",
    "        save_path = os.path.join(SAVE_DIR, f'metrics_{param_id}.{FILE_FORMAT}')\n",
    "        plt.savefig(\n",
    "            save_path,\n",
    "            dpi=DPI,\n",
    "            bbox_inches='tight',\n",
    "            facecolor=fig.get_facecolor(),\n",
    "            edgecolor='none',\n",
    "            transparent=False,\n",
    "            metadata={'CreationDate': None}\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"图表生成失败: {str(e)}\")\n",
    "    finally:\n",
    "        plt.close(fig)\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bfd89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(data_loader, model, optimizer, adj_parameter, output_attentions=False):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_probs, all_labels = [], []\n",
    "\n",
    "    for batch in data_loader:\n",
    "        Methylation_batch, miRNASeq_batch, RNAseq_batch, labels_batch = [\n",
    "            tensor.to(device) for tensor in batch\n",
    "        ]\n",
    "\n",
    "        # 生成三维邻接张量\n",
    "        graph_train = np.stack([\n",
    "            gen_trte_adj_mat(Methylation_batch.cpu(), adj_parameter),\n",
    "            gen_trte_adj_mat(miRNASeq_batch.cpu(), adj_parameter),\n",
    "            gen_trte_adj_mat(RNAseq_batch.cpu(), adj_parameter)\n",
    "        ], axis=-1)\n",
    "        graph_train = torch.from_numpy(graph_train).float().to(device).permute(0, 2, 1)\n",
    "\n",
    "        # 梯度更新流程\n",
    "        optimizer.zero_grad()\n",
    "        ci_loss, ci, feature = model(\n",
    "            Methylation_batch, \n",
    "            miRNASeq_batch,\n",
    "            RNAseq_batch,\n",
    "            graph_train,\n",
    "            labels_batch,\n",
    "            output_attentions=output_attentions\n",
    "        )\n",
    "        ci_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 结果收集\n",
    "        total_loss += ci_loss.item() * labels_batch.size(0)\n",
    "        all_probs.append(F.softmax(ci, dim=1).detach().cpu().numpy())\n",
    "        all_labels.append(labels_batch.detach().cpu().numpy().flatten())\n",
    "\n",
    "    return (\n",
    "        total_loss / len(data_loader.dataset),\n",
    "        np.concatenate(all_probs),\n",
    "        np.concatenate(all_labels)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e66ebda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def val_epoch(data_loader, model, adj_parameter, output_attentions=False):\n",
    "#     model.eval()\n",
    "#     total_loss = 0.0\n",
    "#     all_labels, all_probs = [], []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in data_loader:\n",
    "#             # 数据预处理\n",
    "#             Methylation_batch, miRNASeq_batch, RNAseq_batch, labels_batch = [\n",
    "#                 x.to(device) for x in batch\n",
    "#             ]\n",
    "#             # print(miRNASeq_batch.shape) #torch.Size([32, 217])\n",
    "#             # 生成三维邻接张量\n",
    "#             graph_val = torch.from_numpy(np.stack([\n",
    "#                 gen_trte_adj_mat(d.cpu(), adj_parameter) \n",
    "#                 for d in [Methylation_batch, miRNASeq_batch, RNAseq_batch]\n",
    "#             ], axis=-1)).float().to(device).permute(0,2,1)\n",
    "\n",
    "#             # 前向计算\n",
    "#             ci_loss, ci = model(\n",
    "#                 Methylation_batch, \n",
    "#                 miRNASeq_batch,\n",
    "#                 RNAseq_batch,\n",
    "#                 graph_val,\n",
    "#                 labels_batch,\n",
    "#                 output_attentions=output_attentions\n",
    "#             )\n",
    "\n",
    "#             # 累计结果\n",
    "#             total_loss += ci_loss.item() * labels_batch.size(0)\n",
    "#             all_probs.append(F.softmax(ci, 1).detach().cpu().numpy())\n",
    "#             all_labels.append(labels_batch.detach().cpu().numpy().flatten())\n",
    "\n",
    "#     return (\n",
    "#         total_loss / len(data_loader.dataset),\n",
    "#         np.concatenate(all_probs),\n",
    "#         np.concatenate(all_labels)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a309d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(data_loader, model, adj_parameter, output_attentions=False):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_probs, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            # 数据预处理 (保持原始变量名)\n",
    "            Methylation_batch, miRNASeq_batch, RNAseq_batch, labels_test_tensor = [\n",
    "                tensor.to(device) for tensor in batch\n",
    "            ]\n",
    "\n",
    "            # 生成邻接矩阵 (保持原始逻辑但更紧凑)\n",
    "            graph_test = torch.from_numpy(\n",
    "                np.stack([\n",
    "                    gen_trte_adj_mat(Methylation_batch.cpu(), adj_parameter),\n",
    "                    gen_trte_adj_mat(miRNASeq_batch.cpu(), adj_parameter),\n",
    "                    gen_trte_adj_mat(RNAseq_batch.cpu(), adj_parameter)\n",
    "                ], axis=-1)\n",
    "            ).float().to(device).permute(0, 2, 1)\n",
    "\n",
    "            # 前向计算\n",
    "            loss, logits, feature = model(\n",
    "                Methylation_batch,\n",
    "                miRNASeq_batch,\n",
    "                RNAseq_batch,\n",
    "                graph_test,\n",
    "                labels_test_tensor,\n",
    "                output_attentions=output_attentions\n",
    "            )\n",
    "            feature = pd.DataFrame(feature.detach().cpu().numpy())\n",
    "\n",
    "            # 保存为 CSV 文件\n",
    "            feature.to_csv('feature.csv', index=False)\n",
    "            # 结果收集\n",
    "            total_loss += loss.item() * labels_test_tensor.size(0)\n",
    "            all_probs.append(F.softmax(logits, dim=1).detach().cpu().numpy())\n",
    "            labels_test_tensor_df = pd.DataFrame(labels_test_tensor.detach().cpu().numpy())\n",
    "            labels_test_tensor_df.to_csv('all_probs.csv', index=False)\n",
    "            all_labels.append(labels_test_tensor.detach().cpu().numpy().flatten())\n",
    "\n",
    "    return (\n",
    "        total_loss / len(data_loader.dataset),\n",
    "        np.concatenate(all_probs),\n",
    "        np.concatenate(all_labels)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a07e129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于保存指标数据的列表（移除了AUC相关指标）\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "test_losses = []\n",
    "val_accuracies = []\n",
    "val_f1_weighted_scores = []\n",
    "val_f1_macro_scores = []\n",
    "test_accuracies = []\n",
    "test_f1_weighted_scores = []\n",
    "test_f1_macro_scores = []\n",
    "train_accuracies = []\n",
    "train_f1_weighted_scores = []\n",
    "train_f1_macro_scores = []\n",
    "\n",
    "def train_test(\n",
    "        Methylation_train,\n",
    "        miRNASeq_train,\n",
    "        RNAseq_train,\n",
    "        lr,\n",
    "        depth, \n",
    "        heads, \n",
    "        dim_head,\n",
    "        beta, \n",
    "        attn_dropout,\n",
    "        ff_dropout, \n",
    "        classifier_dropout,\n",
    "        adj_parameter,\n",
    "        num_epoch_pretrain\n",
    "):\n",
    "    # 初始化最佳指标跟踪器（以val_f1_macro为选择标准）\n",
    "    best_metrics = {\n",
    "        'epoch': -1,\n",
    "        # 'val_f1_macro': -1.0,\n",
    "        'test_acc': 0.0,\n",
    "        'test_f1_weighted': 0.0,\n",
    "        'test_f1_macro': 0.0,\n",
    "    }\n",
    "    \n",
    "    DEPTH = depth\n",
    "    HEAD = heads\n",
    "    HEAD_DIM = dim_head\n",
    "\n",
    "    classifier_input = [Methylation_train.shape[1], miRNASeq_train.shape[1], RNAseq_train.shape[1]]\n",
    "    classifier_dim = [\n",
    "        [classifier_input[0], 300, 200],\n",
    "        [classifier_input[1], 300, 200],\n",
    "        [classifier_input[2], 300, 200]\n",
    "    ]\n",
    "    col_dim = 32\n",
    "\n",
    "    # 确定 row_dim 和 embeding_num\n",
    "    if num_view <= 2:\n",
    "        embeding = True\n",
    "        embeding_num = 32\n",
    "        row_dim = embeding_num\n",
    "    else:\n",
    "        embeding = False\n",
    "        row_dim = num_view\n",
    "        embeding_num = 32\n",
    "    \n",
    "    model = pathformer_model(\n",
    "        row_dim=row_dim,\n",
    "        col_dim=col_dim,\n",
    "        depth=DEPTH,\n",
    "        heads=HEAD,\n",
    "        dim_head=HEAD_DIM,\n",
    "        classifier_input=classifier_input,\n",
    "        classifier_dim=classifier_dim,\n",
    "        label_dim=label_dim,\n",
    "        embeding=embeding,\n",
    "        embeding_num=embeding_num,\n",
    "        beta=beta,\n",
    "        attn_dropout=attn_dropout,\n",
    "        ff_dropout=ff_dropout,\n",
    "        classifier_dropout=classifier_dropout,\n",
    "        num_view=num_view,\n",
    "        dim_hvcdn=dim_hvcdn\n",
    "    )\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model.to(device)\n",
    "\n",
    "    logging.info(\"\\nPretrain GCNs...\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    logging.info(\"\\nTraining...\")\n",
    "    for epoch in range(num_epoch_pretrain + 1):\n",
    "        # 训练阶段\n",
    "        train_loss, train_probs, labels_train_tensor = train_epoch(train_loader, model, optimizer, adj_parameter, output_attentions=False)\n",
    "        train_acc = accuracy_score(labels_train_tensor, train_probs.argmax(1))\n",
    "        train_f1_weighted = f1_score(labels_train_tensor, train_probs.argmax(1), average='weighted')\n",
    "        train_f1_macro = f1_score(labels_train_tensor, train_probs.argmax(1), average='macro')\n",
    "\n",
    "        logging.info(f\"Train Epoch {epoch}, Loss: {train_loss}\")\n",
    "        logging.info(f\"Train Epoch {epoch}, ACC: {train_acc:.3f}\")\n",
    "        logging.info(f\"Train Epoch {epoch}, F1 Weighted: {train_f1_weighted:.3f}\")\n",
    "        logging.info(f\"Train Epoch {epoch}, F1 Macro: {train_f1_macro:.3f}\")\n",
    "        \n",
    "        # 验证阶段\n",
    "        # val_loss, val_probs, labels_validation_tensor = val_epoch(val_loader, model, adj_parameter, output_attentions=False)\n",
    "        # val_acc = accuracy_score(labels_validation_tensor, val_probs.argmax(1))\n",
    "        # val_f1_weighted = f1_score(labels_validation_tensor, val_probs.argmax(1), average='weighted')\n",
    "        # val_f1_macro = f1_score(labels_validation_tensor, val_probs.argmax(1), average='macro')\n",
    "        \n",
    "        # logging.info(f\"Val Epoch {epoch}, Loss: {val_loss}\")\n",
    "        # logging.info(f\"Val Epoch {epoch}, ACC: {val_acc:.3f}\")\n",
    "        # logging.info(f\"Val Epoch {epoch}, F1 Weighted: {val_f1_weighted:.3f}\")\n",
    "        # logging.info(f\"Val Epoch {epoch}, F1 Macro: {val_f1_macro:.3f}\")\n",
    "        \n",
    "        # 测试阶段\n",
    "        test_loss, test_probs, labels_test_tensor = test_epoch(test_loader, model, adj_parameter, output_attentions=False)\n",
    "        test_acc = accuracy_score(labels_test_tensor, test_probs.argmax(1))\n",
    "        test_f1_weighted = f1_score(labels_test_tensor, test_probs.argmax(1), average='weighted')\n",
    "        test_f1_macro = f1_score(labels_test_tensor, test_probs.argmax(1), average='macro')\n",
    "        \n",
    "        logging.info(f\"Test Epoch {epoch}, Loss: {test_loss}\")\n",
    "        logging.info(f\"Test Epoch {epoch}, ACC: {test_acc:.3f}\")\n",
    "        logging.info(f\"Test Epoch {epoch}, F1 Weighted: {test_f1_weighted:.3f}\")\n",
    "        logging.info(f\"Test Epoch {epoch}, F1 Macro: {test_f1_macro:.3f}\")\n",
    "\n",
    "        # 更新最佳指标（以val_f1_macro为选择标准）\n",
    "        if test_f1_weighted > best_metrics['test_f1_weighted'] or \\\n",
    "           (test_f1_weighted == best_metrics['test_f1_weighted'] and test_f1_macro > best_metrics['test_f1_macro']):\n",
    "            best_metrics.update({\n",
    "                'epoch': epoch,\n",
    "                # 'val_f1_macro': val_f1_macro,\n",
    "                'test_acc': test_acc,\n",
    "                'test_f1_weighted': test_f1_weighted,\n",
    "                'test_f1_macro': test_f1_macro\n",
    "            })\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"Saved best model at epoch {epoch}\")\n",
    "\n",
    "        logging.info(\"\\n\")\n",
    "\n",
    "        # 保存训练指标\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        train_f1_weighted_scores.append(train_f1_weighted)\n",
    "        train_f1_macro_scores.append(train_f1_macro)\n",
    "        \n",
    "        # val_losses.append(val_loss)\n",
    "        # val_accuracies.append(val_acc)\n",
    "        # val_f1_weighted_scores.append(val_f1_weighted)\n",
    "        # val_f1_macro_scores.append(val_f1_macro)\n",
    "        \n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_acc)\n",
    "        test_f1_weighted_scores.append(test_f1_weighted)\n",
    "        test_f1_macro_scores.append(test_f1_macro)\n",
    "\n",
    "        # 绘制并保存指标图（需调整绘图函数）\n",
    "        # plot_metrics(lr, depth, heads, dim_head, beta, adj_parameter, dropout=attn_dropout)\n",
    "\n",
    "    return (\n",
    "        best_metrics['test_acc'],\n",
    "        best_metrics['test_f1_weighted'],\n",
    "        best_metrics['test_f1_macro']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e819324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 用于保存指标数据的列表\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "# test_losses = []\n",
    "# val_accuracies = []\n",
    "# val_f1_scores = []\n",
    "# val_auc_scores = []\n",
    "# test_accuracies = []\n",
    "# test_f1_scores = []\n",
    "# test_auc_scores = []\n",
    "# train_accuracies = []\n",
    "# train_f1_scores = []\n",
    "# train_auc_scores = []\n",
    "# train_c_index = []\n",
    "# val_c_index = []\n",
    "# test_c_index = []\n",
    "\n",
    "# def train_test(\n",
    "#         Methylation_train,\n",
    "#         Methylation_test,\n",
    "#         miRNASeq_train,\n",
    "#         miRNASeq_test,\n",
    "#         RNAseq_train,\n",
    "#         RNAseq_test,\n",
    "#         label_train,\n",
    "#         label_test, \n",
    "#         labels_tr_tensor,\n",
    "#         lr,\n",
    "#         depth, \n",
    "#         heads, \n",
    "#         dim_head,\n",
    "#         beta, \n",
    "#         attn_dropout,\n",
    "#         ff_dropout, \n",
    "#         classifier_dropout,\n",
    "#         adj_parameter,\n",
    "#         num_epoch_pretrain\n",
    "# ):\n",
    "    \n",
    "#     DEPTH = depth\n",
    "#     HEAD = heads\n",
    "#     HEAD_DIM = dim_head\n",
    "\n",
    "#     classifier_input = [Methylation_train.shape[1], miRNASeq_train.shape[1], RNAseq_train.shape[1]]\n",
    "#     classifier_dim = [\n",
    "#         [classifier_input[0], 300, 200],\n",
    "#         [classifier_input[1], 300, 200],\n",
    "#         [classifier_input[2], 300, 200]\n",
    "#     ]\n",
    "#     col_dim = 71\n",
    "\n",
    "#     # 确定 row_dim 和 embeding_num\n",
    "#     if num_view <= 2:\n",
    "#         embeding = True\n",
    "#         embeding_num = 71\n",
    "#         row_dim = embeding_num  # 请根据需要修改此处\n",
    "#     else:\n",
    "#         embeding = False\n",
    "#         row_dim = num_view\n",
    "#         embeding_num = 71\n",
    "    \n",
    "#     model = pathformer_model(\n",
    "#         row_dim=row_dim,\n",
    "#         col_dim=col_dim,\n",
    "#         depth=DEPTH,\n",
    "#         heads=HEAD,\n",
    "#         dim_head=HEAD_DIM,\n",
    "#         classifier_input=classifier_input,\n",
    "#         classifier_dim=classifier_dim,\n",
    "#         label_dim=label_dim,\n",
    "#         embeding=embeding,\n",
    "#         embeding_num=embeding_num,\n",
    "#         beta=beta,\n",
    "#         attn_dropout=attn_dropout,\n",
    "#         ff_dropout=ff_dropout,\n",
    "#         classifier_dropout=classifier_dropout,\n",
    "#         num_view=num_view,\n",
    "#         dim_hvcdn=dim_hvcdn\n",
    "#     )\n",
    "\n",
    "#     if torch.cuda.is_available():\n",
    "#         model.to(device)\n",
    "\n",
    "#     print(\"\\nPretrain GCNs...\")\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#     print(\"\\nTraining...\")\n",
    "#     for epoch in range(num_epoch_pretrain + 1):\n",
    "#         # 每一轮都进行训练\n",
    "#         train_loss, train_probs, labels_train_tensor = train_epoch(train_loader, model, optimizer, adj_parameter, epoch, output_attentions=False)\n",
    "#         train_acc = accuracy_score(labels_train_tensor, train_probs.argmax(1))\n",
    "#         train_f1 = f1_score(labels_train_tensor, train_probs.argmax(1))\n",
    "#         train_auc = roc_auc_score(labels_train_tensor, train_probs[:, 1])\n",
    "# #         print(f\"Train Epoch {epoch}, Loss: {train_loss}\")\n",
    "# #         print(f\"Train Epoch {epoch}, ACC: Train ACC: {train_acc:.3f}\")\n",
    "# #         print(f\"Train Epoch {epoch}, f1: Train F1: {train_f1:.3f}\")\n",
    "# #         print(f\"Train Epoch {epoch}, AUC: Train AUC: {train_auc:.3f}\")\n",
    "        \n",
    "#         # 每一轮都进行验证\n",
    "#         val_loss, val_probs, labels_val_tensor = val_epoch(val_loader, model, adj_parameter, epoch, output_attentions=False)\n",
    "#         val_acc = accuracy_score(labels_val_tensor, val_probs.argmax(1))\n",
    "#         val_f1 = f1_score(labels_val_tensor, val_probs.argmax(1))\n",
    "#         val_auc = roc_auc_score(labels_val_tensor, val_probs[:, 1])\n",
    "# #         print(f\"val Epoch {epoch}, Loss: {val_loss}\")\n",
    "# #         print(f\"val Epoch {epoch}, ACC: val ACC: {val_acc:.3f}\")\n",
    "# #         print(f\"val Epoch {epoch}, f1: val F1: {val_f1:.3f}\")\n",
    "# #         print(f\"val Epoch {epoch}, AUC: val AUC: {val_auc:.3f}\")\n",
    "        \n",
    "#         # 每一轮都进行测试\n",
    "#         test_loss, test_probs, labels_test_tensor = test_epoch(test_loader, model, adj_parameter, epoch, output_attentions=False)\n",
    "#         test_acc = accuracy_score(labels_test_tensor, test_probs.argmax(1))\n",
    "#         test_f1 = f1_score(labels_test_tensor, test_probs.argmax(1))\n",
    "#         test_auc = roc_auc_score(labels_test_tensor, test_probs[:, 1])\n",
    "# #         print(f\"test Epoch {epoch}, Loss: {test_loss}\")\n",
    "# #         print(f\"test Epoch {epoch}, ACC: test ACC: {test_acc:.3f}\")\n",
    "# #         print(f\"test Epoch {epoch}, f1: test F1: {test_f1:.3f}\")\n",
    "# #         print(f\"test Epoch {epoch}, AUC: test AUC: {test_auc:.3f}\")\n",
    "\n",
    "#         print(\"\\n\")\n",
    "\n",
    "#         # 保存训练指标\n",
    "#         train_losses.append(train_loss)\n",
    "#         train_accuracies.append(train_acc)\n",
    "#         train_f1_scores.append(train_f1)\n",
    "#         train_auc_scores.append(train_auc)\n",
    "        \n",
    "#         val_losses.append(val_loss)\n",
    "#         val_accuracies.append(val_acc)\n",
    "#         val_f1_scores.append(val_f1)\n",
    "#         val_auc_scores.append(val_auc)\n",
    "        \n",
    "#         test_losses.append(test_loss)\n",
    "#         test_accuracies.append(test_acc)\n",
    "#         test_f1_scores.append(test_f1)\n",
    "#         test_auc_scores.append(test_auc)\n",
    "\n",
    "#         # 绘制并保存指标图\n",
    "#         plot_metrics(lr, depth, heads, dim_head, beta, dropout=attn_dropout)\n",
    "\n",
    "#     # 返回最终的预测结果和指标\n",
    "#     final_predictions = test_probs.argmax(1)\n",
    "#     return final_predictions, test_acc, test_f1, test_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccd421e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "# import os\n",
    "# import time\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "# # 配置日志和路径\n",
    "# LOG_FILE = '../../result/THCA_parting/training_log_CMFM.txt'\n",
    "# RESULT_FILE = '../../result/THCA_parting/mogat_training_results.xlsx'\n",
    "# os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)\n",
    "\n",
    "# logging.basicConfig(\n",
    "#     filename=LOG_FILE,\n",
    "#     level=logging.INFO,\n",
    "#     format='%(asctime)s - %(message)s',\n",
    "#     filemode='w'  # 每次运行覆盖旧日志\n",
    "# )\n",
    "\n",
    "# # 超参数配置（便于维护和修改）\n",
    "# HP_CONFIG = {\n",
    "#     'lr': [0.0001, 0.001, 0.01],\n",
    "#     'depth': [2, 3, 4, 6],\n",
    "#     'heads': [4, 6, 8],\n",
    "#     'dim_head': [12, 16, 32, 64],\n",
    "#     'dropout': [0.3],\n",
    "#     'beta': [1],\n",
    "#     'adj_parameter': [2, 4, 6, 8, 10]\n",
    "# }\n",
    "\n",
    "# # 计算总组合数\n",
    "# total_combinations = (\n",
    "#     len(HP_CONFIG['lr']) *\n",
    "#     len(HP_CONFIG['depth']) *\n",
    "#     len(HP_CONFIG['heads']) *\n",
    "#     len(HP_CONFIG['dim_head']) *\n",
    "#     len(HP_CONFIG['dropout']) *\n",
    "#     len(HP_CONFIG['beta']) *\n",
    "#     len(HP_CONFIG['adj_parameter'])\n",
    "# )\n",
    "\n",
    "# # 结果缓存配置\n",
    "# BATCH_SIZE = 5  # 每5个结果写入一次文件\n",
    "# results_cache = []\n",
    "\n",
    "# def save_results(batch: list, force_save: bool = False):\n",
    "#     \"\"\"批量保存结果到文件\"\"\"\n",
    "#     if not force_save and len(batch) < BATCH_SIZE:\n",
    "#         return\n",
    "    \n",
    "#     try:\n",
    "#         df = pd.DataFrame(batch)\n",
    "#         header = not os.path.exists(RESULT_FILE)\n",
    "        \n",
    "#         with pd.ExcelWriter(\n",
    "#             RESULT_FILE,\n",
    "#             mode='a' if os.path.exists(RESULT_FILE) else 'w',\n",
    "#             engine='openpyxl',\n",
    "#             if_sheet_exists='overlay' if os.path.exists(RESULT_FILE) else None\n",
    "#         ) as writer:\n",
    "#             df.to_excel(\n",
    "#                 writer,\n",
    "#                 index=False,\n",
    "#                 sheet_name='Results',\n",
    "#                 header=header,\n",
    "#                 startrow=writer.sheets['Results'].max_row if not header else 0\n",
    "#             )\n",
    "#         batch.clear()\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"保存结果失败: {str(e)}\")\n",
    "#         raise\n",
    "\n",
    "# # 主训练循环\n",
    "# current_combination = 0\n",
    "# start_total = time.time()\n",
    "\n",
    "# try:\n",
    "#     for lr in HP_CONFIG['lr']:\n",
    "#         for depth in HP_CONFIG['depth']:\n",
    "#             for heads in HP_CONFIG['heads']:\n",
    "#                 for dim_head in HP_CONFIG['dim_head']:\n",
    "#                     for dropout in HP_CONFIG['dropout']:\n",
    "#                         for beta in HP_CONFIG['beta']:\n",
    "#                             for adj_param in HP_CONFIG['adj_parameter']:\n",
    "#                                 current_combination += 1\n",
    "#                                 start_iter = time.time()\n",
    "                                \n",
    "#                                 # 日志记录当前参数\n",
    "#                                 param_str = (\n",
    "#                                     f\"lr={lr}, depth={depth}, heads={heads}, \"\n",
    "#                                     f\"dim_head={dim_head}, dropout={dropout}, \"\n",
    "#                                     f\"beta={beta}, adj_parameter={adj_param} \"\n",
    "#                                     f\"({current_combination}/{total_combinations})\"\n",
    "#                                 )\n",
    "#                                 logging.info(f\"开始训练: {param_str}\")\n",
    "\n",
    "#                                 try:\n",
    "#                                     # 执行训练（假设train_test返回acc, f1_weighted, f1_macro）\n",
    "#                                     acc, f1_weighted, f1_macro = train_test(  # 修改变量接收\n",
    "#                                         Methylation_train=Methylation_train,\n",
    "#                                         miRNASeq_train=miRNASeq_train,\n",
    "#                                         RNAseq_train=RNAseq_train,\n",
    "#                                         lr=lr,\n",
    "#                                         depth=depth,\n",
    "#                                         heads=heads,\n",
    "#                                         dim_head=dim_head,\n",
    "#                                         beta=beta,\n",
    "#                                         attn_dropout=dropout,\n",
    "#                                         ff_dropout=dropout,\n",
    "#                                         classifier_dropout=dropout,\n",
    "#                                         adj_parameter=adj_param,\n",
    "#                                         num_epoch_pretrain=1500\n",
    "#                                     )\n",
    "\n",
    "#                                     # 缓存结果（修改键名）\n",
    "#                                     results_cache.append({\n",
    "#                                         'lr': lr,\n",
    "#                                         'depth': depth,\n",
    "#                                         'heads': heads,\n",
    "#                                         'dim_head': dim_head,\n",
    "#                                         'dropout': dropout,\n",
    "#                                         'beta': beta,\n",
    "#                                         'adj_parameter': adj_param,\n",
    "#                                         'accuracy': acc,\n",
    "#                                         'f1_weighted': f1_weighted,  # 修改键名\n",
    "#                                         'f1_macro': f1_macro          # 新增键\n",
    "#                                     })\n",
    "\n",
    "#                                     # 定期保存\n",
    "#                                     save_results(results_cache)\n",
    "\n",
    "#                                     # 记录成功信息（修改日志输出）\n",
    "#                                     elapsed = time.time() - start_iter\n",
    "#                                     logging.info(\n",
    "#                                         f\"完成: {param_str} | \"\n",
    "#                                         f\"耗时: {elapsed:.1f}s | \"\n",
    "#                                         f\"结果: ACC={acc:.4f}, F1_weighted={f1_weighted:.4f}, F1_macro={f1_macro:.4f}\"  # 修改指标名称\n",
    "#                                     )\n",
    "\n",
    "#                                 except torch.cuda.OutOfMemoryError:\n",
    "#                                     logging.warning(f\"显存不足，跳过组合: {param_str}\")\n",
    "#                                     torch.cuda.empty_cache()\n",
    "#                                 except Exception as e:\n",
    "#                                     logging.error(f\"训练失败: {param_str} | 错误: {str(e)}\")\n",
    "#                                     continue\n",
    "#                                 # 用于保存指标数据的列表（移除了AUC相关指标）\n",
    "#                                 train_losses = []\n",
    "#                                 val_losses = []\n",
    "#                                 test_losses = []\n",
    "#                                 val_accuracies = []\n",
    "#                                 val_f1_weighted_scores = []\n",
    "#                                 val_f1_macro_scores = []\n",
    "#                                 test_accuracies = []\n",
    "#                                 test_f1_weighted_scores = []\n",
    "#                                 test_f1_macro_scores = []\n",
    "#                                 train_accuracies = []\n",
    "#                                 train_f1_weighted_scores = []\n",
    "#                                 train_f1_macro_scores = []\n",
    "\n",
    "#     # 最后强制保存剩余结果\n",
    "#     save_results(results_cache, force_save=True)\n",
    "\n",
    "# except KeyboardInterrupt:\n",
    "#     logging.warning(\"用户中断训练，保存已完成的实验结果...\")\n",
    "#     save_results(results_cache, force_save=True)\n",
    "# finally:\n",
    "#     total_time = time.time() - start_total\n",
    "#     logging.info(f\"全部完成! 总耗时: {total_time/3600:.2f} 小时\")\n",
    "\n",
    "# print(\"实验完成，结果保存在:\", RESULT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7c5f32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练参数：lr=0.0001, depth=6, heads=4, dim_head=32, dropout=0.5, beta=1\n",
      "\n",
      "Pretrain GCNs...\n",
      "\n",
      "Training...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "# 假设你已经定义好这三个数据集和train_test函数\n",
    "# from your_module import train_test, Methylation_train, miRNASeq_train, RNAseq_train\n",
    "\n",
    "# 固定超参数\n",
    "lr = 0.001\n",
    "depth = 2\n",
    "heads = 4\n",
    "dim_head = 12\n",
    "dropout = 0.3\n",
    "beta = 1\n",
    "adj_param = 8\n",
    "num_epoch_pretrain = 458\n",
    "\n",
    "# 开始训练\n",
    "start_time = time.time()\n",
    "\n",
    "acc, f1_weighted, f1_macro = train_test(\n",
    "    Methylation_train=Methylation_train,\n",
    "    miRNASeq_train=miRNASeq_train,\n",
    "    RNAseq_train=RNAseq_train,\n",
    "    lr=lr,\n",
    "    depth=depth,\n",
    "    heads=heads,\n",
    "    dim_head=dim_head,\n",
    "    beta=beta,\n",
    "    attn_dropout=dropout,\n",
    "    ff_dropout=dropout,\n",
    "    classifier_dropout=dropout,\n",
    "    adj_parameter=adj_param,\n",
    "    num_epoch_pretrain=num_epoch_pretrain\n",
    ")\n",
    "\n",
    "# 输出结果\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"训练完成 | 耗时: {elapsed:.2f} 秒\")\n",
    "print(f\"准确率: {acc:.4f}\")\n",
    "print(f\"F1_weighted: {f1_weighted:.4f}\")\n",
    "print(f\"F1_macro: {f1_macro:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
